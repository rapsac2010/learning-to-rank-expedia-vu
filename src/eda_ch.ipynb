{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import configparser\n",
    "from tqdm import tqdm\n",
    "from helpers.helper_functions import *\n",
    "from helpers.helper_classes import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read config.ini file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('src/config.ini')\n",
    "os.chdir(config['PATH']['ROOT_DIR'])\n",
    "\n",
    "# # Load data\n",
    "df = pd.read_parquet(config['PATH']['INT_DIR'] + '/training_set_preprocessed.parquet', engine = 'fastparquet')\n",
    "df_test = pd.read_parquet(config['PATH']['INT_DIR'] + '/test_set_preprocessed.parquet', engine = 'fastparquet')\n",
    "\n",
    "# mini df for testing quickly \n",
    "df_mini = df[df['srch_id'] < 10000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train_test_split(df, target_str, test_size=.2):\n",
    "    splitter = GroupShuffleSplit(test_size=test_size, n_splits=2, random_state = 7)\n",
    "    split = splitter.split(df, groups=df['srch_id'])\n",
    "    train_inds, test_inds = next(split)\n",
    "\n",
    "    df_ideal = df.iloc[test_inds].copy().sort_values(by=['srch_id', target_str], ascending=[True, False], inplace=False)\n",
    "\n",
    "    X = df.drop([target_str], axis=1)\n",
    "    y = df[target_str]\n",
    "    X_train, X_test, y_train, y_test, test_ideal = X.iloc[train_inds], X.iloc[test_inds], y.iloc[train_inds], y.iloc[test_inds], df_ideal, \n",
    "\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, test_ideal[['srch_id', 'prop_id', target_str]]\n",
    "\n",
    "def construct_pred_ideal(df_in, df_ideal, y_pred):\n",
    "    df = df_in.copy()\n",
    "    df['pred_grades'] = y_pred\n",
    "    df = df.sort_values(by=['srch_id', 'pred_grades'], ascending=[True, False], inplace=False)\n",
    "\n",
    "    # Merge grades from ideal on srch_id and prop_id\n",
    "    df = df.merge(df_ideal, on=['srch_id', 'prop_id'], how='left')\n",
    "\n",
    "    # Return srch_id, prop_id and pred_grades\n",
    "    return df[['srch_id', 'prop_id', 'pred_grades', 'target']]\n",
    "\n",
    "def construct_pred_submission(df_in, y_pred):\n",
    "    df = df_in.copy()\n",
    "    df['pred_grades'] = y_pred\n",
    "    df = df.sort_values(by=['srch_id', 'pred_grades'], ascending=[True, False], inplace=False)\n",
    "\n",
    "    # Return srch_id, prop_id and pred_grades\n",
    "    return df[['srch_id', 'prop_id']]\n",
    "\n",
    "def constructs_predictions(model, data, ideal_df = None):\n",
    "    y_pred = model.predict_proba(data)\n",
    "    pred_grades = y_pred @ [0, 1, 5]\n",
    "\n",
    "    if ideal_df is not None:\n",
    "        pred_df = construct_pred_ideal(data, ideal_df, pred_grades)\n",
    "    else:\n",
    "        pred_df = construct_pred_submission(data, pred_grades)\n",
    "    return pred_df\n",
    "\n",
    "\n",
    "def calc_NDCG(df_ideal, df_pred, k = 5):\n",
    "    # Group by 5\n",
    "    df_ideal = df_ideal.groupby('srch_id').head(k)\n",
    "    df_pred = df_pred.groupby('srch_id').head(k)\n",
    "\n",
    "    assert df_ideal.shape[0] % k == 0\n",
    "    assert df_pred.shape[0] % k == 0\n",
    "    \n",
    "    # Get grades matrices\n",
    "    ideal_grades = df_ideal['target'].values.reshape(int(df_ideal.shape[0] / k), k)\n",
    "    pred_grades = df_pred['target'].values.reshape(int(df_pred.shape[0] / k), k)\n",
    "\n",
    "    discount_vec = [1/np.log2(i+2) for i in range(k)]\n",
    "\n",
    "    # Calculate NDCG\n",
    "    NDCG = (pred_grades @ discount_vec).sum() / (ideal_grades @ discount_vec).sum()\n",
    "\n",
    "    return NDCG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cur = df\n",
    "X_train, X_test, y_train, y_test, test_ideal = train_test_split(df_cur, 'target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's ndcg@5: 0.260898\tvalid_1's ndcg@5: 0.257203\n",
      "[2]\ttraining's ndcg@5: 0.286388\tvalid_1's ndcg@5: 0.282774\n",
      "[3]\ttraining's ndcg@5: 0.307505\tvalid_1's ndcg@5: 0.305398\n",
      "[4]\ttraining's ndcg@5: 0.32327\tvalid_1's ndcg@5: 0.321026\n",
      "[5]\ttraining's ndcg@5: 0.331178\tvalid_1's ndcg@5: 0.328779\n",
      "[6]\ttraining's ndcg@5: 0.336028\tvalid_1's ndcg@5: 0.3334\n",
      "[7]\ttraining's ndcg@5: 0.337903\tvalid_1's ndcg@5: 0.334434\n",
      "[8]\ttraining's ndcg@5: 0.342956\tvalid_1's ndcg@5: 0.339629\n",
      "[9]\ttraining's ndcg@5: 0.344279\tvalid_1's ndcg@5: 0.340288\n",
      "[10]\ttraining's ndcg@5: 0.345494\tvalid_1's ndcg@5: 0.341548\n",
      "[11]\ttraining's ndcg@5: 0.347928\tvalid_1's ndcg@5: 0.344023\n",
      "[12]\ttraining's ndcg@5: 0.348417\tvalid_1's ndcg@5: 0.344556\n",
      "[13]\ttraining's ndcg@5: 0.350871\tvalid_1's ndcg@5: 0.346388\n",
      "[14]\ttraining's ndcg@5: 0.353239\tvalid_1's ndcg@5: 0.348623\n",
      "[15]\ttraining's ndcg@5: 0.353023\tvalid_1's ndcg@5: 0.348551\n",
      "[16]\ttraining's ndcg@5: 0.353705\tvalid_1's ndcg@5: 0.348446\n",
      "[17]\ttraining's ndcg@5: 0.355641\tvalid_1's ndcg@5: 0.349973\n",
      "[18]\ttraining's ndcg@5: 0.355204\tvalid_1's ndcg@5: 0.349771\n",
      "[19]\ttraining's ndcg@5: 0.35613\tvalid_1's ndcg@5: 0.350321\n",
      "[20]\ttraining's ndcg@5: 0.356545\tvalid_1's ndcg@5: 0.351051\n",
      "[21]\ttraining's ndcg@5: 0.357469\tvalid_1's ndcg@5: 0.352155\n",
      "[22]\ttraining's ndcg@5: 0.357654\tvalid_1's ndcg@5: 0.351991\n",
      "[23]\ttraining's ndcg@5: 0.358398\tvalid_1's ndcg@5: 0.352876\n",
      "[24]\ttraining's ndcg@5: 0.358532\tvalid_1's ndcg@5: 0.353032\n",
      "[25]\ttraining's ndcg@5: 0.358828\tvalid_1's ndcg@5: 0.353409\n",
      "[26]\ttraining's ndcg@5: 0.359349\tvalid_1's ndcg@5: 0.353652\n",
      "[27]\ttraining's ndcg@5: 0.35908\tvalid_1's ndcg@5: 0.353658\n",
      "[28]\ttraining's ndcg@5: 0.359924\tvalid_1's ndcg@5: 0.354077\n",
      "[29]\ttraining's ndcg@5: 0.360019\tvalid_1's ndcg@5: 0.354072\n",
      "[30]\ttraining's ndcg@5: 0.359946\tvalid_1's ndcg@5: 0.35395\n",
      "[31]\ttraining's ndcg@5: 0.360078\tvalid_1's ndcg@5: 0.354559\n",
      "[32]\ttraining's ndcg@5: 0.360148\tvalid_1's ndcg@5: 0.353998\n",
      "[33]\ttraining's ndcg@5: 0.360438\tvalid_1's ndcg@5: 0.354387\n",
      "[34]\ttraining's ndcg@5: 0.360422\tvalid_1's ndcg@5: 0.354128\n",
      "[35]\ttraining's ndcg@5: 0.360223\tvalid_1's ndcg@5: 0.354113\n",
      "[36]\ttraining's ndcg@5: 0.361604\tvalid_1's ndcg@5: 0.355459\n",
      "[37]\ttraining's ndcg@5: 0.361348\tvalid_1's ndcg@5: 0.355607\n",
      "[38]\ttraining's ndcg@5: 0.36208\tvalid_1's ndcg@5: 0.356147\n",
      "[39]\ttraining's ndcg@5: 0.362795\tvalid_1's ndcg@5: 0.356862\n",
      "[40]\ttraining's ndcg@5: 0.362683\tvalid_1's ndcg@5: 0.35693\n",
      "[41]\ttraining's ndcg@5: 0.362855\tvalid_1's ndcg@5: 0.356939\n",
      "[42]\ttraining's ndcg@5: 0.362939\tvalid_1's ndcg@5: 0.356804\n",
      "[43]\ttraining's ndcg@5: 0.363005\tvalid_1's ndcg@5: 0.356759\n",
      "[44]\ttraining's ndcg@5: 0.363596\tvalid_1's ndcg@5: 0.357531\n",
      "[45]\ttraining's ndcg@5: 0.363408\tvalid_1's ndcg@5: 0.357738\n",
      "[46]\ttraining's ndcg@5: 0.363254\tvalid_1's ndcg@5: 0.357712\n",
      "[47]\ttraining's ndcg@5: 0.363888\tvalid_1's ndcg@5: 0.357943\n",
      "[48]\ttraining's ndcg@5: 0.363961\tvalid_1's ndcg@5: 0.358231\n",
      "[49]\ttraining's ndcg@5: 0.363936\tvalid_1's ndcg@5: 0.357996\n",
      "[50]\ttraining's ndcg@5: 0.364145\tvalid_1's ndcg@5: 0.358455\n",
      "[51]\ttraining's ndcg@5: 0.364226\tvalid_1's ndcg@5: 0.358422\n",
      "[52]\ttraining's ndcg@5: 0.364163\tvalid_1's ndcg@5: 0.358621\n",
      "[53]\ttraining's ndcg@5: 0.364642\tvalid_1's ndcg@5: 0.358757\n",
      "[54]\ttraining's ndcg@5: 0.364719\tvalid_1's ndcg@5: 0.358655\n",
      "[55]\ttraining's ndcg@5: 0.364669\tvalid_1's ndcg@5: 0.35865\n",
      "[56]\ttraining's ndcg@5: 0.364829\tvalid_1's ndcg@5: 0.358726\n",
      "[57]\ttraining's ndcg@5: 0.36528\tvalid_1's ndcg@5: 0.359069\n",
      "[58]\ttraining's ndcg@5: 0.365173\tvalid_1's ndcg@5: 0.358935\n",
      "[59]\ttraining's ndcg@5: 0.365252\tvalid_1's ndcg@5: 0.359172\n",
      "[60]\ttraining's ndcg@5: 0.365587\tvalid_1's ndcg@5: 0.359604\n",
      "[61]\ttraining's ndcg@5: 0.365725\tvalid_1's ndcg@5: 0.359609\n",
      "[62]\ttraining's ndcg@5: 0.365548\tvalid_1's ndcg@5: 0.359355\n",
      "[63]\ttraining's ndcg@5: 0.365565\tvalid_1's ndcg@5: 0.35937\n",
      "[64]\ttraining's ndcg@5: 0.366037\tvalid_1's ndcg@5: 0.3598\n",
      "[65]\ttraining's ndcg@5: 0.365998\tvalid_1's ndcg@5: 0.359958\n",
      "[66]\ttraining's ndcg@5: 0.366128\tvalid_1's ndcg@5: 0.360476\n",
      "[67]\ttraining's ndcg@5: 0.366329\tvalid_1's ndcg@5: 0.360541\n",
      "[68]\ttraining's ndcg@5: 0.366446\tvalid_1's ndcg@5: 0.360497\n",
      "[69]\ttraining's ndcg@5: 0.366373\tvalid_1's ndcg@5: 0.360752\n",
      "[70]\ttraining's ndcg@5: 0.366857\tvalid_1's ndcg@5: 0.361136\n",
      "[71]\ttraining's ndcg@5: 0.366986\tvalid_1's ndcg@5: 0.361054\n",
      "[72]\ttraining's ndcg@5: 0.367269\tvalid_1's ndcg@5: 0.361248\n",
      "[73]\ttraining's ndcg@5: 0.367472\tvalid_1's ndcg@5: 0.361439\n",
      "[74]\ttraining's ndcg@5: 0.367398\tvalid_1's ndcg@5: 0.361488\n",
      "[75]\ttraining's ndcg@5: 0.367469\tvalid_1's ndcg@5: 0.361502\n",
      "[76]\ttraining's ndcg@5: 0.367544\tvalid_1's ndcg@5: 0.361593\n",
      "[77]\ttraining's ndcg@5: 0.367613\tvalid_1's ndcg@5: 0.361523\n",
      "[78]\ttraining's ndcg@5: 0.367691\tvalid_1's ndcg@5: 0.361548\n",
      "[79]\ttraining's ndcg@5: 0.367743\tvalid_1's ndcg@5: 0.361569\n",
      "[80]\ttraining's ndcg@5: 0.367879\tvalid_1's ndcg@5: 0.362011\n",
      "[81]\ttraining's ndcg@5: 0.36803\tvalid_1's ndcg@5: 0.362049\n",
      "[82]\ttraining's ndcg@5: 0.367905\tvalid_1's ndcg@5: 0.362122\n",
      "[83]\ttraining's ndcg@5: 0.368041\tvalid_1's ndcg@5: 0.362365\n",
      "[84]\ttraining's ndcg@5: 0.368054\tvalid_1's ndcg@5: 0.362506\n",
      "[85]\ttraining's ndcg@5: 0.368253\tvalid_1's ndcg@5: 0.362613\n",
      "[86]\ttraining's ndcg@5: 0.368488\tvalid_1's ndcg@5: 0.362677\n",
      "[87]\ttraining's ndcg@5: 0.368654\tvalid_1's ndcg@5: 0.362917\n",
      "[88]\ttraining's ndcg@5: 0.368636\tvalid_1's ndcg@5: 0.362937\n",
      "[89]\ttraining's ndcg@5: 0.368724\tvalid_1's ndcg@5: 0.363068\n",
      "[90]\ttraining's ndcg@5: 0.368754\tvalid_1's ndcg@5: 0.36274\n",
      "[91]\ttraining's ndcg@5: 0.36884\tvalid_1's ndcg@5: 0.362951\n",
      "[92]\ttraining's ndcg@5: 0.368978\tvalid_1's ndcg@5: 0.363011\n",
      "[93]\ttraining's ndcg@5: 0.368986\tvalid_1's ndcg@5: 0.362979\n",
      "[94]\ttraining's ndcg@5: 0.369121\tvalid_1's ndcg@5: 0.363225\n",
      "[95]\ttraining's ndcg@5: 0.369108\tvalid_1's ndcg@5: 0.363593\n",
      "[96]\ttraining's ndcg@5: 0.369414\tvalid_1's ndcg@5: 0.363595\n",
      "[97]\ttraining's ndcg@5: 0.369583\tvalid_1's ndcg@5: 0.363672\n",
      "[98]\ttraining's ndcg@5: 0.369616\tvalid_1's ndcg@5: 0.363813\n",
      "[99]\ttraining's ndcg@5: 0.369635\tvalid_1's ndcg@5: 0.363777\n",
      "[100]\ttraining's ndcg@5: 0.369744\tvalid_1's ndcg@5: 0.363905\n",
      "[101]\ttraining's ndcg@5: 0.369884\tvalid_1's ndcg@5: 0.363962\n",
      "[102]\ttraining's ndcg@5: 0.369872\tvalid_1's ndcg@5: 0.36406\n",
      "[103]\ttraining's ndcg@5: 0.37009\tvalid_1's ndcg@5: 0.36404\n",
      "[104]\ttraining's ndcg@5: 0.370155\tvalid_1's ndcg@5: 0.363998\n",
      "[105]\ttraining's ndcg@5: 0.370279\tvalid_1's ndcg@5: 0.364423\n",
      "[106]\ttraining's ndcg@5: 0.37047\tvalid_1's ndcg@5: 0.364368\n",
      "[107]\ttraining's ndcg@5: 0.370451\tvalid_1's ndcg@5: 0.364383\n",
      "[108]\ttraining's ndcg@5: 0.370582\tvalid_1's ndcg@5: 0.364346\n",
      "[109]\ttraining's ndcg@5: 0.370633\tvalid_1's ndcg@5: 0.364365\n",
      "[110]\ttraining's ndcg@5: 0.370662\tvalid_1's ndcg@5: 0.364285\n",
      "[111]\ttraining's ndcg@5: 0.370696\tvalid_1's ndcg@5: 0.364329\n",
      "[112]\ttraining's ndcg@5: 0.370762\tvalid_1's ndcg@5: 0.364599\n",
      "[113]\ttraining's ndcg@5: 0.370883\tvalid_1's ndcg@5: 0.364933\n",
      "[114]\ttraining's ndcg@5: 0.371036\tvalid_1's ndcg@5: 0.364955\n",
      "[115]\ttraining's ndcg@5: 0.371143\tvalid_1's ndcg@5: 0.36513\n",
      "[116]\ttraining's ndcg@5: 0.371354\tvalid_1's ndcg@5: 0.365118\n",
      "[117]\ttraining's ndcg@5: 0.371482\tvalid_1's ndcg@5: 0.36513\n",
      "[118]\ttraining's ndcg@5: 0.371472\tvalid_1's ndcg@5: 0.365074\n",
      "[119]\ttraining's ndcg@5: 0.37151\tvalid_1's ndcg@5: 0.365211\n",
      "[120]\ttraining's ndcg@5: 0.371602\tvalid_1's ndcg@5: 0.365356\n",
      "[121]\ttraining's ndcg@5: 0.371685\tvalid_1's ndcg@5: 0.365247\n",
      "[122]\ttraining's ndcg@5: 0.371698\tvalid_1's ndcg@5: 0.365391\n",
      "[123]\ttraining's ndcg@5: 0.371921\tvalid_1's ndcg@5: 0.365367\n",
      "[124]\ttraining's ndcg@5: 0.37199\tvalid_1's ndcg@5: 0.365194\n",
      "[125]\ttraining's ndcg@5: 0.371998\tvalid_1's ndcg@5: 0.365375\n",
      "[126]\ttraining's ndcg@5: 0.372048\tvalid_1's ndcg@5: 0.365349\n",
      "[127]\ttraining's ndcg@5: 0.372108\tvalid_1's ndcg@5: 0.365398\n",
      "[128]\ttraining's ndcg@5: 0.372189\tvalid_1's ndcg@5: 0.365485\n",
      "[129]\ttraining's ndcg@5: 0.372254\tvalid_1's ndcg@5: 0.365337\n",
      "[130]\ttraining's ndcg@5: 0.372242\tvalid_1's ndcg@5: 0.365422\n",
      "[131]\ttraining's ndcg@5: 0.372369\tvalid_1's ndcg@5: 0.365533\n",
      "[132]\ttraining's ndcg@5: 0.372486\tvalid_1's ndcg@5: 0.365673\n",
      "[133]\ttraining's ndcg@5: 0.372618\tvalid_1's ndcg@5: 0.365664\n",
      "[134]\ttraining's ndcg@5: 0.372659\tvalid_1's ndcg@5: 0.365785\n",
      "[135]\ttraining's ndcg@5: 0.372801\tvalid_1's ndcg@5: 0.36609\n",
      "[136]\ttraining's ndcg@5: 0.372939\tvalid_1's ndcg@5: 0.366153\n",
      "[137]\ttraining's ndcg@5: 0.372738\tvalid_1's ndcg@5: 0.36608\n",
      "[138]\ttraining's ndcg@5: 0.372903\tvalid_1's ndcg@5: 0.36624\n",
      "[139]\ttraining's ndcg@5: 0.37294\tvalid_1's ndcg@5: 0.366101\n",
      "[140]\ttraining's ndcg@5: 0.372918\tvalid_1's ndcg@5: 0.366042\n",
      "[141]\ttraining's ndcg@5: 0.37296\tvalid_1's ndcg@5: 0.366141\n",
      "[142]\ttraining's ndcg@5: 0.373176\tvalid_1's ndcg@5: 0.366328\n",
      "[143]\ttraining's ndcg@5: 0.373251\tvalid_1's ndcg@5: 0.366381\n",
      "[144]\ttraining's ndcg@5: 0.373324\tvalid_1's ndcg@5: 0.366264\n",
      "[145]\ttraining's ndcg@5: 0.373341\tvalid_1's ndcg@5: 0.366385\n",
      "[146]\ttraining's ndcg@5: 0.373321\tvalid_1's ndcg@5: 0.366199\n",
      "[147]\ttraining's ndcg@5: 0.373326\tvalid_1's ndcg@5: 0.366344\n",
      "[148]\ttraining's ndcg@5: 0.373387\tvalid_1's ndcg@5: 0.366354\n",
      "[149]\ttraining's ndcg@5: 0.373614\tvalid_1's ndcg@5: 0.366358\n",
      "[150]\ttraining's ndcg@5: 0.373578\tvalid_1's ndcg@5: 0.366406\n",
      "[151]\ttraining's ndcg@5: 0.373687\tvalid_1's ndcg@5: 0.366474\n",
      "[152]\ttraining's ndcg@5: 0.373651\tvalid_1's ndcg@5: 0.366395\n",
      "[153]\ttraining's ndcg@5: 0.373637\tvalid_1's ndcg@5: 0.366415\n",
      "[154]\ttraining's ndcg@5: 0.373745\tvalid_1's ndcg@5: 0.366327\n",
      "[155]\ttraining's ndcg@5: 0.373728\tvalid_1's ndcg@5: 0.366408\n",
      "[156]\ttraining's ndcg@5: 0.373825\tvalid_1's ndcg@5: 0.366294\n",
      "[157]\ttraining's ndcg@5: 0.373925\tvalid_1's ndcg@5: 0.366605\n",
      "[158]\ttraining's ndcg@5: 0.373857\tvalid_1's ndcg@5: 0.366469\n",
      "[159]\ttraining's ndcg@5: 0.373961\tvalid_1's ndcg@5: 0.366492\n",
      "[160]\ttraining's ndcg@5: 0.37405\tvalid_1's ndcg@5: 0.366552\n",
      "[161]\ttraining's ndcg@5: 0.374139\tvalid_1's ndcg@5: 0.36646\n",
      "[162]\ttraining's ndcg@5: 0.374189\tvalid_1's ndcg@5: 0.366683\n",
      "[163]\ttraining's ndcg@5: 0.374284\tvalid_1's ndcg@5: 0.366639\n",
      "[164]\ttraining's ndcg@5: 0.374405\tvalid_1's ndcg@5: 0.366565\n",
      "[165]\ttraining's ndcg@5: 0.374406\tvalid_1's ndcg@5: 0.366542\n",
      "[166]\ttraining's ndcg@5: 0.37451\tvalid_1's ndcg@5: 0.366634\n",
      "[167]\ttraining's ndcg@5: 0.374564\tvalid_1's ndcg@5: 0.366873\n",
      "[168]\ttraining's ndcg@5: 0.37449\tvalid_1's ndcg@5: 0.36681\n",
      "[169]\ttraining's ndcg@5: 0.374427\tvalid_1's ndcg@5: 0.366871\n",
      "[170]\ttraining's ndcg@5: 0.374466\tvalid_1's ndcg@5: 0.366753\n",
      "[171]\ttraining's ndcg@5: 0.374579\tvalid_1's ndcg@5: 0.366908\n",
      "[172]\ttraining's ndcg@5: 0.374713\tvalid_1's ndcg@5: 0.367108\n",
      "[173]\ttraining's ndcg@5: 0.374791\tvalid_1's ndcg@5: 0.367304\n",
      "[174]\ttraining's ndcg@5: 0.374919\tvalid_1's ndcg@5: 0.367169\n",
      "[175]\ttraining's ndcg@5: 0.375004\tvalid_1's ndcg@5: 0.36719\n",
      "[176]\ttraining's ndcg@5: 0.374949\tvalid_1's ndcg@5: 0.367067\n",
      "[177]\ttraining's ndcg@5: 0.374976\tvalid_1's ndcg@5: 0.367068\n",
      "[178]\ttraining's ndcg@5: 0.375087\tvalid_1's ndcg@5: 0.367157\n",
      "[179]\ttraining's ndcg@5: 0.37518\tvalid_1's ndcg@5: 0.367432\n",
      "[180]\ttraining's ndcg@5: 0.375288\tvalid_1's ndcg@5: 0.367485\n",
      "[181]\ttraining's ndcg@5: 0.375328\tvalid_1's ndcg@5: 0.367385\n",
      "[182]\ttraining's ndcg@5: 0.37527\tvalid_1's ndcg@5: 0.367575\n",
      "[183]\ttraining's ndcg@5: 0.375316\tvalid_1's ndcg@5: 0.367486\n",
      "[184]\ttraining's ndcg@5: 0.375354\tvalid_1's ndcg@5: 0.3673\n",
      "[185]\ttraining's ndcg@5: 0.375445\tvalid_1's ndcg@5: 0.367422\n",
      "[186]\ttraining's ndcg@5: 0.375491\tvalid_1's ndcg@5: 0.367376\n",
      "[187]\ttraining's ndcg@5: 0.375561\tvalid_1's ndcg@5: 0.367572\n",
      "[188]\ttraining's ndcg@5: 0.375546\tvalid_1's ndcg@5: 0.367443\n",
      "[189]\ttraining's ndcg@5: 0.375608\tvalid_1's ndcg@5: 0.367626\n",
      "[190]\ttraining's ndcg@5: 0.375709\tvalid_1's ndcg@5: 0.367756\n",
      "[191]\ttraining's ndcg@5: 0.375763\tvalid_1's ndcg@5: 0.367734\n",
      "[192]\ttraining's ndcg@5: 0.37581\tvalid_1's ndcg@5: 0.367715\n",
      "[193]\ttraining's ndcg@5: 0.375856\tvalid_1's ndcg@5: 0.367728\n",
      "[194]\ttraining's ndcg@5: 0.37583\tvalid_1's ndcg@5: 0.367737\n",
      "[195]\ttraining's ndcg@5: 0.375929\tvalid_1's ndcg@5: 0.367846\n",
      "[196]\ttraining's ndcg@5: 0.37592\tvalid_1's ndcg@5: 0.367616\n",
      "[197]\ttraining's ndcg@5: 0.376003\tvalid_1's ndcg@5: 0.367779\n",
      "[198]\ttraining's ndcg@5: 0.376032\tvalid_1's ndcg@5: 0.367885\n",
      "[199]\ttraining's ndcg@5: 0.376037\tvalid_1's ndcg@5: 0.368094\n",
      "[200]\ttraining's ndcg@5: 0.376118\tvalid_1's ndcg@5: 0.36807\n",
      "[201]\ttraining's ndcg@5: 0.376228\tvalid_1's ndcg@5: 0.368068\n",
      "[202]\ttraining's ndcg@5: 0.376193\tvalid_1's ndcg@5: 0.368144\n",
      "[203]\ttraining's ndcg@5: 0.376223\tvalid_1's ndcg@5: 0.368313\n",
      "[204]\ttraining's ndcg@5: 0.376325\tvalid_1's ndcg@5: 0.368149\n",
      "[205]\ttraining's ndcg@5: 0.37637\tvalid_1's ndcg@5: 0.368391\n",
      "[206]\ttraining's ndcg@5: 0.376489\tvalid_1's ndcg@5: 0.368276\n",
      "[207]\ttraining's ndcg@5: 0.376518\tvalid_1's ndcg@5: 0.36833\n",
      "[208]\ttraining's ndcg@5: 0.376588\tvalid_1's ndcg@5: 0.368457\n",
      "[209]\ttraining's ndcg@5: 0.376699\tvalid_1's ndcg@5: 0.368534\n",
      "[210]\ttraining's ndcg@5: 0.376752\tvalid_1's ndcg@5: 0.368396\n",
      "[211]\ttraining's ndcg@5: 0.376674\tvalid_1's ndcg@5: 0.368461\n",
      "[212]\ttraining's ndcg@5: 0.376699\tvalid_1's ndcg@5: 0.368453\n",
      "[213]\ttraining's ndcg@5: 0.376741\tvalid_1's ndcg@5: 0.368676\n",
      "[214]\ttraining's ndcg@5: 0.376746\tvalid_1's ndcg@5: 0.368721\n",
      "[215]\ttraining's ndcg@5: 0.376748\tvalid_1's ndcg@5: 0.368676\n",
      "[216]\ttraining's ndcg@5: 0.376876\tvalid_1's ndcg@5: 0.368867\n",
      "[217]\ttraining's ndcg@5: 0.376901\tvalid_1's ndcg@5: 0.36881\n",
      "[218]\ttraining's ndcg@5: 0.376948\tvalid_1's ndcg@5: 0.368898\n",
      "[219]\ttraining's ndcg@5: 0.377016\tvalid_1's ndcg@5: 0.36883\n",
      "[220]\ttraining's ndcg@5: 0.377051\tvalid_1's ndcg@5: 0.368917\n",
      "[221]\ttraining's ndcg@5: 0.377079\tvalid_1's ndcg@5: 0.368866\n",
      "[222]\ttraining's ndcg@5: 0.377106\tvalid_1's ndcg@5: 0.368929\n",
      "[223]\ttraining's ndcg@5: 0.377185\tvalid_1's ndcg@5: 0.368943\n",
      "[224]\ttraining's ndcg@5: 0.377221\tvalid_1's ndcg@5: 0.368989\n",
      "[225]\ttraining's ndcg@5: 0.377158\tvalid_1's ndcg@5: 0.368952\n",
      "[226]\ttraining's ndcg@5: 0.37731\tvalid_1's ndcg@5: 0.368992\n",
      "[227]\ttraining's ndcg@5: 0.377386\tvalid_1's ndcg@5: 0.368964\n",
      "[228]\ttraining's ndcg@5: 0.377314\tvalid_1's ndcg@5: 0.369193\n",
      "[229]\ttraining's ndcg@5: 0.377368\tvalid_1's ndcg@5: 0.369041\n",
      "[230]\ttraining's ndcg@5: 0.377491\tvalid_1's ndcg@5: 0.369081\n",
      "[231]\ttraining's ndcg@5: 0.37749\tvalid_1's ndcg@5: 0.368959\n",
      "[232]\ttraining's ndcg@5: 0.377464\tvalid_1's ndcg@5: 0.368958\n",
      "[233]\ttraining's ndcg@5: 0.377498\tvalid_1's ndcg@5: 0.369169\n",
      "[234]\ttraining's ndcg@5: 0.377586\tvalid_1's ndcg@5: 0.369137\n",
      "[235]\ttraining's ndcg@5: 0.377598\tvalid_1's ndcg@5: 0.36922\n",
      "[236]\ttraining's ndcg@5: 0.377645\tvalid_1's ndcg@5: 0.369281\n",
      "[237]\ttraining's ndcg@5: 0.377687\tvalid_1's ndcg@5: 0.369329\n",
      "[238]\ttraining's ndcg@5: 0.377818\tvalid_1's ndcg@5: 0.369261\n",
      "[239]\ttraining's ndcg@5: 0.377928\tvalid_1's ndcg@5: 0.369369\n",
      "[240]\ttraining's ndcg@5: 0.377727\tvalid_1's ndcg@5: 0.369348\n",
      "[241]\ttraining's ndcg@5: 0.377822\tvalid_1's ndcg@5: 0.369452\n",
      "[242]\ttraining's ndcg@5: 0.377873\tvalid_1's ndcg@5: 0.369646\n",
      "[243]\ttraining's ndcg@5: 0.377884\tvalid_1's ndcg@5: 0.369509\n",
      "[244]\ttraining's ndcg@5: 0.378033\tvalid_1's ndcg@5: 0.369458\n",
      "[245]\ttraining's ndcg@5: 0.378051\tvalid_1's ndcg@5: 0.369437\n",
      "[246]\ttraining's ndcg@5: 0.378112\tvalid_1's ndcg@5: 0.369509\n",
      "[247]\ttraining's ndcg@5: 0.37809\tvalid_1's ndcg@5: 0.369494\n",
      "[248]\ttraining's ndcg@5: 0.378096\tvalid_1's ndcg@5: 0.369492\n",
      "[249]\ttraining's ndcg@5: 0.378213\tvalid_1's ndcg@5: 0.369797\n",
      "[250]\ttraining's ndcg@5: 0.378156\tvalid_1's ndcg@5: 0.369627\n",
      "[251]\ttraining's ndcg@5: 0.378152\tvalid_1's ndcg@5: 0.36953\n",
      "[252]\ttraining's ndcg@5: 0.378242\tvalid_1's ndcg@5: 0.36979\n",
      "[253]\ttraining's ndcg@5: 0.378236\tvalid_1's ndcg@5: 0.369689\n",
      "[254]\ttraining's ndcg@5: 0.378313\tvalid_1's ndcg@5: 0.369707\n",
      "[255]\ttraining's ndcg@5: 0.378316\tvalid_1's ndcg@5: 0.369633\n",
      "[256]\ttraining's ndcg@5: 0.378355\tvalid_1's ndcg@5: 0.369499\n",
      "[257]\ttraining's ndcg@5: 0.378347\tvalid_1's ndcg@5: 0.369685\n",
      "[258]\ttraining's ndcg@5: 0.378405\tvalid_1's ndcg@5: 0.369607\n",
      "[259]\ttraining's ndcg@5: 0.378399\tvalid_1's ndcg@5: 0.369679\n",
      "[260]\ttraining's ndcg@5: 0.378426\tvalid_1's ndcg@5: 0.369692\n",
      "[261]\ttraining's ndcg@5: 0.378432\tvalid_1's ndcg@5: 0.369724\n",
      "[262]\ttraining's ndcg@5: 0.378449\tvalid_1's ndcg@5: 0.369827\n",
      "[263]\ttraining's ndcg@5: 0.378446\tvalid_1's ndcg@5: 0.369958\n",
      "[264]\ttraining's ndcg@5: 0.378535\tvalid_1's ndcg@5: 0.36997\n",
      "[265]\ttraining's ndcg@5: 0.378623\tvalid_1's ndcg@5: 0.369957\n",
      "[266]\ttraining's ndcg@5: 0.378612\tvalid_1's ndcg@5: 0.369897\n",
      "[267]\ttraining's ndcg@5: 0.37857\tvalid_1's ndcg@5: 0.369966\n",
      "[268]\ttraining's ndcg@5: 0.378595\tvalid_1's ndcg@5: 0.369926\n",
      "[269]\ttraining's ndcg@5: 0.378722\tvalid_1's ndcg@5: 0.370012\n",
      "[270]\ttraining's ndcg@5: 0.37871\tvalid_1's ndcg@5: 0.37005\n",
      "[271]\ttraining's ndcg@5: 0.378854\tvalid_1's ndcg@5: 0.370096\n",
      "[272]\ttraining's ndcg@5: 0.378937\tvalid_1's ndcg@5: 0.370001\n",
      "[273]\ttraining's ndcg@5: 0.378986\tvalid_1's ndcg@5: 0.37006\n",
      "[274]\ttraining's ndcg@5: 0.37892\tvalid_1's ndcg@5: 0.370045\n",
      "[275]\ttraining's ndcg@5: 0.379003\tvalid_1's ndcg@5: 0.370127\n",
      "[276]\ttraining's ndcg@5: 0.378967\tvalid_1's ndcg@5: 0.370238\n",
      "[277]\ttraining's ndcg@5: 0.378974\tvalid_1's ndcg@5: 0.37019\n",
      "[278]\ttraining's ndcg@5: 0.378984\tvalid_1's ndcg@5: 0.370347\n",
      "[279]\ttraining's ndcg@5: 0.379007\tvalid_1's ndcg@5: 0.370253\n",
      "[280]\ttraining's ndcg@5: 0.379035\tvalid_1's ndcg@5: 0.370462\n",
      "[281]\ttraining's ndcg@5: 0.379018\tvalid_1's ndcg@5: 0.370413\n",
      "[282]\ttraining's ndcg@5: 0.379118\tvalid_1's ndcg@5: 0.37043\n",
      "[283]\ttraining's ndcg@5: 0.379126\tvalid_1's ndcg@5: 0.370316\n",
      "[284]\ttraining's ndcg@5: 0.379272\tvalid_1's ndcg@5: 0.37031\n",
      "[285]\ttraining's ndcg@5: 0.379368\tvalid_1's ndcg@5: 0.370364\n",
      "[286]\ttraining's ndcg@5: 0.379385\tvalid_1's ndcg@5: 0.370398\n",
      "[287]\ttraining's ndcg@5: 0.379482\tvalid_1's ndcg@5: 0.370578\n",
      "[288]\ttraining's ndcg@5: 0.379497\tvalid_1's ndcg@5: 0.370711\n",
      "[289]\ttraining's ndcg@5: 0.379496\tvalid_1's ndcg@5: 0.370686\n",
      "[290]\ttraining's ndcg@5: 0.379505\tvalid_1's ndcg@5: 0.370716\n",
      "[291]\ttraining's ndcg@5: 0.379497\tvalid_1's ndcg@5: 0.370618\n",
      "[292]\ttraining's ndcg@5: 0.379503\tvalid_1's ndcg@5: 0.370661\n",
      "[293]\ttraining's ndcg@5: 0.379589\tvalid_1's ndcg@5: 0.370594\n",
      "[294]\ttraining's ndcg@5: 0.379613\tvalid_1's ndcg@5: 0.370779\n",
      "[295]\ttraining's ndcg@5: 0.379563\tvalid_1's ndcg@5: 0.370698\n",
      "[296]\ttraining's ndcg@5: 0.379532\tvalid_1's ndcg@5: 0.370701\n",
      "[297]\ttraining's ndcg@5: 0.379607\tvalid_1's ndcg@5: 0.370771\n",
      "[298]\ttraining's ndcg@5: 0.379648\tvalid_1's ndcg@5: 0.370775\n",
      "[299]\ttraining's ndcg@5: 0.379691\tvalid_1's ndcg@5: 0.370907\n",
      "[300]\ttraining's ndcg@5: 0.379748\tvalid_1's ndcg@5: 0.370888\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMRanker(importance_type=&#x27;gain&#x27;, learning_rate=0.05, max_depth=10,\n",
       "           metric=&#x27;ndcg&#x27;, n_estimators=300, num_leaves=10,\n",
       "           objective=&#x27;lambdarank&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMRanker</label><div class=\"sk-toggleable__content\"><pre>LGBMRanker(importance_type=&#x27;gain&#x27;, learning_rate=0.05, max_depth=10,\n",
       "           metric=&#x27;ndcg&#x27;, n_estimators=300, num_leaves=10,\n",
       "           objective=&#x27;lambdarank&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMRanker(importance_type='gain', learning_rate=0.05, max_depth=10,\n",
       "           metric='ndcg', n_estimators=300, num_leaves=10,\n",
       "           objective='lambdarank')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LightGBM ranker\n",
    "import lightgbm as lgb\n",
    "import optuna.integration.lightgbm as optuna_lgb\n",
    "\n",
    "# Create dataset\n",
    "group_train = X_train.groupby('srch_id').size().values\n",
    "group_val = X_test.groupby('srch_id').size().values\n",
    "group_test = df_test.groupby('srch_id').size().values\n",
    "\n",
    "X_train_lgb = X_train.drop(['srch_id'], axis=1)\n",
    "X_val_lgb = X_test.drop(['srch_id'], axis=1)\n",
    "\n",
    "# Creating the ranker object\n",
    "ranker = lgb.LGBMRanker(\n",
    "                    objective=\"lambdarank\",\n",
    "                    boosting_type = \"gbdt\",\n",
    "                    n_estimators = 300,\n",
    "                    importance_type = \"gain\",\n",
    "                    metric= \"ndcg\",\n",
    "                    num_leaves = 10,\n",
    "                    learning_rate = 0.05,\n",
    "                    max_depth = 10)\n",
    "                    # label_gain =[i for i in range(max(y_train.max(), y_test.max()) + 1)])\n",
    "\n",
    "# Training the model\n",
    "ranker.fit(\n",
    "      X=X_train_lgb,\n",
    "      y=y_train,\n",
    "      group=group_train,\n",
    "      eval_set=[(X_train_lgb, y_train),(X_val_lgb, y_test)],\n",
    "      eval_group=[group_train, group_val],\n",
    "      eval_at=[5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the scores\n",
    "# test = X_test\n",
    "test = df_test\n",
    "\n",
    "test_input = test.drop(['srch_id'], axis=1)\n",
    "\n",
    "y_pred = ranker.predict(test_input)\n",
    "df_res = test.copy()\n",
    "df_res['pred_grades'] = y_pred\n",
    "df_res = df_res.sort_values(by=['srch_id', 'pred_grades'], ascending=[True, False], inplace=False)\n",
    "# df_res = df_res.merge(test_ideal, on=['srch_id', 'prop_id'], how='left')\n",
    "# df_res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_submission = df_res[['srch_id', 'prop_id']]\n",
    "lgbm_submission.to_csv(config['PATH']['SUBMISSION_DIR'] + '/lgbm_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM: 0.39285307523155705, Random: 0.004541951536113568\n"
     ]
    }
   ],
   "source": [
    "print(f\"LGBM: {calc_NDCG(test_ideal, df_res)}, Random: {calc_NDCG(test_ideal, pred_random)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGB\n"
     ]
    }
   ],
   "source": [
    "# # Random forest\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# print('Training RF')\n",
    "# rf = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "# rf.fit(X_train, y_train)\n",
    "\n",
    "# XGBoost\n",
    "print('Training XGB')\n",
    "import xgboost as xgb\n",
    "y_train_xgb = y_train.astype(int)\n",
    "y_train_xgb[y_train == 5] = 2\n",
    "xgb_model = xgb.XGBClassifier(objective=\"multi:softprob\", random_state=42)\n",
    "xgb_model.fit(X_train, y_train_xgb)\n",
    "\n",
    "# pred_ideal_rf = constructs_predictions(rf, X_test, ideal_df=test_ideal)\n",
    "pred_xgb = constructs_predictions(xgb_model, X_test, ideal_df=test_ideal)\n",
    "pred_random = construct_pred_ideal(X_test, test_ideal, np.random.rand(len(X_test)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>srch_id</th>\n",
       "      <th>prop_id</th>\n",
       "      <th>pred_grades</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>68914</td>\n",
       "      <td>1.333926</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>88218</td>\n",
       "      <td>0.736154</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>97247</td>\n",
       "      <td>0.577588</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>29604</td>\n",
       "      <td>0.575803</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>95307</td>\n",
       "      <td>0.480201</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29766</th>\n",
       "      <td>9988</td>\n",
       "      <td>39329</td>\n",
       "      <td>0.089737</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29767</th>\n",
       "      <td>9988</td>\n",
       "      <td>1495</td>\n",
       "      <td>0.088096</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29768</th>\n",
       "      <td>9988</td>\n",
       "      <td>77411</td>\n",
       "      <td>0.054477</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29769</th>\n",
       "      <td>9988</td>\n",
       "      <td>139360</td>\n",
       "      <td>0.045843</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29770</th>\n",
       "      <td>9988</td>\n",
       "      <td>89367</td>\n",
       "      <td>0.020703</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29771 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       srch_id  prop_id  pred_grades  target\n",
       "0            1    68914     1.333926     5.0\n",
       "1            1    88218     0.736154     0.0\n",
       "2            1    97247     0.577588     0.0\n",
       "3            1    29604     0.575803     0.0\n",
       "4            1    95307     0.480201     0.0\n",
       "...        ...      ...          ...     ...\n",
       "29766     9988    39329     0.089737     0.0\n",
       "29767     9988     1495     0.088096     0.0\n",
       "29768     9988    77411     0.054477     0.0\n",
       "29769     9988   139360     0.045843     0.0\n",
       "29770     9988    89367     0.020703     0.0\n",
       "\n",
       "[29771 rows x 4 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB: 0.3375755506487008, Random: 0.15050172446700524\n"
     ]
    }
   ],
   "source": [
    "# print(f\"RF: {calc_NDCG(test_ideal, pred_ideal_rf)}\\n,XGB: {calc_NDCG(test_ideal, pred_xgb_optimized)},\\nRandom: {calc_NDCG(test_ideal, pred_random)}\")\n",
    "print(f\"XGB: {calc_NDCG(test_ideal, pred_xgb)}, Random: {calc_NDCG(test_ideal, pred_random)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna + XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-09 13:56:31,830]\u001b[0m A new study created in memory with name: no-name-3a7cbff6-7ed5-4fc1-8c20-9778f5cce14a\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-09 13:58:02,635]\u001b[0m Trial 0 finished with value: 0.3422170675116014 and parameters: {'n_estimators': 218, 'max_depth': 10, 'learning_rate': 0.002795642578981349, 'subsample': 0.8932459525721343, 'colsample_bytree': 0.6546014752508442, 'gamma': 0.4545479889258107, 'reg_alpha': 0.0006735472057143736, 'reg_lambda': 0.05659086785788689}. Best is trial 0 with value: 0.3422170675116014.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 13:58:34,110]\u001b[0m Trial 1 finished with value: 0.3296238751532763 and parameters: {'n_estimators': 122, 'max_depth': 5, 'learning_rate': 0.00020205115375924383, 'subsample': 0.6995347755906247, 'colsample_bytree': 0.9885228465832642, 'gamma': 0.19381601429279216, 'reg_alpha': 0.03803815623242628, 'reg_lambda': 0.00015357257740569215}. Best is trial 0 with value: 0.3422170675116014.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 13:59:37,760]\u001b[0m Trial 2 finished with value: 0.3368665578611753 and parameters: {'n_estimators': 472, 'max_depth': 4, 'learning_rate': 0.004549507912707027, 'subsample': 0.9923851580887791, 'colsample_bytree': 0.6814279943069872, 'gamma': 0.7401357393941124, 'reg_alpha': 0.0002258603906050948, 'reg_lambda': 0.028653594053630608}. Best is trial 0 with value: 0.3422170675116014.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:01:29,664]\u001b[0m Trial 3 finished with value: 0.339781517888877 and parameters: {'n_estimators': 306, 'max_depth': 6, 'learning_rate': 0.060275684993274646, 'subsample': 0.620320331182772, 'colsample_bytree': 0.9464989061090534, 'gamma': 0.3525375597606377, 'reg_alpha': 0.0003216130966291681, 'reg_lambda': 0.0005114090643305632}. Best is trial 0 with value: 0.3422170675116014.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:03:28,696]\u001b[0m Trial 4 finished with value: 0.3496082715804649 and parameters: {'n_estimators': 429, 'max_depth': 5, 'learning_rate': 0.06142220536965048, 'subsample': 0.6712997719996967, 'colsample_bytree': 0.8271779419344812, 'gamma': 0.7969399896200798, 'reg_alpha': 0.000677426646577888, 'reg_lambda': 0.03700082394822008}. Best is trial 4 with value: 0.3496082715804649.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:05:12,196]\u001b[0m Trial 5 finished with value: 0.3384792262060793 and parameters: {'n_estimators': 255, 'max_depth': 7, 'learning_rate': 0.04445704548420475, 'subsample': 0.5764517066858506, 'colsample_bytree': 0.8572909403044449, 'gamma': 0.26447303148260715, 'reg_alpha': 0.014856269301417238, 'reg_lambda': 0.06756544690765487}. Best is trial 4 with value: 0.3496082715804649.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:07:47,778]\u001b[0m Trial 6 finished with value: 0.3288661873274956 and parameters: {'n_estimators': 312, 'max_depth': 10, 'learning_rate': 0.06361043068846312, 'subsample': 0.614109694485647, 'colsample_bytree': 0.690990627225202, 'gamma': 0.6320192081820498, 'reg_alpha': 0.00021742258214232484, 'reg_lambda': 0.0005896239122363013}. Best is trial 4 with value: 0.3496082715804649.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:08:27,834]\u001b[0m Trial 7 finished with value: 0.34384549938446224 and parameters: {'n_estimators': 69, 'max_depth': 8, 'learning_rate': 0.0010117902047042774, 'subsample': 0.5509143251305197, 'colsample_bytree': 0.8569236855819127, 'gamma': 0.971456658004933, 'reg_alpha': 0.004542345369774203, 'reg_lambda': 0.003093509800346809}. Best is trial 4 with value: 0.3496082715804649.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:10:40,908]\u001b[0m Trial 8 finished with value: 0.34011673137777976 and parameters: {'n_estimators': 452, 'max_depth': 7, 'learning_rate': 0.0011321008620682482, 'subsample': 0.8295478463576602, 'colsample_bytree': 0.671414737192363, 'gamma': 0.7305115721105879, 'reg_alpha': 0.00012385564006791645, 'reg_lambda': 0.0002558991665383762}. Best is trial 4 with value: 0.3496082715804649.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:12:07,736]\u001b[0m Trial 9 finished with value: 0.3424228910661798 and parameters: {'n_estimators': 269, 'max_depth': 8, 'learning_rate': 0.00037359210176421875, 'subsample': 0.8060751589443202, 'colsample_bytree': 0.5510785207522584, 'gamma': 0.7526724292155883, 'reg_alpha': 0.0009308042088312982, 'reg_lambda': 0.00016796739944054614}. Best is trial 4 with value: 0.3496082715804649.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:12:55,928]\u001b[0m Trial 10 finished with value: 0.3442294127671065 and parameters: {'n_estimators': 386, 'max_depth': 3, 'learning_rate': 0.013364142140460148, 'subsample': 0.708724261611066, 'colsample_bytree': 0.7896130232357127, 'gamma': 0.05618920251021964, 'reg_alpha': 0.0015431363571254905, 'reg_lambda': 0.014196822762253699}. Best is trial 4 with value: 0.3496082715804649.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:13:47,232]\u001b[0m Trial 11 finished with value: 0.3560830490052606 and parameters: {'n_estimators': 392, 'max_depth': 3, 'learning_rate': 0.018998420243927058, 'subsample': 0.7068269457210393, 'colsample_bytree': 0.7846266947658518, 'gamma': 0.05287718678884501, 'reg_alpha': 0.0022645116016606925, 'reg_lambda': 0.013744086612321477}. Best is trial 11 with value: 0.3560830490052606.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:14:39,802]\u001b[0m Trial 12 finished with value: 0.3543543071898208 and parameters: {'n_estimators': 387, 'max_depth': 3, 'learning_rate': 0.018756027574930046, 'subsample': 0.6712932319218762, 'colsample_bytree': 0.780370594379618, 'gamma': 0.5142230603254847, 'reg_alpha': 0.004081559049808262, 'reg_lambda': 0.009443211407634506}. Best is trial 11 with value: 0.3560830490052606.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:15:30,405]\u001b[0m Trial 13 finished with value: 0.3516628290995864 and parameters: {'n_estimators': 361, 'max_depth': 3, 'learning_rate': 0.015102319848196757, 'subsample': 0.5014584532543078, 'colsample_bytree': 0.7617930521271803, 'gamma': 0.010571810108823965, 'reg_alpha': 0.004453410464019551, 'reg_lambda': 0.009540155317686446}. Best is trial 11 with value: 0.3560830490052606.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:16:36,573]\u001b[0m Trial 14 finished with value: 0.3553393333527243 and parameters: {'n_estimators': 396, 'max_depth': 4, 'learning_rate': 0.01648798679985499, 'subsample': 0.7400765255992919, 'colsample_bytree': 0.7472726903550169, 'gamma': 0.47012405874481084, 'reg_alpha': 0.002494784409867919, 'reg_lambda': 0.005039066603505889}. Best is trial 11 with value: 0.3560830490052606.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:17:07,310]\u001b[0m Trial 15 finished with value: 0.33721518793193683 and parameters: {'n_estimators': 194, 'max_depth': 4, 'learning_rate': 0.00876749486753917, 'subsample': 0.7587969640800073, 'colsample_bytree': 0.7225094152765875, 'gamma': 0.20481708761514125, 'reg_alpha': 0.0023349612169283715, 'reg_lambda': 0.0033495501384705265}. Best is trial 11 with value: 0.3560830490052606.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:18:12,175]\u001b[0m Trial 16 finished with value: 0.3586960449767132 and parameters: {'n_estimators': 489, 'max_depth': 4, 'learning_rate': 0.027855876474626316, 'subsample': 0.7527275830005006, 'colsample_bytree': 0.6115010002948956, 'gamma': 0.1062913910208294, 'reg_alpha': 0.011119124655306968, 'reg_lambda': 0.09892315408249078}. Best is trial 16 with value: 0.3586960449767132.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:19:33,635]\u001b[0m Trial 17 finished with value: 0.3531469396569031 and parameters: {'n_estimators': 500, 'max_depth': 5, 'learning_rate': 0.027832715612830964, 'subsample': 0.7777013015138864, 'colsample_bytree': 0.6041003878041254, 'gamma': 0.0997557254325905, 'reg_alpha': 0.016012295331838736, 'reg_lambda': 0.070444644128135}. Best is trial 16 with value: 0.3586960449767132.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:20:35,604]\u001b[0m Trial 18 finished with value: 0.35224771448175324 and parameters: {'n_estimators': 339, 'max_depth': 6, 'learning_rate': 0.027987411838695936, 'subsample': 0.861869338388761, 'colsample_bytree': 0.5065642665059094, 'gamma': 0.11492715330320435, 'reg_alpha': 0.09182530901018138, 'reg_lambda': 0.08885290572015923}. Best is trial 16 with value: 0.3586960449767132.\u001b[0m\n",
      "\u001b[32m[I 2023-05-09 14:21:40,752]\u001b[0m Trial 19 finished with value: 0.35291651561050097 and parameters: {'n_estimators': 430, 'max_depth': 4, 'learning_rate': 0.09529096995868931, 'subsample': 0.7375071931614383, 'colsample_bytree': 0.6244981709223065, 'gamma': 0.03240613404282796, 'reg_alpha': 0.0065337960411954756, 'reg_lambda': 0.02198876587004922}. Best is trial 16 with value: 0.3586960449767132.\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid classes inferred from unique values of `y`.  Expected: [0 1 2], got [0 1 5]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m best_params \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_params\n\u001b[0;32m     42\u001b[0m xgb_model_optimized \u001b[39m=\u001b[39m xgb\u001b[39m.\u001b[39mXGBClassifier(objective\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmulti:softprob\u001b[39m\u001b[39m\"\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbest_params)\n\u001b[1;32m---> 43\u001b[0m xgb_model_optimized\u001b[39m.\u001b[39;49mfit(X_train, y_train\u001b[39m.\u001b[39;49mastype(\u001b[39mint\u001b[39;49m))\n\u001b[0;32m     45\u001b[0m \u001b[39m# Evaluate the optimized model\u001b[39;00m\n\u001b[0;32m     46\u001b[0m pred_xgb_optimized \u001b[39m=\u001b[39m constructs_predictions(xgb_model_optimized, X_test, ideal_df\u001b[39m=\u001b[39mtest_ideal)\n",
      "File \u001b[1;32mc:\\Users\\caspa\\Desktop\\data-mining-techniques-vu\\.venv\\Lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\caspa\\Desktop\\data-mining-techniques-vu\\.venv\\Lib\\site-packages\\xgboost\\sklearn.py:1440\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1435\u001b[0m     expected_classes \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_)\n\u001b[0;32m   1436\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1437\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m expected_classes\u001b[39m.\u001b[39mshape\n\u001b[0;32m   1438\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m==\u001b[39m expected_classes)\u001b[39m.\u001b[39mall()\n\u001b[0;32m   1439\u001b[0m ):\n\u001b[1;32m-> 1440\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1441\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid classes inferred from unique values of `y`.  \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1442\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected: \u001b[39m\u001b[39m{\u001b[39;00mexpected_classes\u001b[39m}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1443\u001b[0m     )\n\u001b[0;32m   1445\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_xgb_params()\n\u001b[0;32m   1447\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid classes inferred from unique values of `y`.  Expected: [0 1 2], got [0 1 5]"
     ]
    }
   ],
   "source": [
    "# Optimize XGB with optuna\n",
    "import optuna\n",
    "from functools import partial\n",
    "\n",
    "def objective(trial, X_train, y_train, X_test, test_ideal):\n",
    "    y_train_xgb = y_train.astype(int)\n",
    "    y_train_xgb[y_train == 5] = 2\n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"multi:softprob\",\n",
    "        \"random_state\": 42,\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0, 1),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-4, 1e-1, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-4, 1e-1, log=True),\n",
    "    }\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier(**params)\n",
    "    xgb_model.fit(X_train, y_train_xgb)\n",
    "\n",
    "    pred_xgb = constructs_predictions(xgb_model, X_test, ideal_df=test_ideal)\n",
    "    ndcg = calc_NDCG(test_ideal, pred_xgb)\n",
    "\n",
    "    return ndcg\n",
    "\n",
    "print(\"Training XGB\")\n",
    "# Assuming you have defined X_train, y_train, X_test, and test_ideal before this point.\n",
    "\n",
    "# Wrap the objective function with the input data\n",
    "objective_with_data = partial(objective, X_train=X_train, y_train=y_train, X_test=X_test, test_ideal=test_ideal)\n",
    "\n",
    "# Create an Optuna study and optimize the objective function\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective_with_data, n_trials=20)\n",
    "\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "y_train_xgb = y_train.astype(int)\n",
    "y_train_xgb[y_train == 5] = 2\n",
    "\n",
    "best_params = study.best_params\n",
    "xgb_model_optimized = xgb.XGBClassifier(objective=\"multi:softprob\", random_state=42, **best_params)\n",
    "xgb_model_optimized.fit(X_train, y_train_xgb)\n",
    "\n",
    "# Evaluate the optimized model\n",
    "pred_xgb_optimized = constructs_predictions(xgb_model_optimized, X_test, ideal_df=test_ideal)\n",
    "pred_xgb_submission = constructs_predictions(xgb_model_optimized, df_test)\n",
    "print(f\"XGB Optimized: {calc_NDCG(test_ideal, pred_xgb_optimized)}\")\n",
    "\n",
    "# pred_submission.to_csv(config['PATH']['DATA_DIR'] + '/submission_RF.csv', index=False)\n",
    "pred_xgb_submission.to_csv(config['PATH']['DATA_DIR'] + '/submission_XGB.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "870a2fdea9c478e83829f6dfe3253b795de45ac3a80f5a95ea9a0c5415ff5b84"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
