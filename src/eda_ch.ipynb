{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Evaluation</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import configparser\n",
    "from tqdm import tqdm\n",
    "from helpers.helper_functions import *\n",
    "from helpers.helper_classes import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', None)\n",
    "os.chdir('/Users/hrvanelderen/Documents/Master/DMT/data-mining-techniques-vu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read config.ini file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('src/config.ini')\n",
    "os.chdir(config['PATH']['ROOT_DIR'])\n",
    "\n",
    "# # Load data\n",
    "df = pd.read_parquet(config['PATH']['INT_DIR'] + '/training_set_preprocessed_nodrop.parquet', engine = 'auto')\n",
    "df_test = pd.read_parquet(config['PATH']['INT_DIR'] + '/test_set_preprocessed_nodrop.parquet', engine = 'auto')\n",
    "df_mini = df[df['srch_id'] < 10000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['hour', 'day', 'month', 'day_of_week', 'site_id', 'visitor_location_country_id', 'prop_country_id', 'prop_id', 'srch_destination_id']\n",
    "\n",
    "for c in categorical_features:\n",
    "    df[c] = df[c].astype('category')\n",
    "    df_test[c] = df_test[c].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's ndcg@5: 0.32275\n",
      "[2]\ttraining's ndcg@5: 0.35759\n",
      "[3]\ttraining's ndcg@5: 0.365995\n",
      "[4]\ttraining's ndcg@5: 0.37607\n",
      "[5]\ttraining's ndcg@5: 0.384951\n",
      "[6]\ttraining's ndcg@5: 0.387053\n",
      "[7]\ttraining's ndcg@5: 0.387839\n",
      "[8]\ttraining's ndcg@5: 0.393158\n",
      "[9]\ttraining's ndcg@5: 0.393664\n",
      "[10]\ttraining's ndcg@5: 0.397127\n",
      "[11]\ttraining's ndcg@5: 0.4004\n",
      "[12]\ttraining's ndcg@5: 0.402784\n",
      "[13]\ttraining's ndcg@5: 0.403729\n",
      "[14]\ttraining's ndcg@5: 0.405827\n",
      "[15]\ttraining's ndcg@5: 0.408048\n",
      "[16]\ttraining's ndcg@5: 0.408987\n",
      "[17]\ttraining's ndcg@5: 0.4092\n",
      "[18]\ttraining's ndcg@5: 0.409163\n",
      "[19]\ttraining's ndcg@5: 0.411067\n",
      "[20]\ttraining's ndcg@5: 0.413123\n",
      "[21]\ttraining's ndcg@5: 0.414839\n",
      "[22]\ttraining's ndcg@5: 0.415175\n",
      "[23]\ttraining's ndcg@5: 0.415292\n",
      "[24]\ttraining's ndcg@5: 0.416967\n",
      "[25]\ttraining's ndcg@5: 0.417142\n",
      "[26]\ttraining's ndcg@5: 0.417607\n",
      "[27]\ttraining's ndcg@5: 0.418991\n",
      "[28]\ttraining's ndcg@5: 0.41938\n",
      "[29]\ttraining's ndcg@5: 0.419342\n",
      "[30]\ttraining's ndcg@5: 0.419123\n",
      "[31]\ttraining's ndcg@5: 0.419396\n",
      "[32]\ttraining's ndcg@5: 0.420643\n",
      "[33]\ttraining's ndcg@5: 0.420846\n",
      "[34]\ttraining's ndcg@5: 0.420992\n",
      "[35]\ttraining's ndcg@5: 0.422369\n",
      "[36]\ttraining's ndcg@5: 0.423599\n",
      "[37]\ttraining's ndcg@5: 0.423626\n",
      "[38]\ttraining's ndcg@5: 0.423722\n",
      "[39]\ttraining's ndcg@5: 0.423628\n",
      "[40]\ttraining's ndcg@5: 0.424046\n",
      "[41]\ttraining's ndcg@5: 0.424109\n",
      "[42]\ttraining's ndcg@5: 0.424281\n",
      "[43]\ttraining's ndcg@5: 0.425882\n",
      "[44]\ttraining's ndcg@5: 0.425963\n",
      "[45]\ttraining's ndcg@5: 0.426324\n",
      "[46]\ttraining's ndcg@5: 0.427261\n",
      "[47]\ttraining's ndcg@5: 0.427279\n",
      "[48]\ttraining's ndcg@5: 0.427244\n",
      "[49]\ttraining's ndcg@5: 0.428518\n",
      "[50]\ttraining's ndcg@5: 0.429653\n",
      "[51]\ttraining's ndcg@5: 0.430801\n",
      "[52]\ttraining's ndcg@5: 0.431464\n",
      "[53]\ttraining's ndcg@5: 0.431515\n",
      "[54]\ttraining's ndcg@5: 0.432529\n",
      "[55]\ttraining's ndcg@5: 0.433782\n",
      "[56]\ttraining's ndcg@5: 0.434777\n",
      "[57]\ttraining's ndcg@5: 0.434813\n",
      "[58]\ttraining's ndcg@5: 0.434843\n",
      "[59]\ttraining's ndcg@5: 0.435903\n",
      "[60]\ttraining's ndcg@5: 0.435945\n",
      "[61]\ttraining's ndcg@5: 0.436225\n",
      "[62]\ttraining's ndcg@5: 0.436304\n",
      "[63]\ttraining's ndcg@5: 0.43729\n",
      "[64]\ttraining's ndcg@5: 0.437603\n",
      "[65]\ttraining's ndcg@5: 0.438343\n",
      "[66]\ttraining's ndcg@5: 0.439067\n",
      "[67]\ttraining's ndcg@5: 0.43986\n",
      "[68]\ttraining's ndcg@5: 0.440198\n",
      "[69]\ttraining's ndcg@5: 0.440689\n",
      "[70]\ttraining's ndcg@5: 0.441644\n",
      "[71]\ttraining's ndcg@5: 0.441797\n",
      "[72]\ttraining's ndcg@5: 0.442457\n",
      "[73]\ttraining's ndcg@5: 0.443185\n",
      "[74]\ttraining's ndcg@5: 0.443174\n",
      "[75]\ttraining's ndcg@5: 0.443249\n",
      "[76]\ttraining's ndcg@5: 0.443186\n",
      "[77]\ttraining's ndcg@5: 0.444179\n",
      "[78]\ttraining's ndcg@5: 0.444295\n",
      "[79]\ttraining's ndcg@5: 0.444877\n",
      "[80]\ttraining's ndcg@5: 0.44522\n",
      "[81]\ttraining's ndcg@5: 0.445286\n",
      "[82]\ttraining's ndcg@5: 0.445513\n",
      "[83]\ttraining's ndcg@5: 0.446042\n",
      "[84]\ttraining's ndcg@5: 0.445955\n",
      "[85]\ttraining's ndcg@5: 0.44609\n",
      "[86]\ttraining's ndcg@5: 0.446181\n",
      "[87]\ttraining's ndcg@5: 0.446985\n",
      "[88]\ttraining's ndcg@5: 0.447029\n",
      "[89]\ttraining's ndcg@5: 0.447042\n",
      "[90]\ttraining's ndcg@5: 0.44742\n",
      "[91]\ttraining's ndcg@5: 0.448034\n",
      "[92]\ttraining's ndcg@5: 0.44818\n",
      "[93]\ttraining's ndcg@5: 0.448499\n",
      "[94]\ttraining's ndcg@5: 0.448676\n",
      "[95]\ttraining's ndcg@5: 0.448891\n",
      "[96]\ttraining's ndcg@5: 0.448919\n",
      "[97]\ttraining's ndcg@5: 0.449481\n",
      "[98]\ttraining's ndcg@5: 0.449624\n",
      "[99]\ttraining's ndcg@5: 0.450196\n",
      "[100]\ttraining's ndcg@5: 0.450328\n",
      "[101]\ttraining's ndcg@5: 0.450742\n",
      "[102]\ttraining's ndcg@5: 0.45107\n",
      "[103]\ttraining's ndcg@5: 0.451509\n",
      "[104]\ttraining's ndcg@5: 0.451575\n",
      "[105]\ttraining's ndcg@5: 0.451577\n",
      "[106]\ttraining's ndcg@5: 0.451577\n",
      "[107]\ttraining's ndcg@5: 0.452062\n",
      "[108]\ttraining's ndcg@5: 0.452861\n",
      "[109]\ttraining's ndcg@5: 0.452985\n",
      "[110]\ttraining's ndcg@5: 0.452962\n",
      "[111]\ttraining's ndcg@5: 0.453236\n",
      "[112]\ttraining's ndcg@5: 0.453604\n",
      "[113]\ttraining's ndcg@5: 0.453675\n",
      "[114]\ttraining's ndcg@5: 0.453715\n",
      "[115]\ttraining's ndcg@5: 0.453783\n",
      "[116]\ttraining's ndcg@5: 0.45394\n",
      "[117]\ttraining's ndcg@5: 0.454298\n",
      "[118]\ttraining's ndcg@5: 0.454411\n",
      "[119]\ttraining's ndcg@5: 0.454702\n",
      "[120]\ttraining's ndcg@5: 0.454684\n",
      "[121]\ttraining's ndcg@5: 0.454958\n",
      "[122]\ttraining's ndcg@5: 0.454985\n",
      "[123]\ttraining's ndcg@5: 0.455126\n",
      "[124]\ttraining's ndcg@5: 0.455395\n",
      "[125]\ttraining's ndcg@5: 0.455545\n",
      "[126]\ttraining's ndcg@5: 0.455707\n",
      "[127]\ttraining's ndcg@5: 0.45609\n",
      "[128]\ttraining's ndcg@5: 0.45606\n",
      "[129]\ttraining's ndcg@5: 0.456288\n",
      "[130]\ttraining's ndcg@5: 0.456601\n",
      "[131]\ttraining's ndcg@5: 0.456836\n",
      "[132]\ttraining's ndcg@5: 0.456807\n",
      "[133]\ttraining's ndcg@5: 0.456986\n",
      "[134]\ttraining's ndcg@5: 0.457205\n",
      "[135]\ttraining's ndcg@5: 0.457276\n",
      "[136]\ttraining's ndcg@5: 0.457554\n",
      "[137]\ttraining's ndcg@5: 0.45786\n",
      "[138]\ttraining's ndcg@5: 0.458118\n",
      "[139]\ttraining's ndcg@5: 0.458263\n",
      "[140]\ttraining's ndcg@5: 0.458482\n",
      "[141]\ttraining's ndcg@5: 0.458504\n",
      "[142]\ttraining's ndcg@5: 0.458577\n",
      "[143]\ttraining's ndcg@5: 0.458763\n",
      "[144]\ttraining's ndcg@5: 0.458787\n",
      "[145]\ttraining's ndcg@5: 0.458922\n",
      "[146]\ttraining's ndcg@5: 0.459\n",
      "[147]\ttraining's ndcg@5: 0.459201\n",
      "[148]\ttraining's ndcg@5: 0.459413\n",
      "[149]\ttraining's ndcg@5: 0.459566\n",
      "[150]\ttraining's ndcg@5: 0.45977\n",
      "[151]\ttraining's ndcg@5: 0.460005\n",
      "[152]\ttraining's ndcg@5: 0.460097\n",
      "[153]\ttraining's ndcg@5: 0.460077\n",
      "[154]\ttraining's ndcg@5: 0.460253\n",
      "[155]\ttraining's ndcg@5: 0.460263\n",
      "[156]\ttraining's ndcg@5: 0.460352\n",
      "[157]\ttraining's ndcg@5: 0.460554\n",
      "[158]\ttraining's ndcg@5: 0.460761\n",
      "[159]\ttraining's ndcg@5: 0.46087\n",
      "[160]\ttraining's ndcg@5: 0.461001\n",
      "[161]\ttraining's ndcg@5: 0.461271\n",
      "[162]\ttraining's ndcg@5: 0.461468\n",
      "[163]\ttraining's ndcg@5: 0.461568\n",
      "[164]\ttraining's ndcg@5: 0.46178\n",
      "[165]\ttraining's ndcg@5: 0.461935\n",
      "[166]\ttraining's ndcg@5: 0.461962\n",
      "[167]\ttraining's ndcg@5: 0.462098\n",
      "[168]\ttraining's ndcg@5: 0.462128\n",
      "[169]\ttraining's ndcg@5: 0.462072\n",
      "[170]\ttraining's ndcg@5: 0.462245\n",
      "[171]\ttraining's ndcg@5: 0.462452\n",
      "[172]\ttraining's ndcg@5: 0.462553\n",
      "[173]\ttraining's ndcg@5: 0.46276\n",
      "[174]\ttraining's ndcg@5: 0.462902\n",
      "[175]\ttraining's ndcg@5: 0.463104\n",
      "[176]\ttraining's ndcg@5: 0.463281\n",
      "[177]\ttraining's ndcg@5: 0.463456\n",
      "[178]\ttraining's ndcg@5: 0.463518\n",
      "[179]\ttraining's ndcg@5: 0.463706\n",
      "[180]\ttraining's ndcg@5: 0.463724\n",
      "[181]\ttraining's ndcg@5: 0.463855\n",
      "[182]\ttraining's ndcg@5: 0.464061\n",
      "[183]\ttraining's ndcg@5: 0.464157\n",
      "[184]\ttraining's ndcg@5: 0.464201\n",
      "[185]\ttraining's ndcg@5: 0.464305\n",
      "[186]\ttraining's ndcg@5: 0.46449\n",
      "[187]\ttraining's ndcg@5: 0.464549\n",
      "[188]\ttraining's ndcg@5: 0.464676\n",
      "[189]\ttraining's ndcg@5: 0.464845\n",
      "[190]\ttraining's ndcg@5: 0.464907\n",
      "[191]\ttraining's ndcg@5: 0.464848\n",
      "[192]\ttraining's ndcg@5: 0.465022\n",
      "[193]\ttraining's ndcg@5: 0.465038\n",
      "[194]\ttraining's ndcg@5: 0.465088\n",
      "[195]\ttraining's ndcg@5: 0.465132\n",
      "[196]\ttraining's ndcg@5: 0.465227\n",
      "[197]\ttraining's ndcg@5: 0.465347\n",
      "[198]\ttraining's ndcg@5: 0.465462\n",
      "[199]\ttraining's ndcg@5: 0.4656\n",
      "[200]\ttraining's ndcg@5: 0.465592\n",
      "[201]\ttraining's ndcg@5: 0.465856\n",
      "[202]\ttraining's ndcg@5: 0.466004\n",
      "[203]\ttraining's ndcg@5: 0.466169\n",
      "[204]\ttraining's ndcg@5: 0.466365\n",
      "[205]\ttraining's ndcg@5: 0.466506\n",
      "[206]\ttraining's ndcg@5: 0.466669\n",
      "[207]\ttraining's ndcg@5: 0.466785\n",
      "[208]\ttraining's ndcg@5: 0.466911\n",
      "[209]\ttraining's ndcg@5: 0.466997\n",
      "[210]\ttraining's ndcg@5: 0.467162\n",
      "[211]\ttraining's ndcg@5: 0.4672\n",
      "[212]\ttraining's ndcg@5: 0.467379\n",
      "[213]\ttraining's ndcg@5: 0.467404\n",
      "[214]\ttraining's ndcg@5: 0.467581\n",
      "[215]\ttraining's ndcg@5: 0.467754\n",
      "[216]\ttraining's ndcg@5: 0.467887\n",
      "[217]\ttraining's ndcg@5: 0.467867\n",
      "[218]\ttraining's ndcg@5: 0.467949\n",
      "[219]\ttraining's ndcg@5: 0.467939\n",
      "[220]\ttraining's ndcg@5: 0.46812\n",
      "[221]\ttraining's ndcg@5: 0.468245\n",
      "[222]\ttraining's ndcg@5: 0.468339\n",
      "[223]\ttraining's ndcg@5: 0.46854\n",
      "[224]\ttraining's ndcg@5: 0.468675\n",
      "[225]\ttraining's ndcg@5: 0.468813\n",
      "[226]\ttraining's ndcg@5: 0.468942\n",
      "[227]\ttraining's ndcg@5: 0.469084\n",
      "[228]\ttraining's ndcg@5: 0.469237\n",
      "[229]\ttraining's ndcg@5: 0.469334\n",
      "[230]\ttraining's ndcg@5: 0.46945\n",
      "[231]\ttraining's ndcg@5: 0.469574\n",
      "[232]\ttraining's ndcg@5: 0.469734\n",
      "[233]\ttraining's ndcg@5: 0.469933\n",
      "[234]\ttraining's ndcg@5: 0.470077\n",
      "[235]\ttraining's ndcg@5: 0.470119\n",
      "[236]\ttraining's ndcg@5: 0.470205\n",
      "[237]\ttraining's ndcg@5: 0.470319\n",
      "[238]\ttraining's ndcg@5: 0.470534\n",
      "[239]\ttraining's ndcg@5: 0.470633\n",
      "[240]\ttraining's ndcg@5: 0.470548\n",
      "[241]\ttraining's ndcg@5: 0.470688\n",
      "[242]\ttraining's ndcg@5: 0.470764\n",
      "[243]\ttraining's ndcg@5: 0.470936\n",
      "[244]\ttraining's ndcg@5: 0.47106\n",
      "[245]\ttraining's ndcg@5: 0.471207\n",
      "[246]\ttraining's ndcg@5: 0.471315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[247]\ttraining's ndcg@5: 0.471455\n",
      "[248]\ttraining's ndcg@5: 0.471511\n",
      "[249]\ttraining's ndcg@5: 0.471667\n",
      "[250]\ttraining's ndcg@5: 0.471806\n",
      "[251]\ttraining's ndcg@5: 0.471945\n",
      "[252]\ttraining's ndcg@5: 0.471964\n",
      "[253]\ttraining's ndcg@5: 0.472125\n",
      "[254]\ttraining's ndcg@5: 0.472196\n",
      "[255]\ttraining's ndcg@5: 0.472276\n",
      "[256]\ttraining's ndcg@5: 0.47247\n",
      "[257]\ttraining's ndcg@5: 0.472577\n",
      "[258]\ttraining's ndcg@5: 0.472552\n",
      "[259]\ttraining's ndcg@5: 0.472638\n",
      "[260]\ttraining's ndcg@5: 0.472785\n",
      "[261]\ttraining's ndcg@5: 0.472948\n",
      "[262]\ttraining's ndcg@5: 0.473048\n",
      "[263]\ttraining's ndcg@5: 0.473195\n",
      "[264]\ttraining's ndcg@5: 0.4733\n",
      "[265]\ttraining's ndcg@5: 0.473471\n",
      "[266]\ttraining's ndcg@5: 0.473535\n",
      "[267]\ttraining's ndcg@5: 0.47365\n",
      "[268]\ttraining's ndcg@5: 0.473703\n",
      "[269]\ttraining's ndcg@5: 0.473819\n",
      "[270]\ttraining's ndcg@5: 0.473908\n",
      "[271]\ttraining's ndcg@5: 0.474083\n",
      "[272]\ttraining's ndcg@5: 0.474218\n",
      "[273]\ttraining's ndcg@5: 0.47423\n",
      "[274]\ttraining's ndcg@5: 0.474398\n",
      "[275]\ttraining's ndcg@5: 0.474505\n",
      "[276]\ttraining's ndcg@5: 0.474584\n",
      "[277]\ttraining's ndcg@5: 0.474719\n",
      "[278]\ttraining's ndcg@5: 0.474922\n",
      "[279]\ttraining's ndcg@5: 0.475002\n",
      "[280]\ttraining's ndcg@5: 0.475021\n",
      "[281]\ttraining's ndcg@5: 0.475209\n",
      "[282]\ttraining's ndcg@5: 0.475354\n",
      "[283]\ttraining's ndcg@5: 0.475461\n",
      "[284]\ttraining's ndcg@5: 0.475598\n",
      "[285]\ttraining's ndcg@5: 0.475765\n",
      "[286]\ttraining's ndcg@5: 0.475751\n",
      "[287]\ttraining's ndcg@5: 0.475916\n",
      "[288]\ttraining's ndcg@5: 0.475964\n",
      "[289]\ttraining's ndcg@5: 0.476025\n",
      "[290]\ttraining's ndcg@5: 0.476155\n",
      "[291]\ttraining's ndcg@5: 0.476285\n",
      "[292]\ttraining's ndcg@5: 0.476416\n",
      "[293]\ttraining's ndcg@5: 0.47656\n",
      "[294]\ttraining's ndcg@5: 0.476612\n",
      "[295]\ttraining's ndcg@5: 0.476824\n",
      "[296]\ttraining's ndcg@5: 0.47685\n",
      "[297]\ttraining's ndcg@5: 0.476965\n",
      "[298]\ttraining's ndcg@5: 0.477109\n",
      "[299]\ttraining's ndcg@5: 0.477183\n",
      "[300]\ttraining's ndcg@5: 0.477247\n",
      "[301]\ttraining's ndcg@5: 0.477315\n",
      "[302]\ttraining's ndcg@5: 0.477461\n",
      "[303]\ttraining's ndcg@5: 0.477551\n",
      "[304]\ttraining's ndcg@5: 0.477666\n",
      "[305]\ttraining's ndcg@5: 0.477704\n",
      "[306]\ttraining's ndcg@5: 0.47768\n",
      "[307]\ttraining's ndcg@5: 0.477842\n",
      "[308]\ttraining's ndcg@5: 0.477946\n",
      "[309]\ttraining's ndcg@5: 0.478077\n",
      "[310]\ttraining's ndcg@5: 0.47818\n",
      "[311]\ttraining's ndcg@5: 0.478273\n",
      "[312]\ttraining's ndcg@5: 0.478347\n",
      "[313]\ttraining's ndcg@5: 0.478391\n",
      "[314]\ttraining's ndcg@5: 0.478544\n",
      "[315]\ttraining's ndcg@5: 0.478604\n",
      "[316]\ttraining's ndcg@5: 0.478675\n",
      "[317]\ttraining's ndcg@5: 0.478751\n",
      "[318]\ttraining's ndcg@5: 0.478876\n",
      "[319]\ttraining's ndcg@5: 0.478919\n",
      "[320]\ttraining's ndcg@5: 0.479005\n",
      "[321]\ttraining's ndcg@5: 0.479173\n",
      "[322]\ttraining's ndcg@5: 0.479302\n",
      "[323]\ttraining's ndcg@5: 0.47948\n",
      "[324]\ttraining's ndcg@5: 0.479586\n",
      "[325]\ttraining's ndcg@5: 0.479719\n",
      "[326]\ttraining's ndcg@5: 0.479821\n",
      "[327]\ttraining's ndcg@5: 0.479914\n",
      "[328]\ttraining's ndcg@5: 0.480079\n",
      "[329]\ttraining's ndcg@5: 0.480174\n",
      "[330]\ttraining's ndcg@5: 0.480266\n",
      "[331]\ttraining's ndcg@5: 0.480415\n",
      "[332]\ttraining's ndcg@5: 0.480494\n",
      "[333]\ttraining's ndcg@5: 0.480587\n",
      "[334]\ttraining's ndcg@5: 0.480708\n",
      "[335]\ttraining's ndcg@5: 0.480819\n",
      "[336]\ttraining's ndcg@5: 0.480948\n",
      "[337]\ttraining's ndcg@5: 0.481059\n",
      "[338]\ttraining's ndcg@5: 0.481177\n",
      "[339]\ttraining's ndcg@5: 0.481308\n",
      "[340]\ttraining's ndcg@5: 0.481416\n",
      "[341]\ttraining's ndcg@5: 0.481491\n",
      "[342]\ttraining's ndcg@5: 0.481636\n",
      "[343]\ttraining's ndcg@5: 0.481719\n",
      "[344]\ttraining's ndcg@5: 0.481834\n",
      "[345]\ttraining's ndcg@5: 0.481934\n",
      "[346]\ttraining's ndcg@5: 0.482093\n",
      "[347]\ttraining's ndcg@5: 0.482155\n",
      "[348]\ttraining's ndcg@5: 0.48221\n",
      "[349]\ttraining's ndcg@5: 0.482173\n",
      "[350]\ttraining's ndcg@5: 0.482243\n",
      "[351]\ttraining's ndcg@5: 0.482384\n",
      "[352]\ttraining's ndcg@5: 0.482536\n",
      "[353]\ttraining's ndcg@5: 0.482637\n",
      "[354]\ttraining's ndcg@5: 0.482809\n",
      "[355]\ttraining's ndcg@5: 0.482935\n",
      "[356]\ttraining's ndcg@5: 0.482963\n",
      "[357]\ttraining's ndcg@5: 0.483071\n",
      "[358]\ttraining's ndcg@5: 0.483091\n",
      "[359]\ttraining's ndcg@5: 0.483215\n",
      "[360]\ttraining's ndcg@5: 0.483311\n",
      "[361]\ttraining's ndcg@5: 0.483443\n",
      "[362]\ttraining's ndcg@5: 0.48354\n",
      "[363]\ttraining's ndcg@5: 0.483665\n",
      "[364]\ttraining's ndcg@5: 0.483788\n",
      "[365]\ttraining's ndcg@5: 0.483894\n",
      "[366]\ttraining's ndcg@5: 0.484006\n",
      "[367]\ttraining's ndcg@5: 0.484098\n",
      "[368]\ttraining's ndcg@5: 0.484191\n",
      "[369]\ttraining's ndcg@5: 0.484306\n",
      "[370]\ttraining's ndcg@5: 0.484325\n",
      "[371]\ttraining's ndcg@5: 0.4844\n",
      "[372]\ttraining's ndcg@5: 0.484501\n",
      "[373]\ttraining's ndcg@5: 0.48466\n",
      "[374]\ttraining's ndcg@5: 0.484792\n",
      "[375]\ttraining's ndcg@5: 0.484913\n",
      "[376]\ttraining's ndcg@5: 0.485004\n",
      "[377]\ttraining's ndcg@5: 0.48509\n",
      "[378]\ttraining's ndcg@5: 0.485241\n",
      "[379]\ttraining's ndcg@5: 0.485355\n",
      "[380]\ttraining's ndcg@5: 0.485485\n",
      "[381]\ttraining's ndcg@5: 0.485591\n",
      "[382]\ttraining's ndcg@5: 0.485712\n",
      "[383]\ttraining's ndcg@5: 0.485738\n",
      "[384]\ttraining's ndcg@5: 0.485831\n",
      "[385]\ttraining's ndcg@5: 0.485914\n",
      "[386]\ttraining's ndcg@5: 0.485996\n",
      "[387]\ttraining's ndcg@5: 0.486109\n",
      "[388]\ttraining's ndcg@5: 0.486172\n",
      "[389]\ttraining's ndcg@5: 0.486275\n",
      "[390]\ttraining's ndcg@5: 0.486358\n",
      "[391]\ttraining's ndcg@5: 0.486484\n",
      "[392]\ttraining's ndcg@5: 0.486577\n",
      "[393]\ttraining's ndcg@5: 0.486663\n",
      "[394]\ttraining's ndcg@5: 0.48676\n",
      "[395]\ttraining's ndcg@5: 0.486829\n",
      "[396]\ttraining's ndcg@5: 0.486924\n",
      "[397]\ttraining's ndcg@5: 0.487065\n",
      "[398]\ttraining's ndcg@5: 0.487118\n",
      "[399]\ttraining's ndcg@5: 0.487202\n",
      "[400]\ttraining's ndcg@5: 0.487232\n",
      "[401]\ttraining's ndcg@5: 0.487368\n",
      "[402]\ttraining's ndcg@5: 0.487426\n",
      "[403]\ttraining's ndcg@5: 0.487582\n",
      "[404]\ttraining's ndcg@5: 0.487657\n",
      "[405]\ttraining's ndcg@5: 0.487765\n",
      "[406]\ttraining's ndcg@5: 0.487852\n",
      "[407]\ttraining's ndcg@5: 0.488011\n",
      "[408]\ttraining's ndcg@5: 0.488097\n",
      "[409]\ttraining's ndcg@5: 0.48818\n",
      "[410]\ttraining's ndcg@5: 0.488259\n",
      "[411]\ttraining's ndcg@5: 0.488294\n",
      "[412]\ttraining's ndcg@5: 0.48844\n",
      "[413]\ttraining's ndcg@5: 0.488526\n",
      "[414]\ttraining's ndcg@5: 0.4886\n",
      "[415]\ttraining's ndcg@5: 0.488709\n",
      "[416]\ttraining's ndcg@5: 0.488738\n",
      "[417]\ttraining's ndcg@5: 0.488776\n",
      "[418]\ttraining's ndcg@5: 0.48891\n",
      "[419]\ttraining's ndcg@5: 0.489014\n",
      "[420]\ttraining's ndcg@5: 0.489065\n",
      "[421]\ttraining's ndcg@5: 0.489147\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMRanker(colsample_bytree=0.44512901152675777,\n",
       "           learning_rate=0.07500274597972945, max_depth=9, min_child_samples=96,\n",
       "           min_child_weight=0.06539586276577038, n_estimators=421,\n",
       "           num_leaves=36, reg_alpha=0.13399352212166932,\n",
       "           reg_lambda=0.13466379319544275, subsample=0.44073175180110147)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMRanker</label><div class=\"sk-toggleable__content\"><pre>LGBMRanker(colsample_bytree=0.44512901152675777,\n",
       "           learning_rate=0.07500274597972945, max_depth=9, min_child_samples=96,\n",
       "           min_child_weight=0.06539586276577038, n_estimators=421,\n",
       "           num_leaves=36, reg_alpha=0.13399352212166932,\n",
       "           reg_lambda=0.13466379319544275, subsample=0.44073175180110147)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMRanker(colsample_bytree=0.44512901152675777,\n",
       "           learning_rate=0.07500274597972945, max_depth=9, min_child_samples=96,\n",
       "           min_child_weight=0.06539586276577038, n_estimators=421,\n",
       "           num_leaves=36, reg_alpha=0.13399352212166932,\n",
       "           reg_lambda=0.13466379319544275, subsample=0.44073175180110147)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Best run\n",
    "\n",
    "best_params = {'n_estimators': 421, \n",
    " 'num_leaves': 36, \n",
    " 'max_depth': 9, \n",
    " 'learning_rate': 0.07500274597972945, \n",
    " 'subsample': 0.44073175180110147, \n",
    " 'colsample_bytree': 0.44512901152675777, \n",
    " 'reg_alpha': 0.13399352212166932, \n",
    " 'reg_lambda': 0.13466379319544275, \n",
    " 'min_child_samples': 96, \n",
    " 'min_child_weight': 0.06539586276577038, \n",
    " 'val_size': 0.3154338751135015}\n",
    "\n",
    "# all but val_size in lgb_best_params\n",
    "val_size = best_params.pop('val_size')\n",
    "\n",
    "X_train_full, X_val_full, y_train_full, y_val_full, _ = train_test_split(df, 'target', test_size=val_size)\n",
    "\n",
    "_, desire_df_click_full = construct_desire(X_val_full)\n",
    "_, desire_df_book_full = construct_desire(X_val_full, target = 'booking_bool')\n",
    "\n",
    "prop_counts = X_val_full['prop_id'].value_counts()\n",
    "prop_counts.name = 'prop_counts'\n",
    "prop_counts = pd.DataFrame({'prop_id':prop_counts.index, 'count':prop_counts.values})\n",
    "\n",
    "srch_dest_counts = X_val_full['srch_destination_id'].value_counts()\n",
    "srch_dest_counts.name = 'srch_dest_counts'\n",
    "srch_dest_counts = pd.DataFrame({'srch_destination_id':srch_dest_counts.index, 'count':srch_dest_counts.values})\n",
    "\n",
    "merge_df_list = [(desire_df_click_full, 'prop_id'), (desire_df_book_full, 'prop_id'), (prop_counts, 'prop_id'), (srch_dest_counts, 'srch_destination_id')]   \n",
    "\n",
    "X_train_full = merge_and_drop(X_train_full, merge_df_list)\n",
    "df_test = merge_and_drop(df_test, merge_df_list, drop=False)\n",
    "X_val_full.drop(['click_bool', 'booking_bool'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "group_train = X_train_full.groupby('srch_id').size().values\n",
    "X_train_lgb = X_train_full.drop(['srch_id'], axis=1)\n",
    "# X_val_lgb = X_test.drop(['srch_id'], axis=1)\n",
    "\n",
    "ranker = lgb.LGBMRanker(**best_params)\n",
    "\n",
    "# Training the model\n",
    "ranker.fit(\n",
    "      X=X_train_lgb,\n",
    "      y=y_train_full,\n",
    "      group=group_train,\n",
    "      eval_set=[(X_train_lgb, y_train_full)],\n",
    "      eval_group=[group_train],\n",
    "      eval_at=[5],\n",
    "      feature_name='auto', \n",
    "      categorical_feature = 'auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting...\n",
      "Done predicting\n"
     ]
    }
   ],
   "source": [
    "# Predicting the scores\n",
    "# test = X_val\n",
    "test = df_test\n",
    "test_input = test.drop(['srch_id'], axis=1)\n",
    "df_res = test\n",
    "\n",
    "for c in categorical_features:\n",
    "    test_input[c] = test_input[c].astype('category')\n",
    "\n",
    "print(\"Predicting...\")\n",
    "y_pred = ranker.predict(test_input)\n",
    "df_res['pred_grades'] = y_pred\n",
    "print(\"Done predicting\")\n",
    "\n",
    "df_res = df_res.sort_values(by=['srch_id', 'pred_grades'], ascending=[True, False], inplace=False)\n",
    "\n",
    "df_res\n",
    "lgbm_submission_desire = df_res[['srch_id', 'prop_id']]\n",
    "lgbm_submission_desire.to_csv(config['PATH']['SUBMISSION_DIR'] + '/lgbm_submission_categorical_opt.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_importance(ranker, figsize = (12,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hrvanelderen/Documents/Master/DMT/data-mining-techniques-vu/src/helpers/helper_functions.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test.sort_values(by=['srch_id', target_str], ascending=[True, False], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "best_params = {'n_estimators': 421, \n",
    " 'num_leaves': 36, \n",
    " 'max_depth': 9, \n",
    " 'learning_rate': 0.07500274597972945, \n",
    " 'subsample': 0.44073175180110147, \n",
    " 'colsample_bytree': 0.44512901152675777, \n",
    " 'reg_alpha': 0.13399352212166932, \n",
    " 'reg_lambda': 0.13466379319544275, \n",
    " 'min_child_samples': 96, \n",
    " 'min_child_weight': 0.06539586276577038, \n",
    " 'val_size': 0.3154338751135015}\n",
    "               \n",
    "lgb_params = best_params.copy()\n",
    "val_size = lgb_params.pop('val_size')\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, test_ideal = train_val_test_split(df, 'target', test_size=.15, val_size=val_size, random_state=7)\n",
    "\n",
    "_, desire_df_click = construct_desire(X_val)\n",
    "_, desire_df_book = construct_desire(X_val, target = 'booking_bool')\n",
    "\n",
    "prop_counts = X_val['prop_id'].value_counts()\n",
    "prop_counts.name = 'prop_counts'\n",
    "prop_counts = pd.DataFrame({'prop_id':prop_counts.index, 'count':prop_counts.values})\n",
    "\n",
    "srch_dest_counts = X_val['srch_destination_id'].value_counts()\n",
    "srch_dest_counts.name = 'srch_dest_counts'\n",
    "srch_dest_counts = pd.DataFrame({'srch_destination_id':srch_dest_counts.index, 'count':srch_dest_counts.values})\n",
    "\n",
    "merge_df_list = [(desire_df_click, 'prop_id'), (desire_df_book, 'prop_id'), (prop_counts, 'prop_id'), (srch_dest_counts, 'srch_destination_id')]   \n",
    "\n",
    "X_train = merge_and_drop(X_train, merge_df_list)\n",
    "X_test = merge_and_drop(X_test, merge_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hrvanelderen/anaconda3/lib/python3.10/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/Users/hrvanelderen/anaconda3/lib/python3.10/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's ndcg@5: 0.37622\tvalid_1's ndcg@5: 0.370568\n",
      "[2]\ttraining's ndcg@5: 0.37095\tvalid_1's ndcg@5: 0.361069\n",
      "[3]\ttraining's ndcg@5: 0.372393\tvalid_1's ndcg@5: 0.361603\n",
      "[4]\ttraining's ndcg@5: 0.381609\tvalid_1's ndcg@5: 0.365202\n",
      "[5]\ttraining's ndcg@5: 0.387045\tvalid_1's ndcg@5: 0.366969\n",
      "[6]\ttraining's ndcg@5: 0.388141\tvalid_1's ndcg@5: 0.369443\n",
      "[7]\ttraining's ndcg@5: 0.389023\tvalid_1's ndcg@5: 0.370057\n",
      "[8]\ttraining's ndcg@5: 0.394476\tvalid_1's ndcg@5: 0.372953\n",
      "[9]\ttraining's ndcg@5: 0.39474\tvalid_1's ndcg@5: 0.373361\n",
      "[10]\ttraining's ndcg@5: 0.398864\tvalid_1's ndcg@5: 0.373979\n",
      "[11]\ttraining's ndcg@5: 0.401249\tvalid_1's ndcg@5: 0.374627\n",
      "[12]\ttraining's ndcg@5: 0.405009\tvalid_1's ndcg@5: 0.376207\n",
      "[13]\ttraining's ndcg@5: 0.405355\tvalid_1's ndcg@5: 0.376313\n",
      "[14]\ttraining's ndcg@5: 0.407963\tvalid_1's ndcg@5: 0.377503\n",
      "[15]\ttraining's ndcg@5: 0.410643\tvalid_1's ndcg@5: 0.378849\n",
      "[16]\ttraining's ndcg@5: 0.411314\tvalid_1's ndcg@5: 0.381057\n",
      "[17]\ttraining's ndcg@5: 0.411643\tvalid_1's ndcg@5: 0.380851\n",
      "[18]\ttraining's ndcg@5: 0.411686\tvalid_1's ndcg@5: 0.38093\n",
      "[19]\ttraining's ndcg@5: 0.413584\tvalid_1's ndcg@5: 0.382027\n",
      "[20]\ttraining's ndcg@5: 0.415575\tvalid_1's ndcg@5: 0.382106\n",
      "[21]\ttraining's ndcg@5: 0.417443\tvalid_1's ndcg@5: 0.382688\n",
      "[22]\ttraining's ndcg@5: 0.417682\tvalid_1's ndcg@5: 0.382969\n",
      "[23]\ttraining's ndcg@5: 0.417998\tvalid_1's ndcg@5: 0.383688\n",
      "[24]\ttraining's ndcg@5: 0.419731\tvalid_1's ndcg@5: 0.383774\n",
      "[25]\ttraining's ndcg@5: 0.420147\tvalid_1's ndcg@5: 0.385528\n",
      "[26]\ttraining's ndcg@5: 0.420126\tvalid_1's ndcg@5: 0.385898\n",
      "[27]\ttraining's ndcg@5: 0.422093\tvalid_1's ndcg@5: 0.385926\n",
      "[28]\ttraining's ndcg@5: 0.422438\tvalid_1's ndcg@5: 0.385743\n",
      "[29]\ttraining's ndcg@5: 0.422227\tvalid_1's ndcg@5: 0.386583\n",
      "[30]\ttraining's ndcg@5: 0.42223\tvalid_1's ndcg@5: 0.386346\n",
      "[31]\ttraining's ndcg@5: 0.422221\tvalid_1's ndcg@5: 0.386976\n",
      "[32]\ttraining's ndcg@5: 0.423896\tvalid_1's ndcg@5: 0.386848\n",
      "[33]\ttraining's ndcg@5: 0.423879\tvalid_1's ndcg@5: 0.387182\n",
      "[34]\ttraining's ndcg@5: 0.423741\tvalid_1's ndcg@5: 0.387606\n",
      "[35]\ttraining's ndcg@5: 0.425257\tvalid_1's ndcg@5: 0.387797\n",
      "[36]\ttraining's ndcg@5: 0.426764\tvalid_1's ndcg@5: 0.388406\n",
      "[37]\ttraining's ndcg@5: 0.426912\tvalid_1's ndcg@5: 0.38877\n",
      "[38]\ttraining's ndcg@5: 0.426763\tvalid_1's ndcg@5: 0.389161\n",
      "[39]\ttraining's ndcg@5: 0.426499\tvalid_1's ndcg@5: 0.389125\n",
      "[40]\ttraining's ndcg@5: 0.426821\tvalid_1's ndcg@5: 0.389388\n",
      "[41]\ttraining's ndcg@5: 0.427095\tvalid_1's ndcg@5: 0.389897\n",
      "[42]\ttraining's ndcg@5: 0.427378\tvalid_1's ndcg@5: 0.389986\n",
      "[43]\ttraining's ndcg@5: 0.428988\tvalid_1's ndcg@5: 0.389489\n",
      "[44]\ttraining's ndcg@5: 0.428843\tvalid_1's ndcg@5: 0.389697\n",
      "[45]\ttraining's ndcg@5: 0.428921\tvalid_1's ndcg@5: 0.389772\n",
      "[46]\ttraining's ndcg@5: 0.430458\tvalid_1's ndcg@5: 0.389706\n",
      "[47]\ttraining's ndcg@5: 0.430549\tvalid_1's ndcg@5: 0.389915\n",
      "[48]\ttraining's ndcg@5: 0.430505\tvalid_1's ndcg@5: 0.390084\n",
      "[49]\ttraining's ndcg@5: 0.431445\tvalid_1's ndcg@5: 0.390127\n",
      "[50]\ttraining's ndcg@5: 0.432881\tvalid_1's ndcg@5: 0.390094\n",
      "[51]\ttraining's ndcg@5: 0.434329\tvalid_1's ndcg@5: 0.390501\n",
      "[52]\ttraining's ndcg@5: 0.435119\tvalid_1's ndcg@5: 0.390594\n",
      "[53]\ttraining's ndcg@5: 0.435175\tvalid_1's ndcg@5: 0.390718\n",
      "[54]\ttraining's ndcg@5: 0.436229\tvalid_1's ndcg@5: 0.390881\n",
      "[55]\ttraining's ndcg@5: 0.437405\tvalid_1's ndcg@5: 0.391217\n",
      "[56]\ttraining's ndcg@5: 0.438134\tvalid_1's ndcg@5: 0.391183\n",
      "[57]\ttraining's ndcg@5: 0.438175\tvalid_1's ndcg@5: 0.39121\n",
      "[58]\ttraining's ndcg@5: 0.438208\tvalid_1's ndcg@5: 0.391197\n",
      "[59]\ttraining's ndcg@5: 0.439266\tvalid_1's ndcg@5: 0.391242\n",
      "[60]\ttraining's ndcg@5: 0.439053\tvalid_1's ndcg@5: 0.391511\n",
      "[61]\ttraining's ndcg@5: 0.439351\tvalid_1's ndcg@5: 0.391871\n",
      "[62]\ttraining's ndcg@5: 0.439562\tvalid_1's ndcg@5: 0.391732\n",
      "[63]\ttraining's ndcg@5: 0.44057\tvalid_1's ndcg@5: 0.391946\n",
      "[64]\ttraining's ndcg@5: 0.440796\tvalid_1's ndcg@5: 0.391861\n",
      "[65]\ttraining's ndcg@5: 0.441459\tvalid_1's ndcg@5: 0.391987\n",
      "[66]\ttraining's ndcg@5: 0.442373\tvalid_1's ndcg@5: 0.391938\n",
      "[67]\ttraining's ndcg@5: 0.443292\tvalid_1's ndcg@5: 0.391768\n",
      "[68]\ttraining's ndcg@5: 0.443512\tvalid_1's ndcg@5: 0.392021\n",
      "[69]\ttraining's ndcg@5: 0.444061\tvalid_1's ndcg@5: 0.391921\n",
      "[70]\ttraining's ndcg@5: 0.444955\tvalid_1's ndcg@5: 0.392274\n",
      "[71]\ttraining's ndcg@5: 0.44494\tvalid_1's ndcg@5: 0.392364\n",
      "[72]\ttraining's ndcg@5: 0.445653\tvalid_1's ndcg@5: 0.392266\n",
      "[73]\ttraining's ndcg@5: 0.446507\tvalid_1's ndcg@5: 0.392405\n",
      "[74]\ttraining's ndcg@5: 0.446605\tvalid_1's ndcg@5: 0.392639\n",
      "[75]\ttraining's ndcg@5: 0.446685\tvalid_1's ndcg@5: 0.392795\n",
      "[76]\ttraining's ndcg@5: 0.44682\tvalid_1's ndcg@5: 0.392933\n",
      "[77]\ttraining's ndcg@5: 0.447746\tvalid_1's ndcg@5: 0.393002\n",
      "[78]\ttraining's ndcg@5: 0.447833\tvalid_1's ndcg@5: 0.393298\n",
      "[79]\ttraining's ndcg@5: 0.448395\tvalid_1's ndcg@5: 0.393425\n",
      "[80]\ttraining's ndcg@5: 0.448464\tvalid_1's ndcg@5: 0.393598\n",
      "[81]\ttraining's ndcg@5: 0.448496\tvalid_1's ndcg@5: 0.393828\n",
      "[82]\ttraining's ndcg@5: 0.448682\tvalid_1's ndcg@5: 0.393861\n",
      "[83]\ttraining's ndcg@5: 0.449316\tvalid_1's ndcg@5: 0.394032\n",
      "[84]\ttraining's ndcg@5: 0.449329\tvalid_1's ndcg@5: 0.394131\n",
      "[85]\ttraining's ndcg@5: 0.44933\tvalid_1's ndcg@5: 0.394168\n",
      "[86]\ttraining's ndcg@5: 0.449452\tvalid_1's ndcg@5: 0.394007\n",
      "[87]\ttraining's ndcg@5: 0.450097\tvalid_1's ndcg@5: 0.394318\n",
      "[88]\ttraining's ndcg@5: 0.450149\tvalid_1's ndcg@5: 0.394399\n",
      "[89]\ttraining's ndcg@5: 0.450172\tvalid_1's ndcg@5: 0.394476\n",
      "[90]\ttraining's ndcg@5: 0.450625\tvalid_1's ndcg@5: 0.394614\n",
      "[91]\ttraining's ndcg@5: 0.451185\tvalid_1's ndcg@5: 0.394818\n",
      "[92]\ttraining's ndcg@5: 0.451278\tvalid_1's ndcg@5: 0.394891\n",
      "[93]\ttraining's ndcg@5: 0.451821\tvalid_1's ndcg@5: 0.394836\n",
      "[94]\ttraining's ndcg@5: 0.451978\tvalid_1's ndcg@5: 0.394781\n",
      "[95]\ttraining's ndcg@5: 0.45214\tvalid_1's ndcg@5: 0.395254\n",
      "[96]\ttraining's ndcg@5: 0.452303\tvalid_1's ndcg@5: 0.395501\n",
      "[97]\ttraining's ndcg@5: 0.452877\tvalid_1's ndcg@5: 0.39548\n",
      "[98]\ttraining's ndcg@5: 0.453032\tvalid_1's ndcg@5: 0.395822\n",
      "[99]\ttraining's ndcg@5: 0.453549\tvalid_1's ndcg@5: 0.39585\n",
      "[100]\ttraining's ndcg@5: 0.453677\tvalid_1's ndcg@5: 0.395944\n",
      "[101]\ttraining's ndcg@5: 0.454232\tvalid_1's ndcg@5: 0.395901\n",
      "[102]\ttraining's ndcg@5: 0.454458\tvalid_1's ndcg@5: 0.39609\n",
      "[103]\ttraining's ndcg@5: 0.454825\tvalid_1's ndcg@5: 0.396127\n",
      "[104]\ttraining's ndcg@5: 0.455122\tvalid_1's ndcg@5: 0.396343\n",
      "[105]\ttraining's ndcg@5: 0.455177\tvalid_1's ndcg@5: 0.39637\n",
      "[106]\ttraining's ndcg@5: 0.455103\tvalid_1's ndcg@5: 0.3964\n",
      "[107]\ttraining's ndcg@5: 0.455707\tvalid_1's ndcg@5: 0.396502\n",
      "[108]\ttraining's ndcg@5: 0.45615\tvalid_1's ndcg@5: 0.396469\n",
      "[109]\ttraining's ndcg@5: 0.456212\tvalid_1's ndcg@5: 0.396429\n",
      "[110]\ttraining's ndcg@5: 0.456308\tvalid_1's ndcg@5: 0.396626\n",
      "[111]\ttraining's ndcg@5: 0.456639\tvalid_1's ndcg@5: 0.396488\n",
      "[112]\ttraining's ndcg@5: 0.456905\tvalid_1's ndcg@5: 0.396666\n",
      "[113]\ttraining's ndcg@5: 0.456978\tvalid_1's ndcg@5: 0.396798\n",
      "[114]\ttraining's ndcg@5: 0.456961\tvalid_1's ndcg@5: 0.396942\n",
      "[115]\ttraining's ndcg@5: 0.456936\tvalid_1's ndcg@5: 0.396886\n",
      "[116]\ttraining's ndcg@5: 0.457001\tvalid_1's ndcg@5: 0.39718\n",
      "[117]\ttraining's ndcg@5: 0.457503\tvalid_1's ndcg@5: 0.397247\n",
      "[118]\ttraining's ndcg@5: 0.457747\tvalid_1's ndcg@5: 0.39761\n",
      "[119]\ttraining's ndcg@5: 0.458022\tvalid_1's ndcg@5: 0.397556\n",
      "[120]\ttraining's ndcg@5: 0.458152\tvalid_1's ndcg@5: 0.397483\n",
      "[121]\ttraining's ndcg@5: 0.458417\tvalid_1's ndcg@5: 0.397439\n",
      "[122]\ttraining's ndcg@5: 0.458498\tvalid_1's ndcg@5: 0.397471\n",
      "[123]\ttraining's ndcg@5: 0.458546\tvalid_1's ndcg@5: 0.397671\n",
      "[124]\ttraining's ndcg@5: 0.458825\tvalid_1's ndcg@5: 0.397659\n",
      "[125]\ttraining's ndcg@5: 0.459144\tvalid_1's ndcg@5: 0.397535\n",
      "[126]\ttraining's ndcg@5: 0.459256\tvalid_1's ndcg@5: 0.397282\n",
      "[127]\ttraining's ndcg@5: 0.459531\tvalid_1's ndcg@5: 0.397323\n",
      "[128]\ttraining's ndcg@5: 0.459623\tvalid_1's ndcg@5: 0.397463\n",
      "[129]\ttraining's ndcg@5: 0.459839\tvalid_1's ndcg@5: 0.397507\n",
      "[130]\ttraining's ndcg@5: 0.460149\tvalid_1's ndcg@5: 0.397612\n",
      "[131]\ttraining's ndcg@5: 0.460441\tvalid_1's ndcg@5: 0.397623\n",
      "[132]\ttraining's ndcg@5: 0.460498\tvalid_1's ndcg@5: 0.397574\n",
      "[133]\ttraining's ndcg@5: 0.46084\tvalid_1's ndcg@5: 0.397462\n",
      "[134]\ttraining's ndcg@5: 0.461221\tvalid_1's ndcg@5: 0.397477\n",
      "[135]\ttraining's ndcg@5: 0.461345\tvalid_1's ndcg@5: 0.397524\n",
      "[136]\ttraining's ndcg@5: 0.461625\tvalid_1's ndcg@5: 0.397637\n",
      "[137]\ttraining's ndcg@5: 0.461943\tvalid_1's ndcg@5: 0.397777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[138]\ttraining's ndcg@5: 0.462179\tvalid_1's ndcg@5: 0.397715\n",
      "[139]\ttraining's ndcg@5: 0.462173\tvalid_1's ndcg@5: 0.39764\n",
      "[140]\ttraining's ndcg@5: 0.462424\tvalid_1's ndcg@5: 0.397847\n",
      "[141]\ttraining's ndcg@5: 0.462565\tvalid_1's ndcg@5: 0.397854\n",
      "[142]\ttraining's ndcg@5: 0.462831\tvalid_1's ndcg@5: 0.39802\n",
      "[143]\ttraining's ndcg@5: 0.462968\tvalid_1's ndcg@5: 0.397943\n",
      "[144]\ttraining's ndcg@5: 0.462967\tvalid_1's ndcg@5: 0.398136\n",
      "[145]\ttraining's ndcg@5: 0.463046\tvalid_1's ndcg@5: 0.398064\n",
      "[146]\ttraining's ndcg@5: 0.463192\tvalid_1's ndcg@5: 0.398303\n",
      "[147]\ttraining's ndcg@5: 0.463367\tvalid_1's ndcg@5: 0.398254\n",
      "[148]\ttraining's ndcg@5: 0.463607\tvalid_1's ndcg@5: 0.398046\n",
      "[149]\ttraining's ndcg@5: 0.463687\tvalid_1's ndcg@5: 0.398139\n",
      "[150]\ttraining's ndcg@5: 0.463935\tvalid_1's ndcg@5: 0.398037\n",
      "[151]\ttraining's ndcg@5: 0.46419\tvalid_1's ndcg@5: 0.39815\n",
      "[152]\ttraining's ndcg@5: 0.464339\tvalid_1's ndcg@5: 0.398236\n",
      "[153]\ttraining's ndcg@5: 0.464344\tvalid_1's ndcg@5: 0.398187\n",
      "[154]\ttraining's ndcg@5: 0.464326\tvalid_1's ndcg@5: 0.398127\n",
      "[155]\ttraining's ndcg@5: 0.464516\tvalid_1's ndcg@5: 0.398257\n",
      "[156]\ttraining's ndcg@5: 0.464644\tvalid_1's ndcg@5: 0.398563\n",
      "[157]\ttraining's ndcg@5: 0.464858\tvalid_1's ndcg@5: 0.398441\n",
      "[158]\ttraining's ndcg@5: 0.465001\tvalid_1's ndcg@5: 0.398506\n",
      "[159]\ttraining's ndcg@5: 0.46505\tvalid_1's ndcg@5: 0.398694\n",
      "[160]\ttraining's ndcg@5: 0.465295\tvalid_1's ndcg@5: 0.39899\n",
      "[161]\ttraining's ndcg@5: 0.465523\tvalid_1's ndcg@5: 0.398968\n",
      "[162]\ttraining's ndcg@5: 0.465727\tvalid_1's ndcg@5: 0.399001\n",
      "[163]\ttraining's ndcg@5: 0.466016\tvalid_1's ndcg@5: 0.399094\n",
      "[164]\ttraining's ndcg@5: 0.466154\tvalid_1's ndcg@5: 0.399154\n",
      "[165]\ttraining's ndcg@5: 0.466377\tvalid_1's ndcg@5: 0.399089\n",
      "[166]\ttraining's ndcg@5: 0.466293\tvalid_1's ndcg@5: 0.399171\n",
      "[167]\ttraining's ndcg@5: 0.466456\tvalid_1's ndcg@5: 0.399119\n",
      "[168]\ttraining's ndcg@5: 0.466562\tvalid_1's ndcg@5: 0.399284\n",
      "[169]\ttraining's ndcg@5: 0.466714\tvalid_1's ndcg@5: 0.399361\n",
      "[170]\ttraining's ndcg@5: 0.46688\tvalid_1's ndcg@5: 0.399485\n",
      "[171]\ttraining's ndcg@5: 0.467087\tvalid_1's ndcg@5: 0.399516\n",
      "[172]\ttraining's ndcg@5: 0.467232\tvalid_1's ndcg@5: 0.399601\n",
      "[173]\ttraining's ndcg@5: 0.467418\tvalid_1's ndcg@5: 0.399656\n",
      "[174]\ttraining's ndcg@5: 0.46761\tvalid_1's ndcg@5: 0.399657\n",
      "[175]\ttraining's ndcg@5: 0.467828\tvalid_1's ndcg@5: 0.399591\n",
      "[176]\ttraining's ndcg@5: 0.468137\tvalid_1's ndcg@5: 0.399484\n",
      "[177]\ttraining's ndcg@5: 0.468291\tvalid_1's ndcg@5: 0.399481\n",
      "[178]\ttraining's ndcg@5: 0.468406\tvalid_1's ndcg@5: 0.399429\n",
      "[179]\ttraining's ndcg@5: 0.46857\tvalid_1's ndcg@5: 0.399406\n",
      "[180]\ttraining's ndcg@5: 0.468605\tvalid_1's ndcg@5: 0.399546\n",
      "[181]\ttraining's ndcg@5: 0.468785\tvalid_1's ndcg@5: 0.39948\n",
      "[182]\ttraining's ndcg@5: 0.469\tvalid_1's ndcg@5: 0.399443\n",
      "[183]\ttraining's ndcg@5: 0.469077\tvalid_1's ndcg@5: 0.399363\n",
      "[184]\ttraining's ndcg@5: 0.469211\tvalid_1's ndcg@5: 0.39962\n",
      "[185]\ttraining's ndcg@5: 0.46931\tvalid_1's ndcg@5: 0.399466\n",
      "[186]\ttraining's ndcg@5: 0.46952\tvalid_1's ndcg@5: 0.399524\n",
      "[187]\ttraining's ndcg@5: 0.469503\tvalid_1's ndcg@5: 0.399614\n",
      "[188]\ttraining's ndcg@5: 0.469544\tvalid_1's ndcg@5: 0.399682\n",
      "[189]\ttraining's ndcg@5: 0.469717\tvalid_1's ndcg@5: 0.399729\n",
      "[190]\ttraining's ndcg@5: 0.469827\tvalid_1's ndcg@5: 0.399557\n",
      "[191]\ttraining's ndcg@5: 0.46988\tvalid_1's ndcg@5: 0.399709\n",
      "[192]\ttraining's ndcg@5: 0.469937\tvalid_1's ndcg@5: 0.399664\n",
      "[193]\ttraining's ndcg@5: 0.470024\tvalid_1's ndcg@5: 0.399807\n",
      "[194]\ttraining's ndcg@5: 0.470115\tvalid_1's ndcg@5: 0.39968\n",
      "[195]\ttraining's ndcg@5: 0.470198\tvalid_1's ndcg@5: 0.399773\n",
      "[196]\ttraining's ndcg@5: 0.470223\tvalid_1's ndcg@5: 0.399787\n",
      "[197]\ttraining's ndcg@5: 0.470346\tvalid_1's ndcg@5: 0.399683\n",
      "[198]\ttraining's ndcg@5: 0.470471\tvalid_1's ndcg@5: 0.39979\n",
      "[199]\ttraining's ndcg@5: 0.470621\tvalid_1's ndcg@5: 0.399941\n",
      "[200]\ttraining's ndcg@5: 0.470639\tvalid_1's ndcg@5: 0.400049\n",
      "[201]\ttraining's ndcg@5: 0.470846\tvalid_1's ndcg@5: 0.400051\n",
      "[202]\ttraining's ndcg@5: 0.470963\tvalid_1's ndcg@5: 0.400052\n",
      "[203]\ttraining's ndcg@5: 0.471106\tvalid_1's ndcg@5: 0.400112\n",
      "[204]\ttraining's ndcg@5: 0.471281\tvalid_1's ndcg@5: 0.400133\n",
      "[205]\ttraining's ndcg@5: 0.471452\tvalid_1's ndcg@5: 0.400092\n",
      "[206]\ttraining's ndcg@5: 0.471618\tvalid_1's ndcg@5: 0.400104\n",
      "[207]\ttraining's ndcg@5: 0.471826\tvalid_1's ndcg@5: 0.400249\n",
      "[208]\ttraining's ndcg@5: 0.471994\tvalid_1's ndcg@5: 0.400233\n",
      "[209]\ttraining's ndcg@5: 0.472105\tvalid_1's ndcg@5: 0.400324\n",
      "[210]\ttraining's ndcg@5: 0.472262\tvalid_1's ndcg@5: 0.400348\n",
      "[211]\ttraining's ndcg@5: 0.472237\tvalid_1's ndcg@5: 0.400533\n",
      "[212]\ttraining's ndcg@5: 0.472389\tvalid_1's ndcg@5: 0.400567\n",
      "[213]\ttraining's ndcg@5: 0.472502\tvalid_1's ndcg@5: 0.400572\n",
      "[214]\ttraining's ndcg@5: 0.472714\tvalid_1's ndcg@5: 0.400529\n",
      "[215]\ttraining's ndcg@5: 0.472754\tvalid_1's ndcg@5: 0.400479\n",
      "[216]\ttraining's ndcg@5: 0.472948\tvalid_1's ndcg@5: 0.400433\n",
      "[217]\ttraining's ndcg@5: 0.472982\tvalid_1's ndcg@5: 0.400474\n",
      "[218]\ttraining's ndcg@5: 0.472999\tvalid_1's ndcg@5: 0.400692\n",
      "[219]\ttraining's ndcg@5: 0.473102\tvalid_1's ndcg@5: 0.400918\n",
      "[220]\ttraining's ndcg@5: 0.473312\tvalid_1's ndcg@5: 0.400933\n",
      "[221]\ttraining's ndcg@5: 0.473423\tvalid_1's ndcg@5: 0.400758\n",
      "[222]\ttraining's ndcg@5: 0.473512\tvalid_1's ndcg@5: 0.400738\n",
      "[223]\ttraining's ndcg@5: 0.473675\tvalid_1's ndcg@5: 0.400757\n",
      "[224]\ttraining's ndcg@5: 0.473758\tvalid_1's ndcg@5: 0.400683\n",
      "[225]\ttraining's ndcg@5: 0.473903\tvalid_1's ndcg@5: 0.400889\n",
      "[226]\ttraining's ndcg@5: 0.474095\tvalid_1's ndcg@5: 0.400946\n",
      "[227]\ttraining's ndcg@5: 0.474307\tvalid_1's ndcg@5: 0.400953\n",
      "[228]\ttraining's ndcg@5: 0.474474\tvalid_1's ndcg@5: 0.400998\n",
      "[229]\ttraining's ndcg@5: 0.474562\tvalid_1's ndcg@5: 0.40095\n",
      "[230]\ttraining's ndcg@5: 0.47468\tvalid_1's ndcg@5: 0.400991\n",
      "[231]\ttraining's ndcg@5: 0.474767\tvalid_1's ndcg@5: 0.40108\n",
      "[232]\ttraining's ndcg@5: 0.47485\tvalid_1's ndcg@5: 0.401169\n",
      "[233]\ttraining's ndcg@5: 0.47502\tvalid_1's ndcg@5: 0.401249\n",
      "[234]\ttraining's ndcg@5: 0.475166\tvalid_1's ndcg@5: 0.401197\n",
      "[235]\ttraining's ndcg@5: 0.475343\tvalid_1's ndcg@5: 0.401272\n",
      "[236]\ttraining's ndcg@5: 0.475501\tvalid_1's ndcg@5: 0.401499\n",
      "[237]\ttraining's ndcg@5: 0.475608\tvalid_1's ndcg@5: 0.401472\n",
      "[238]\ttraining's ndcg@5: 0.475762\tvalid_1's ndcg@5: 0.401381\n",
      "[239]\ttraining's ndcg@5: 0.475827\tvalid_1's ndcg@5: 0.401392\n",
      "[240]\ttraining's ndcg@5: 0.475852\tvalid_1's ndcg@5: 0.401507\n",
      "[241]\ttraining's ndcg@5: 0.476041\tvalid_1's ndcg@5: 0.401463\n",
      "[242]\ttraining's ndcg@5: 0.476215\tvalid_1's ndcg@5: 0.401603\n",
      "[243]\ttraining's ndcg@5: 0.476387\tvalid_1's ndcg@5: 0.401625\n",
      "[244]\ttraining's ndcg@5: 0.476506\tvalid_1's ndcg@5: 0.40159\n",
      "[245]\ttraining's ndcg@5: 0.476643\tvalid_1's ndcg@5: 0.401392\n",
      "[246]\ttraining's ndcg@5: 0.476796\tvalid_1's ndcg@5: 0.401411\n",
      "[247]\ttraining's ndcg@5: 0.476846\tvalid_1's ndcg@5: 0.401605\n",
      "[248]\ttraining's ndcg@5: 0.476937\tvalid_1's ndcg@5: 0.401611\n",
      "[249]\ttraining's ndcg@5: 0.477097\tvalid_1's ndcg@5: 0.4016\n",
      "[250]\ttraining's ndcg@5: 0.477245\tvalid_1's ndcg@5: 0.401574\n",
      "[251]\ttraining's ndcg@5: 0.477353\tvalid_1's ndcg@5: 0.401658\n",
      "[252]\ttraining's ndcg@5: 0.477415\tvalid_1's ndcg@5: 0.401517\n",
      "[253]\ttraining's ndcg@5: 0.477549\tvalid_1's ndcg@5: 0.401472\n",
      "[254]\ttraining's ndcg@5: 0.477603\tvalid_1's ndcg@5: 0.40148\n",
      "[255]\ttraining's ndcg@5: 0.4776\tvalid_1's ndcg@5: 0.401787\n",
      "[256]\ttraining's ndcg@5: 0.47773\tvalid_1's ndcg@5: 0.401804\n",
      "[257]\ttraining's ndcg@5: 0.477883\tvalid_1's ndcg@5: 0.4018\n",
      "[258]\ttraining's ndcg@5: 0.477942\tvalid_1's ndcg@5: 0.401879\n",
      "[259]\ttraining's ndcg@5: 0.478096\tvalid_1's ndcg@5: 0.401868\n",
      "[260]\ttraining's ndcg@5: 0.478336\tvalid_1's ndcg@5: 0.401816\n",
      "[261]\ttraining's ndcg@5: 0.478552\tvalid_1's ndcg@5: 0.401836\n",
      "[262]\ttraining's ndcg@5: 0.478724\tvalid_1's ndcg@5: 0.401928\n",
      "[263]\ttraining's ndcg@5: 0.478868\tvalid_1's ndcg@5: 0.401954\n",
      "[264]\ttraining's ndcg@5: 0.479036\tvalid_1's ndcg@5: 0.401917\n",
      "[265]\ttraining's ndcg@5: 0.479036\tvalid_1's ndcg@5: 0.40194\n",
      "[266]\ttraining's ndcg@5: 0.479079\tvalid_1's ndcg@5: 0.401943\n",
      "[267]\ttraining's ndcg@5: 0.479199\tvalid_1's ndcg@5: 0.401866\n",
      "[268]\ttraining's ndcg@5: 0.4793\tvalid_1's ndcg@5: 0.401698\n",
      "[269]\ttraining's ndcg@5: 0.479309\tvalid_1's ndcg@5: 0.40185\n",
      "[270]\ttraining's ndcg@5: 0.479409\tvalid_1's ndcg@5: 0.401847\n",
      "[271]\ttraining's ndcg@5: 0.479553\tvalid_1's ndcg@5: 0.401852\n",
      "[272]\ttraining's ndcg@5: 0.479703\tvalid_1's ndcg@5: 0.401863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[273]\ttraining's ndcg@5: 0.479747\tvalid_1's ndcg@5: 0.401944\n",
      "[274]\ttraining's ndcg@5: 0.479936\tvalid_1's ndcg@5: 0.401975\n",
      "[275]\ttraining's ndcg@5: 0.480066\tvalid_1's ndcg@5: 0.402051\n",
      "[276]\ttraining's ndcg@5: 0.48021\tvalid_1's ndcg@5: 0.402021\n",
      "[277]\ttraining's ndcg@5: 0.480378\tvalid_1's ndcg@5: 0.402122\n",
      "[278]\ttraining's ndcg@5: 0.480571\tvalid_1's ndcg@5: 0.402239\n",
      "[279]\ttraining's ndcg@5: 0.480684\tvalid_1's ndcg@5: 0.402305\n",
      "[280]\ttraining's ndcg@5: 0.480668\tvalid_1's ndcg@5: 0.402379\n",
      "[281]\ttraining's ndcg@5: 0.480791\tvalid_1's ndcg@5: 0.402411\n",
      "[282]\ttraining's ndcg@5: 0.480928\tvalid_1's ndcg@5: 0.402384\n",
      "[283]\ttraining's ndcg@5: 0.48112\tvalid_1's ndcg@5: 0.40228\n",
      "[284]\ttraining's ndcg@5: 0.481284\tvalid_1's ndcg@5: 0.402266\n",
      "[285]\ttraining's ndcg@5: 0.481419\tvalid_1's ndcg@5: 0.402251\n",
      "[286]\ttraining's ndcg@5: 0.481506\tvalid_1's ndcg@5: 0.402385\n",
      "[287]\ttraining's ndcg@5: 0.481569\tvalid_1's ndcg@5: 0.402446\n",
      "[288]\ttraining's ndcg@5: 0.48165\tvalid_1's ndcg@5: 0.402501\n",
      "[289]\ttraining's ndcg@5: 0.481715\tvalid_1's ndcg@5: 0.40229\n",
      "[290]\ttraining's ndcg@5: 0.481885\tvalid_1's ndcg@5: 0.402217\n",
      "[291]\ttraining's ndcg@5: 0.48205\tvalid_1's ndcg@5: 0.402186\n",
      "[292]\ttraining's ndcg@5: 0.48222\tvalid_1's ndcg@5: 0.402162\n",
      "[293]\ttraining's ndcg@5: 0.482394\tvalid_1's ndcg@5: 0.402162\n",
      "[294]\ttraining's ndcg@5: 0.482488\tvalid_1's ndcg@5: 0.402186\n",
      "[295]\ttraining's ndcg@5: 0.48259\tvalid_1's ndcg@5: 0.402383\n",
      "[296]\ttraining's ndcg@5: 0.482764\tvalid_1's ndcg@5: 0.402477\n",
      "[297]\ttraining's ndcg@5: 0.482941\tvalid_1's ndcg@5: 0.40243\n",
      "[298]\ttraining's ndcg@5: 0.483109\tvalid_1's ndcg@5: 0.402412\n",
      "[299]\ttraining's ndcg@5: 0.483161\tvalid_1's ndcg@5: 0.402331\n",
      "[300]\ttraining's ndcg@5: 0.483252\tvalid_1's ndcg@5: 0.402276\n",
      "[301]\ttraining's ndcg@5: 0.483324\tvalid_1's ndcg@5: 0.402257\n",
      "[302]\ttraining's ndcg@5: 0.483533\tvalid_1's ndcg@5: 0.402219\n",
      "[303]\ttraining's ndcg@5: 0.483666\tvalid_1's ndcg@5: 0.402201\n",
      "[304]\ttraining's ndcg@5: 0.483733\tvalid_1's ndcg@5: 0.402333\n",
      "[305]\ttraining's ndcg@5: 0.483843\tvalid_1's ndcg@5: 0.402109\n",
      "[306]\ttraining's ndcg@5: 0.483884\tvalid_1's ndcg@5: 0.402171\n",
      "[307]\ttraining's ndcg@5: 0.484074\tvalid_1's ndcg@5: 0.402152\n",
      "[308]\ttraining's ndcg@5: 0.484116\tvalid_1's ndcg@5: 0.402211\n",
      "[309]\ttraining's ndcg@5: 0.484277\tvalid_1's ndcg@5: 0.402284\n",
      "[310]\ttraining's ndcg@5: 0.484419\tvalid_1's ndcg@5: 0.402351\n",
      "[311]\ttraining's ndcg@5: 0.484583\tvalid_1's ndcg@5: 0.402315\n",
      "[312]\ttraining's ndcg@5: 0.484655\tvalid_1's ndcg@5: 0.402351\n",
      "[313]\ttraining's ndcg@5: 0.484806\tvalid_1's ndcg@5: 0.402292\n",
      "[314]\ttraining's ndcg@5: 0.484937\tvalid_1's ndcg@5: 0.402323\n",
      "[315]\ttraining's ndcg@5: 0.485066\tvalid_1's ndcg@5: 0.402294\n",
      "[316]\ttraining's ndcg@5: 0.485088\tvalid_1's ndcg@5: 0.40226\n",
      "[317]\ttraining's ndcg@5: 0.485102\tvalid_1's ndcg@5: 0.402352\n",
      "[318]\ttraining's ndcg@5: 0.485304\tvalid_1's ndcg@5: 0.402383\n",
      "[319]\ttraining's ndcg@5: 0.485395\tvalid_1's ndcg@5: 0.40247\n",
      "[320]\ttraining's ndcg@5: 0.485464\tvalid_1's ndcg@5: 0.402591\n",
      "[321]\ttraining's ndcg@5: 0.485612\tvalid_1's ndcg@5: 0.402685\n",
      "[322]\ttraining's ndcg@5: 0.485668\tvalid_1's ndcg@5: 0.402731\n",
      "[323]\ttraining's ndcg@5: 0.485812\tvalid_1's ndcg@5: 0.40279\n",
      "[324]\ttraining's ndcg@5: 0.485917\tvalid_1's ndcg@5: 0.402846\n",
      "[325]\ttraining's ndcg@5: 0.486032\tvalid_1's ndcg@5: 0.402861\n",
      "[326]\ttraining's ndcg@5: 0.486136\tvalid_1's ndcg@5: 0.402992\n",
      "[327]\ttraining's ndcg@5: 0.486235\tvalid_1's ndcg@5: 0.403045\n",
      "[328]\ttraining's ndcg@5: 0.486332\tvalid_1's ndcg@5: 0.403024\n",
      "[329]\ttraining's ndcg@5: 0.486402\tvalid_1's ndcg@5: 0.403042\n",
      "[330]\ttraining's ndcg@5: 0.486539\tvalid_1's ndcg@5: 0.403001\n",
      "[331]\ttraining's ndcg@5: 0.486641\tvalid_1's ndcg@5: 0.403054\n",
      "[332]\ttraining's ndcg@5: 0.48674\tvalid_1's ndcg@5: 0.402983\n",
      "[333]\ttraining's ndcg@5: 0.486813\tvalid_1's ndcg@5: 0.402941\n",
      "[334]\ttraining's ndcg@5: 0.486946\tvalid_1's ndcg@5: 0.402949\n",
      "[335]\ttraining's ndcg@5: 0.487014\tvalid_1's ndcg@5: 0.402938\n",
      "[336]\ttraining's ndcg@5: 0.487108\tvalid_1's ndcg@5: 0.403071\n",
      "[337]\ttraining's ndcg@5: 0.487224\tvalid_1's ndcg@5: 0.403123\n",
      "[338]\ttraining's ndcg@5: 0.487342\tvalid_1's ndcg@5: 0.402979\n",
      "[339]\ttraining's ndcg@5: 0.487481\tvalid_1's ndcg@5: 0.402982\n",
      "[340]\ttraining's ndcg@5: 0.487592\tvalid_1's ndcg@5: 0.403074\n",
      "[341]\ttraining's ndcg@5: 0.487676\tvalid_1's ndcg@5: 0.403099\n",
      "[342]\ttraining's ndcg@5: 0.487822\tvalid_1's ndcg@5: 0.403127\n",
      "[343]\ttraining's ndcg@5: 0.487906\tvalid_1's ndcg@5: 0.403126\n",
      "[344]\ttraining's ndcg@5: 0.488038\tvalid_1's ndcg@5: 0.403159\n",
      "[345]\ttraining's ndcg@5: 0.488156\tvalid_1's ndcg@5: 0.403107\n",
      "[346]\ttraining's ndcg@5: 0.488254\tvalid_1's ndcg@5: 0.403099\n",
      "[347]\ttraining's ndcg@5: 0.488325\tvalid_1's ndcg@5: 0.403248\n",
      "[348]\ttraining's ndcg@5: 0.488509\tvalid_1's ndcg@5: 0.403266\n",
      "[349]\ttraining's ndcg@5: 0.488535\tvalid_1's ndcg@5: 0.403149\n",
      "[350]\ttraining's ndcg@5: 0.488581\tvalid_1's ndcg@5: 0.403211\n",
      "[351]\ttraining's ndcg@5: 0.488651\tvalid_1's ndcg@5: 0.4032\n",
      "[352]\ttraining's ndcg@5: 0.48874\tvalid_1's ndcg@5: 0.403254\n",
      "[353]\ttraining's ndcg@5: 0.488869\tvalid_1's ndcg@5: 0.40327\n",
      "[354]\ttraining's ndcg@5: 0.488963\tvalid_1's ndcg@5: 0.403386\n",
      "[355]\ttraining's ndcg@5: 0.489062\tvalid_1's ndcg@5: 0.40336\n",
      "[356]\ttraining's ndcg@5: 0.489115\tvalid_1's ndcg@5: 0.403324\n",
      "[357]\ttraining's ndcg@5: 0.489187\tvalid_1's ndcg@5: 0.403379\n",
      "[358]\ttraining's ndcg@5: 0.489243\tvalid_1's ndcg@5: 0.403225\n",
      "[359]\ttraining's ndcg@5: 0.489444\tvalid_1's ndcg@5: 0.40333\n",
      "[360]\ttraining's ndcg@5: 0.489527\tvalid_1's ndcg@5: 0.403427\n",
      "[361]\ttraining's ndcg@5: 0.489657\tvalid_1's ndcg@5: 0.403403\n",
      "[362]\ttraining's ndcg@5: 0.489744\tvalid_1's ndcg@5: 0.403341\n",
      "[363]\ttraining's ndcg@5: 0.489873\tvalid_1's ndcg@5: 0.403325\n",
      "[364]\ttraining's ndcg@5: 0.490022\tvalid_1's ndcg@5: 0.403306\n",
      "[365]\ttraining's ndcg@5: 0.490151\tvalid_1's ndcg@5: 0.403322\n",
      "[366]\ttraining's ndcg@5: 0.490272\tvalid_1's ndcg@5: 0.403304\n",
      "[367]\ttraining's ndcg@5: 0.490409\tvalid_1's ndcg@5: 0.403369\n",
      "[368]\ttraining's ndcg@5: 0.490496\tvalid_1's ndcg@5: 0.403347\n",
      "[369]\ttraining's ndcg@5: 0.490527\tvalid_1's ndcg@5: 0.403268\n",
      "[370]\ttraining's ndcg@5: 0.490557\tvalid_1's ndcg@5: 0.403325\n",
      "[371]\ttraining's ndcg@5: 0.490682\tvalid_1's ndcg@5: 0.403301\n",
      "[372]\ttraining's ndcg@5: 0.490899\tvalid_1's ndcg@5: 0.403106\n",
      "[373]\ttraining's ndcg@5: 0.491013\tvalid_1's ndcg@5: 0.40312\n",
      "[374]\ttraining's ndcg@5: 0.491139\tvalid_1's ndcg@5: 0.40311\n",
      "[375]\ttraining's ndcg@5: 0.491229\tvalid_1's ndcg@5: 0.403102\n",
      "[376]\ttraining's ndcg@5: 0.491254\tvalid_1's ndcg@5: 0.403036\n",
      "[377]\ttraining's ndcg@5: 0.491434\tvalid_1's ndcg@5: 0.403147\n",
      "[378]\ttraining's ndcg@5: 0.491551\tvalid_1's ndcg@5: 0.403302\n",
      "[379]\ttraining's ndcg@5: 0.491669\tvalid_1's ndcg@5: 0.403232\n",
      "[380]\ttraining's ndcg@5: 0.49175\tvalid_1's ndcg@5: 0.403343\n",
      "[381]\ttraining's ndcg@5: 0.491882\tvalid_1's ndcg@5: 0.403325\n",
      "[382]\ttraining's ndcg@5: 0.49194\tvalid_1's ndcg@5: 0.403344\n",
      "[383]\ttraining's ndcg@5: 0.492026\tvalid_1's ndcg@5: 0.403386\n",
      "[384]\ttraining's ndcg@5: 0.4921\tvalid_1's ndcg@5: 0.403348\n",
      "[385]\ttraining's ndcg@5: 0.49216\tvalid_1's ndcg@5: 0.403321\n",
      "[386]\ttraining's ndcg@5: 0.49226\tvalid_1's ndcg@5: 0.403422\n",
      "[387]\ttraining's ndcg@5: 0.492364\tvalid_1's ndcg@5: 0.403389\n",
      "[388]\ttraining's ndcg@5: 0.492456\tvalid_1's ndcg@5: 0.403378\n",
      "[389]\ttraining's ndcg@5: 0.492618\tvalid_1's ndcg@5: 0.403366\n",
      "[390]\ttraining's ndcg@5: 0.492695\tvalid_1's ndcg@5: 0.403442\n",
      "[391]\ttraining's ndcg@5: 0.49285\tvalid_1's ndcg@5: 0.403363\n",
      "[392]\ttraining's ndcg@5: 0.493005\tvalid_1's ndcg@5: 0.40353\n",
      "[393]\ttraining's ndcg@5: 0.493046\tvalid_1's ndcg@5: 0.403608\n",
      "[394]\ttraining's ndcg@5: 0.493127\tvalid_1's ndcg@5: 0.403494\n",
      "[395]\ttraining's ndcg@5: 0.49326\tvalid_1's ndcg@5: 0.403537\n",
      "[396]\ttraining's ndcg@5: 0.493396\tvalid_1's ndcg@5: 0.403442\n",
      "[397]\ttraining's ndcg@5: 0.493547\tvalid_1's ndcg@5: 0.403425\n",
      "[398]\ttraining's ndcg@5: 0.493557\tvalid_1's ndcg@5: 0.403491\n",
      "[399]\ttraining's ndcg@5: 0.493685\tvalid_1's ndcg@5: 0.403478\n",
      "[400]\ttraining's ndcg@5: 0.493766\tvalid_1's ndcg@5: 0.403425\n",
      "[401]\ttraining's ndcg@5: 0.493903\tvalid_1's ndcg@5: 0.403388\n",
      "[402]\ttraining's ndcg@5: 0.493957\tvalid_1's ndcg@5: 0.403379\n",
      "[403]\ttraining's ndcg@5: 0.494116\tvalid_1's ndcg@5: 0.403235\n",
      "[404]\ttraining's ndcg@5: 0.494177\tvalid_1's ndcg@5: 0.403187\n",
      "[405]\ttraining's ndcg@5: 0.494291\tvalid_1's ndcg@5: 0.403206\n",
      "[406]\ttraining's ndcg@5: 0.494348\tvalid_1's ndcg@5: 0.403348\n",
      "[407]\ttraining's ndcg@5: 0.494484\tvalid_1's ndcg@5: 0.403323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[408]\ttraining's ndcg@5: 0.494588\tvalid_1's ndcg@5: 0.403317\n",
      "[409]\ttraining's ndcg@5: 0.494582\tvalid_1's ndcg@5: 0.403207\n",
      "[410]\ttraining's ndcg@5: 0.494663\tvalid_1's ndcg@5: 0.403313\n",
      "[411]\ttraining's ndcg@5: 0.494725\tvalid_1's ndcg@5: 0.403287\n",
      "[412]\ttraining's ndcg@5: 0.494806\tvalid_1's ndcg@5: 0.403209\n",
      "[413]\ttraining's ndcg@5: 0.494929\tvalid_1's ndcg@5: 0.403176\n",
      "[414]\ttraining's ndcg@5: 0.495074\tvalid_1's ndcg@5: 0.40319\n",
      "[415]\ttraining's ndcg@5: 0.495186\tvalid_1's ndcg@5: 0.403137\n",
      "[416]\ttraining's ndcg@5: 0.495302\tvalid_1's ndcg@5: 0.403167\n",
      "[417]\ttraining's ndcg@5: 0.495449\tvalid_1's ndcg@5: 0.403305\n",
      "[418]\ttraining's ndcg@5: 0.495619\tvalid_1's ndcg@5: 0.403328\n",
      "[419]\ttraining's ndcg@5: 0.495746\tvalid_1's ndcg@5: 0.403189\n",
      "[420]\ttraining's ndcg@5: 0.495873\tvalid_1's ndcg@5: 0.40314\n",
      "[421]\ttraining's ndcg@5: 0.496058\tvalid_1's ndcg@5: 0.403153\n",
      "Predicting...\n",
      "Done predicting\n",
      "result final:0.4260613335094323\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "# LightGBM ranker\n",
    "import lightgbm as lgb\n",
    "# import wandb\n",
    "# from wandb.lightgbm import wandb_callback, log_summary\n",
    "\n",
    "# Create dataset\n",
    "group_train = X_train.groupby('srch_id').size().values\n",
    "group_val = X_test.groupby('srch_id').size().values\n",
    "\n",
    "X_train_lgb = X_train.drop(['srch_id'], axis=1)\n",
    "X_val_lgb = X_test.drop(['srch_id'], axis=1)\n",
    "\n",
    "\n",
    "ranker = lgb.LGBMRanker(**lgb_params)\n",
    "\n",
    "# wandb.init(project='DMT-2023', config = best_params, notes='Now with class_weight = balanced', name='possibly-balanced-tiger-2')\n",
    "\n",
    "\n",
    "# Training the model\n",
    "ranker.fit(\n",
    "      X=X_train_lgb,\n",
    "      y=y_train,\n",
    "      group=group_train,\n",
    "      eval_set=[(X_train_lgb, y_train),(X_val_lgb, y_test)],\n",
    "      eval_group=[group_train, group_val],\n",
    "      eval_at=[5],\n",
    "      callbacks=[],\n",
    "      feature_name='auto', \n",
    "      categorical_feature = 'auto')\n",
    "\n",
    "# Predicting the scores\n",
    "test = X_test.drop(['srch_id'], axis=1).copy()\n",
    "\n",
    "print(\"Predicting...\")\n",
    "y_pred = ranker.predict(test)\n",
    "print(\"Done predicting\")\n",
    "\n",
    "df_res = X_test.copy()\n",
    "df_res['pred_grades'] = y_pred\n",
    "df_res = df_res.sort_values(by=['srch_id', 'pred_grades'], ascending=[True, False], inplace=False)\n",
    "df_res = df_res.merge(test_ideal, on=['srch_id', 'prop_id'], how='left')\n",
    "\n",
    "final_ndcg = calc_NDCG(test_ideal, df_res)\n",
    "# wandb.log({'ndcg_final': final_ndcg})\n",
    "print(f\"result final:{final_ndcg}\")\n",
    "# wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the scores\n",
    "# test = X_val\n",
    "test = df_test\n",
    "test_input = test.drop(['srch_id'], axis=1)\n",
    "df_res = test\n",
    "\n",
    "\n",
    "print(\"Predicting...\")\n",
    "y_pred = best_ranker.predict(test_input)\n",
    "df_res['pred_grades'] = y_pred\n",
    "print(\"Done predicting\")\n",
    "\n",
    "df_res = df_res.sort_values(by=['srch_id', 'pred_grades'], ascending=[True, False], inplace=False)\n",
    "\n",
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lgbm_submission = df_res[['srch_id', 'prop_id']]\n",
    "lgbm_submission.to_csv(config['PATH']['SUBMISSION_DIR'] + '/lgbm_submission_optuna.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"RF: {calc_NDCG(test_ideal, pred_ideal_rf)}\\n,XGB: {calc_NDCG(test_ideal, pred_xgb_optimized)},\\nRandom: {calc_NDCG(test_ideal, pred_random)}\")\n",
    "print(f\"XGB: {calc_NDCG(df_ideal, pred_xgb)}, Random: {calc_NDCG(test_ideal, pred_random)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna + XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize XGB with optuna\n",
    "import optuna\n",
    "from functools import partial\n",
    "\n",
    "def objective(trial, X_train, y_train, X_test, test_ideal):\n",
    "    y_train_xgb = y_train.astype(int)\n",
    "    y_train_xgb[y_train == 5] = 2\n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"multi:softprob\",\n",
    "        \"random_state\": 42,\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0, 1),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-4, 1e-1, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-4, 1e-1, log=True),\n",
    "    }\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier(**params)\n",
    "    xgb_model.fit(X_train, y_train_xgb)\n",
    "\n",
    "    pred_xgb = constructs_predictions(xgb_model, X_test, ideal_df=test_ideal)\n",
    "    ndcg = calc_NDCG(test_ideal, pred_xgb)\n",
    "\n",
    "    return ndcg\n",
    "\n",
    "print(\"Training XGB\")\n",
    "# Assuming you have defined X_train, y_train, X_test, and test_ideal before this point.\n",
    "\n",
    "# Wrap the objective function with the input data\n",
    "objective_with_data = partial(objective, X_train=X_train, y_train=y_train, X_test=X_test, test_ideal=test_ideal)\n",
    "\n",
    "# Create an Optuna study and optimize the objective function\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective_with_data, n_trials=20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "y_train_xgb = y_train.astype(int)\n",
    "y_train_xgb[y_train == 5] = 2\n",
    "\n",
    "best_params = study.best_params\n",
    "xgb_model_optimized = xgb.XGBClassifier(objective=\"multi:softprob\", random_state=42, **best_params)\n",
    "xgb_model_optimized.fit(X_train, y_train_xgb)\n",
    "\n",
    "# Evaluate the optimized model\n",
    "pred_xgb_optimized = constructs_predictions(xgb_model_optimized, X_test, ideal_df=test_ideal)\n",
    "pred_xgb_submission = constructs_predictions(xgb_model_optimized, df_test)\n",
    "print(f\"XGB Optimized: {calc_NDCG(test_ideal, pred_xgb_optimized)}\")\n",
    "\n",
    "# pred_submission.to_csv(config['PATH']['DATA_DIR'] + '/submission_RF.csv', index=False)\n",
    "pred_xgb_submission.to_csv(config['PATH']['DATA_DIR'] + '/submission_XGB.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "3c2f59e369251f7d3cab5e49b0f40adb27d0cea1bb07f20083413ee2433d097b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
