{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "788da39c",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95539adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing\n",
    "import timeit\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a95be1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load data\n",
    "df_train = pd.read_parquet('../data/training_set.parquet', engine = 'auto')\n",
    "#df_train = df_train[df_train['srch_id'] < 10000]\n",
    "df_test = pd.read_parquet('../data/test_set.parquet', engine = 'auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581da370",
   "metadata": {},
   "source": [
    "<h1>Data prep<h1\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "671eca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_score(df):\n",
    "    df.loc[:, 'score'] = np.zeros(len(df))\n",
    "    df.loc[df['click_bool'] == 1, 'score'] = 1\n",
    "    df.loc[df['booking_bool'] == 1, 'score'] = 5\n",
    "    return df\n",
    "\n",
    "# Add features for hour, day and month.\n",
    "def date_time(df):\n",
    "    df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "    df['hour'] = df['date_time'].dt.hour\n",
    "    df['day'] = df['date_time'].dt.day\n",
    "    df['month'] = df['date_time'].dt.month\n",
    "    df = df.drop('date_time', axis=1)\n",
    "    return df\n",
    "\n",
    "def remove_cols(df, cols):\n",
    "    return df.drop(cols, axis=1)\n",
    "\n",
    "def remove_cols_nan(df, limit):\n",
    "    for col in df.columns:\n",
    "        if len(df[col]) * limit < df[col].isna().sum():\n",
    "            df = df.drop(col, axis=1)\n",
    "    return df\n",
    "\n",
    "# Add column with a ranking for each property in a search based on another column.\n",
    "def create_rank_feature(df, col):\n",
    "    df['rank_' + str(col)] = df.groupby('srch_id')[col].rank(ascending=False)\n",
    "    return df\n",
    "\n",
    "# location score 2 has missing values for property on some rows while some rows have a score.\n",
    "# take average of rows that do have a score. reduces nans from 1090348 to 182213.\n",
    "def fill_location_score_2(df):\n",
    "    df['prop_location_score2'] = df.groupby('prop_id')['prop_location_score2'].transform(lambda x: x.fillna(x.mean()))\n",
    "    return df\n",
    "\n",
    "# adds a normalised version of a column based on a chosen grouping.\n",
    "def add_normalized_column(df, col, group):\n",
    "    df['norm_' + str(col) + \"_\" + str(group)] = (\n",
    "        (df[col] - df.groupby(group)[col].transform('mean')) \n",
    "        / df.groupby(group)[col].transform('std')\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def prep_data(df, target_cols, test=False):\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    if not test:\n",
    "        print('add score')\n",
    "        df_new = make_score(df_new)\n",
    "        df_new = df_new.drop(target_cols, axis=1)\n",
    "  \n",
    "\n",
    "    print('add hour, day, month')\n",
    "    df_new = date_time(df_new)\n",
    "    #df_new = remove_cols_nan(df_new, 0.9)\n",
    "    \n",
    "    # difference features assumes that users purchase in same category as history.\n",
    "    print('add difference features')\n",
    "    df_new['usd_diff'] = df_new['visitor_hist_adr_usd'] - df_new['price_usd']\n",
    "    df_new['star_diff'] = df_new['visitor_hist_starrating'] - df_new['prop_starrating']    \n",
    "    df_new['log_price_diff'] = df_new['prop_log_historical_price'] - np.log(df_new['price_usd'])\n",
    "    \n",
    "    # count variables \n",
    "    # theory: A property that is in more searches is purchased more often.\n",
    "    df_new['prop_id_count'] = df.groupby('prop_id')['prop_id'].transform('count')\n",
    "    \n",
    "    df_new['srch_destination_id_count'] = df.groupby('srch_destination_id')['srch_destination_id'].transform('count')\n",
    "\n",
    "    \n",
    "    # ranking features\n",
    "    print('add rank features')\n",
    "    df_new = create_rank_feature(df_new, 'price_usd')\n",
    "    df_new = create_rank_feature(df_new, 'prop_starrating')\n",
    "    df_new = create_rank_feature(df_new, 'prop_review_score')\n",
    "    df_new = create_rank_feature(df_new, 'prop_location_score1')\n",
    "    df_new = create_rank_feature(df_new, 'prop_location_score2')\n",
    "    \n",
    "    print(\"fill ls2\")\n",
    "    df_new = fill_location_score_2(df_new)\n",
    "    \n",
    "    # Fill distance with mean.\n",
    "    df['orig_destination_distance'].fillna(df['orig_destination_distance'].mean(),inplace=True)\n",
    "    \n",
    "    \n",
    "    print(\"remove nan\")\n",
    "    # Fill rest of nan values with lowest.\n",
    "    for i in df_new.columns[df_new.isnull().any(axis=0)]:\n",
    "        df_new[i].fillna(0,inplace=True)\n",
    "    \n",
    "    print('add normalised features')\n",
    "    groups = ['srch_id', 'prop_country_id', 'srch_destination_id', 'srch_length_of_stay', 'srch_booking_window', 'month']\n",
    "    targets = ['price_usd', 'prop_starrating', 'prop_review_score', 'prop_location_score1', 'prop_location_score2']\n",
    "    for group in groups:\n",
    "        for target in targets:\n",
    "            df_new = add_normalized_column(df_new, target, group)\n",
    "   \n",
    "    df_new = add_normalized_column(df_new, 'price_usd', 'prop_id')\n",
    "    df_new = add_normalized_column(df_new, 'price_usd', 'srch_room_count')\n",
    "    \n",
    "    # Normalisation might create nans\n",
    "    print(\"remove nan\\n\")\n",
    "    for i in df_new.columns[df_new.isnull().any(axis=0)]:\n",
    "        df_new[i].fillna(0,inplace=True)\n",
    "        \n",
    "    for c in categorical_features:\n",
    "        df_new[c] = df_new[c].astype('category')\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d4d6eb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepping test data\n",
      "add hour, day, month\n",
      "add difference features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hrvanelderen/anaconda3/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add rank features\n",
      "fill ls2\n",
      "remove nan\n",
      "add normalised features\n",
      "remove nan\n",
      "\n",
      "Prepping training data\n",
      "add score\n",
      "add hour, day, month\n",
      "add difference features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hrvanelderen/anaconda3/lib/python3.10/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add rank features\n",
      "fill ls2\n",
      "remove nan\n",
      "add normalised features\n",
      "remove nan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_cols = ['click_bool', 'booking_bool', 'gross_bookings_usd', 'position']\n",
    "categorical_features = ['hour', 'day', 'month', 'site_id', 'visitor_location_country_id', 'prop_country_id', 'prop_id', 'srch_destination_id']\n",
    "\n",
    "print(\"Prepping test data\")\n",
    "df_test = prep_data(df_test, target_cols, True)\n",
    "print(\"Prepping training data\")\n",
    "df_train = prep_data(df_train, target_cols, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b643f96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da28ce6",
   "metadata": {},
   "source": [
    "<h1>Data split<h1\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8daeec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "splitter = GroupShuffleSplit(test_size=0.2, n_splits=2, random_state = 7)\n",
    "split = splitter.split(df_train, groups=df_train['srch_id'])\n",
    "train_inds, test_inds = next(split)\n",
    "\n",
    "df_ideal = df_train.iloc[test_inds].copy().sort_values(by=['srch_id', 'score'], ascending=[True, False], inplace=False)\n",
    "\n",
    "X = df_train.drop(['score'], axis=1)\n",
    "y = df_train['score']\n",
    "\n",
    "X_train, X_test, y_train, y_test, test_ideal = X.iloc[train_inds], X.iloc[test_inds], y.iloc[train_inds], y.iloc[test_inds], df_ideal\n",
    "\n",
    "train_groups = X_train.groupby('srch_id').size().to_frame('size')['size'].to_numpy()\n",
    "\n",
    "test_groups = X_test.groupby('srch_id').size().to_frame('size')['size'].to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2360896",
   "metadata": {},
   "source": [
    "<h1>Training <h1\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d562c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"objective\": \"lambdarank\",\n",
    "    \"boosting_type\":\"dart\",\n",
    "    \"metric\":\"ndcg\",\n",
    "    'n_estimators': 500, \n",
    "    'max_depth': 10, \n",
    "    'learning_rate': 0.08925380432712779, \n",
    "    'subsample': 0.523890758165789, \n",
    "    'colsample_bytree': 0.5433556425106324, \n",
    "    'feature_fraction': 0.9677058301342538,\n",
    "    'reg_alpha': 0.00011669441178850782, \n",
    "    'reg_lambda': 0.008250891056480582\n",
    "}\n",
    "\n",
    "gbm = lgb.LGBMRanker(**params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6f71f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9677058301342538, colsample_bytree=0.5433556425106324 will be ignored. Current value: feature_fraction=0.9677058301342538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hrvanelderen/anaconda3/lib/python3.10/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/Users/hrvanelderen/anaconda3/lib/python3.10/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's ndcg@5: 0.314398\tvalid_1's ndcg@5: 0.308587\n",
      "[2]\ttraining's ndcg@5: 0.349608\tvalid_1's ndcg@5: 0.337742\n",
      "[3]\ttraining's ndcg@5: 0.362457\tvalid_1's ndcg@5: 0.348189\n",
      "[4]\ttraining's ndcg@5: 0.37095\tvalid_1's ndcg@5: 0.354145\n",
      "[5]\ttraining's ndcg@5: 0.377096\tvalid_1's ndcg@5: 0.35734\n",
      "[6]\ttraining's ndcg@5: 0.380535\tvalid_1's ndcg@5: 0.359582\n",
      "[7]\ttraining's ndcg@5: 0.384123\tvalid_1's ndcg@5: 0.361027\n",
      "[8]\ttraining's ndcg@5: 0.384617\tvalid_1's ndcg@5: 0.361647\n",
      "[9]\ttraining's ndcg@5: 0.388722\tvalid_1's ndcg@5: 0.364217\n",
      "[10]\ttraining's ndcg@5: 0.392096\tvalid_1's ndcg@5: 0.365759\n",
      "[11]\ttraining's ndcg@5: 0.39443\tvalid_1's ndcg@5: 0.367582\n",
      "[12]\ttraining's ndcg@5: 0.394529\tvalid_1's ndcg@5: 0.368003\n",
      "[13]\ttraining's ndcg@5: 0.395835\tvalid_1's ndcg@5: 0.368471\n",
      "[14]\ttraining's ndcg@5: 0.397747\tvalid_1's ndcg@5: 0.36981\n",
      "[15]\ttraining's ndcg@5: 0.399365\tvalid_1's ndcg@5: 0.370259\n",
      "[16]\ttraining's ndcg@5: 0.401667\tvalid_1's ndcg@5: 0.371299\n",
      "[17]\ttraining's ndcg@5: 0.403048\tvalid_1's ndcg@5: 0.37218\n",
      "[18]\ttraining's ndcg@5: 0.404301\tvalid_1's ndcg@5: 0.37297\n",
      "[19]\ttraining's ndcg@5: 0.40613\tvalid_1's ndcg@5: 0.373067\n",
      "[20]\ttraining's ndcg@5: 0.407508\tvalid_1's ndcg@5: 0.373531\n",
      "[21]\ttraining's ndcg@5: 0.408007\tvalid_1's ndcg@5: 0.37375\n",
      "[22]\ttraining's ndcg@5: 0.40941\tvalid_1's ndcg@5: 0.375043\n",
      "[23]\ttraining's ndcg@5: 0.410943\tvalid_1's ndcg@5: 0.375953\n",
      "[24]\ttraining's ndcg@5: 0.411722\tvalid_1's ndcg@5: 0.376238\n",
      "[25]\ttraining's ndcg@5: 0.412935\tvalid_1's ndcg@5: 0.376658\n",
      "[26]\ttraining's ndcg@5: 0.41402\tvalid_1's ndcg@5: 0.37769\n",
      "[27]\ttraining's ndcg@5: 0.415642\tvalid_1's ndcg@5: 0.378635\n",
      "[28]\ttraining's ndcg@5: 0.415695\tvalid_1's ndcg@5: 0.378537\n",
      "[29]\ttraining's ndcg@5: 0.417076\tvalid_1's ndcg@5: 0.37881\n",
      "[30]\ttraining's ndcg@5: 0.418713\tvalid_1's ndcg@5: 0.378994\n",
      "[31]\ttraining's ndcg@5: 0.419196\tvalid_1's ndcg@5: 0.379604\n",
      "[32]\ttraining's ndcg@5: 0.420077\tvalid_1's ndcg@5: 0.379564\n",
      "[33]\ttraining's ndcg@5: 0.421047\tvalid_1's ndcg@5: 0.380291\n",
      "[34]\ttraining's ndcg@5: 0.421911\tvalid_1's ndcg@5: 0.380567\n",
      "[35]\ttraining's ndcg@5: 0.421922\tvalid_1's ndcg@5: 0.380389\n",
      "[36]\ttraining's ndcg@5: 0.42193\tvalid_1's ndcg@5: 0.380052\n",
      "[37]\ttraining's ndcg@5: 0.422807\tvalid_1's ndcg@5: 0.380173\n",
      "[38]\ttraining's ndcg@5: 0.424328\tvalid_1's ndcg@5: 0.380803\n",
      "[39]\ttraining's ndcg@5: 0.425933\tvalid_1's ndcg@5: 0.381344\n",
      "[40]\ttraining's ndcg@5: 0.426406\tvalid_1's ndcg@5: 0.381602\n",
      "[41]\ttraining's ndcg@5: 0.426441\tvalid_1's ndcg@5: 0.381676\n",
      "[42]\ttraining's ndcg@5: 0.427744\tvalid_1's ndcg@5: 0.381742\n",
      "[43]\ttraining's ndcg@5: 0.427671\tvalid_1's ndcg@5: 0.381783\n",
      "[44]\ttraining's ndcg@5: 0.42862\tvalid_1's ndcg@5: 0.381866\n",
      "[45]\ttraining's ndcg@5: 0.42958\tvalid_1's ndcg@5: 0.382423\n",
      "[46]\ttraining's ndcg@5: 0.430079\tvalid_1's ndcg@5: 0.382304\n",
      "[47]\ttraining's ndcg@5: 0.431033\tvalid_1's ndcg@5: 0.382737\n",
      "[48]\ttraining's ndcg@5: 0.431674\tvalid_1's ndcg@5: 0.382978\n",
      "[49]\ttraining's ndcg@5: 0.431797\tvalid_1's ndcg@5: 0.38271\n",
      "[50]\ttraining's ndcg@5: 0.431655\tvalid_1's ndcg@5: 0.382714\n",
      "[51]\ttraining's ndcg@5: 0.432361\tvalid_1's ndcg@5: 0.383259\n",
      "[52]\ttraining's ndcg@5: 0.432752\tvalid_1's ndcg@5: 0.383106\n",
      "[53]\ttraining's ndcg@5: 0.432905\tvalid_1's ndcg@5: 0.383056\n",
      "[54]\ttraining's ndcg@5: 0.433276\tvalid_1's ndcg@5: 0.383466\n",
      "[55]\ttraining's ndcg@5: 0.434454\tvalid_1's ndcg@5: 0.383686\n",
      "[56]\ttraining's ndcg@5: 0.434415\tvalid_1's ndcg@5: 0.383606\n",
      "[57]\ttraining's ndcg@5: 0.435008\tvalid_1's ndcg@5: 0.383607\n",
      "[58]\ttraining's ndcg@5: 0.435408\tvalid_1's ndcg@5: 0.383857\n",
      "[59]\ttraining's ndcg@5: 0.435556\tvalid_1's ndcg@5: 0.383593\n",
      "[60]\ttraining's ndcg@5: 0.436122\tvalid_1's ndcg@5: 0.383798\n",
      "[61]\ttraining's ndcg@5: 0.436288\tvalid_1's ndcg@5: 0.383779\n",
      "[62]\ttraining's ndcg@5: 0.436573\tvalid_1's ndcg@5: 0.384193\n",
      "[63]\ttraining's ndcg@5: 0.437027\tvalid_1's ndcg@5: 0.384324\n",
      "[64]\ttraining's ndcg@5: 0.437094\tvalid_1's ndcg@5: 0.38445\n",
      "[65]\ttraining's ndcg@5: 0.4369\tvalid_1's ndcg@5: 0.384427\n",
      "[66]\ttraining's ndcg@5: 0.437453\tvalid_1's ndcg@5: 0.384437\n",
      "[67]\ttraining's ndcg@5: 0.438364\tvalid_1's ndcg@5: 0.384416\n",
      "[68]\ttraining's ndcg@5: 0.439059\tvalid_1's ndcg@5: 0.384651\n",
      "[69]\ttraining's ndcg@5: 0.43893\tvalid_1's ndcg@5: 0.384781\n",
      "[70]\ttraining's ndcg@5: 0.439074\tvalid_1's ndcg@5: 0.384855\n",
      "[71]\ttraining's ndcg@5: 0.439579\tvalid_1's ndcg@5: 0.38475\n",
      "[72]\ttraining's ndcg@5: 0.440075\tvalid_1's ndcg@5: 0.38488\n",
      "[73]\ttraining's ndcg@5: 0.440545\tvalid_1's ndcg@5: 0.385117\n",
      "[74]\ttraining's ndcg@5: 0.440517\tvalid_1's ndcg@5: 0.385022\n",
      "[75]\ttraining's ndcg@5: 0.440876\tvalid_1's ndcg@5: 0.385063\n",
      "[76]\ttraining's ndcg@5: 0.441286\tvalid_1's ndcg@5: 0.385376\n",
      "[77]\ttraining's ndcg@5: 0.441212\tvalid_1's ndcg@5: 0.385311\n",
      "[78]\ttraining's ndcg@5: 0.441284\tvalid_1's ndcg@5: 0.385309\n",
      "[79]\ttraining's ndcg@5: 0.441852\tvalid_1's ndcg@5: 0.385584\n",
      "[80]\ttraining's ndcg@5: 0.44236\tvalid_1's ndcg@5: 0.385483\n",
      "[81]\ttraining's ndcg@5: 0.442255\tvalid_1's ndcg@5: 0.385334\n",
      "[82]\ttraining's ndcg@5: 0.442782\tvalid_1's ndcg@5: 0.385237\n",
      "[83]\ttraining's ndcg@5: 0.442812\tvalid_1's ndcg@5: 0.385487\n",
      "[84]\ttraining's ndcg@5: 0.442942\tvalid_1's ndcg@5: 0.385655\n",
      "[85]\ttraining's ndcg@5: 0.442925\tvalid_1's ndcg@5: 0.38583\n",
      "[86]\ttraining's ndcg@5: 0.443377\tvalid_1's ndcg@5: 0.385912\n",
      "[87]\ttraining's ndcg@5: 0.443648\tvalid_1's ndcg@5: 0.385774\n",
      "[88]\ttraining's ndcg@5: 0.443591\tvalid_1's ndcg@5: 0.385884\n",
      "[89]\ttraining's ndcg@5: 0.443731\tvalid_1's ndcg@5: 0.385839\n",
      "[90]\ttraining's ndcg@5: 0.443605\tvalid_1's ndcg@5: 0.385816\n",
      "[91]\ttraining's ndcg@5: 0.443846\tvalid_1's ndcg@5: 0.385963\n",
      "[92]\ttraining's ndcg@5: 0.444823\tvalid_1's ndcg@5: 0.386479\n",
      "[93]\ttraining's ndcg@5: 0.445606\tvalid_1's ndcg@5: 0.386244\n",
      "[94]\ttraining's ndcg@5: 0.445386\tvalid_1's ndcg@5: 0.386046\n",
      "[95]\ttraining's ndcg@5: 0.445395\tvalid_1's ndcg@5: 0.385987\n",
      "[96]\ttraining's ndcg@5: 0.445444\tvalid_1's ndcg@5: 0.386087\n",
      "[97]\ttraining's ndcg@5: 0.445663\tvalid_1's ndcg@5: 0.386067\n",
      "[98]\ttraining's ndcg@5: 0.445787\tvalid_1's ndcg@5: 0.386176\n",
      "[99]\ttraining's ndcg@5: 0.446045\tvalid_1's ndcg@5: 0.385959\n",
      "[100]\ttraining's ndcg@5: 0.446623\tvalid_1's ndcg@5: 0.386211\n",
      "[101]\ttraining's ndcg@5: 0.446737\tvalid_1's ndcg@5: 0.386052\n",
      "[102]\ttraining's ndcg@5: 0.446637\tvalid_1's ndcg@5: 0.386077\n",
      "[103]\ttraining's ndcg@5: 0.446516\tvalid_1's ndcg@5: 0.386056\n",
      "[104]\ttraining's ndcg@5: 0.446822\tvalid_1's ndcg@5: 0.386619\n",
      "[105]\ttraining's ndcg@5: 0.446963\tvalid_1's ndcg@5: 0.386573\n",
      "[106]\ttraining's ndcg@5: 0.44697\tvalid_1's ndcg@5: 0.386561\n",
      "[107]\ttraining's ndcg@5: 0.447017\tvalid_1's ndcg@5: 0.386647\n",
      "[108]\ttraining's ndcg@5: 0.447199\tvalid_1's ndcg@5: 0.386697\n",
      "[109]\ttraining's ndcg@5: 0.447057\tvalid_1's ndcg@5: 0.386651\n",
      "[110]\ttraining's ndcg@5: 0.447073\tvalid_1's ndcg@5: 0.386622\n",
      "[111]\ttraining's ndcg@5: 0.447659\tvalid_1's ndcg@5: 0.386769\n",
      "[112]\ttraining's ndcg@5: 0.448247\tvalid_1's ndcg@5: 0.386901\n",
      "[113]\ttraining's ndcg@5: 0.448769\tvalid_1's ndcg@5: 0.386757\n",
      "[114]\ttraining's ndcg@5: 0.44893\tvalid_1's ndcg@5: 0.386772\n",
      "[115]\ttraining's ndcg@5: 0.449358\tvalid_1's ndcg@5: 0.38695\n",
      "[116]\ttraining's ndcg@5: 0.450032\tvalid_1's ndcg@5: 0.387031\n",
      "[117]\ttraining's ndcg@5: 0.449997\tvalid_1's ndcg@5: 0.38702\n",
      "[118]\ttraining's ndcg@5: 0.450503\tvalid_1's ndcg@5: 0.386731\n",
      "[119]\ttraining's ndcg@5: 0.450421\tvalid_1's ndcg@5: 0.386762\n",
      "[120]\ttraining's ndcg@5: 0.450389\tvalid_1's ndcg@5: 0.38646\n",
      "[121]\ttraining's ndcg@5: 0.450248\tvalid_1's ndcg@5: 0.386637\n",
      "[122]\ttraining's ndcg@5: 0.450214\tvalid_1's ndcg@5: 0.386503\n",
      "[123]\ttraining's ndcg@5: 0.450863\tvalid_1's ndcg@5: 0.38643\n",
      "[124]\ttraining's ndcg@5: 0.45166\tvalid_1's ndcg@5: 0.386632\n",
      "[125]\ttraining's ndcg@5: 0.451592\tvalid_1's ndcg@5: 0.386749\n",
      "[126]\ttraining's ndcg@5: 0.451525\tvalid_1's ndcg@5: 0.386721\n",
      "[127]\ttraining's ndcg@5: 0.45214\tvalid_1's ndcg@5: 0.386929\n",
      "[128]\ttraining's ndcg@5: 0.452153\tvalid_1's ndcg@5: 0.386815\n",
      "[129]\ttraining's ndcg@5: 0.452343\tvalid_1's ndcg@5: 0.387232\n",
      "[130]\ttraining's ndcg@5: 0.452902\tvalid_1's ndcg@5: 0.387456\n",
      "[131]\ttraining's ndcg@5: 0.452973\tvalid_1's ndcg@5: 0.387463\n",
      "[132]\ttraining's ndcg@5: 0.453657\tvalid_1's ndcg@5: 0.387026\n",
      "[133]\ttraining's ndcg@5: 0.453907\tvalid_1's ndcg@5: 0.387076\n",
      "[134]\ttraining's ndcg@5: 0.453971\tvalid_1's ndcg@5: 0.387047\n",
      "[135]\ttraining's ndcg@5: 0.454476\tvalid_1's ndcg@5: 0.387339\n",
      "[136]\ttraining's ndcg@5: 0.454774\tvalid_1's ndcg@5: 0.387483\n",
      "[137]\ttraining's ndcg@5: 0.454801\tvalid_1's ndcg@5: 0.387338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[138]\ttraining's ndcg@5: 0.454974\tvalid_1's ndcg@5: 0.387388\n",
      "[139]\ttraining's ndcg@5: 0.455674\tvalid_1's ndcg@5: 0.387377\n",
      "[140]\ttraining's ndcg@5: 0.455641\tvalid_1's ndcg@5: 0.387262\n",
      "[141]\ttraining's ndcg@5: 0.455636\tvalid_1's ndcg@5: 0.387393\n",
      "[142]\ttraining's ndcg@5: 0.455537\tvalid_1's ndcg@5: 0.387324\n",
      "[143]\ttraining's ndcg@5: 0.455981\tvalid_1's ndcg@5: 0.387969\n",
      "[144]\ttraining's ndcg@5: 0.456064\tvalid_1's ndcg@5: 0.387996\n",
      "[145]\ttraining's ndcg@5: 0.456478\tvalid_1's ndcg@5: 0.388114\n",
      "[146]\ttraining's ndcg@5: 0.456379\tvalid_1's ndcg@5: 0.387785\n",
      "[147]\ttraining's ndcg@5: 0.456399\tvalid_1's ndcg@5: 0.38785\n",
      "[148]\ttraining's ndcg@5: 0.456422\tvalid_1's ndcg@5: 0.387982\n",
      "[149]\ttraining's ndcg@5: 0.456939\tvalid_1's ndcg@5: 0.388091\n",
      "[150]\ttraining's ndcg@5: 0.457441\tvalid_1's ndcg@5: 0.388168\n",
      "[151]\ttraining's ndcg@5: 0.457567\tvalid_1's ndcg@5: 0.388135\n",
      "[152]\ttraining's ndcg@5: 0.457631\tvalid_1's ndcg@5: 0.388316\n",
      "[153]\ttraining's ndcg@5: 0.458019\tvalid_1's ndcg@5: 0.387932\n",
      "[154]\ttraining's ndcg@5: 0.458018\tvalid_1's ndcg@5: 0.387831\n",
      "[155]\ttraining's ndcg@5: 0.458322\tvalid_1's ndcg@5: 0.388202\n",
      "[156]\ttraining's ndcg@5: 0.458191\tvalid_1's ndcg@5: 0.388092\n",
      "[157]\ttraining's ndcg@5: 0.458305\tvalid_1's ndcg@5: 0.388179\n",
      "[158]\ttraining's ndcg@5: 0.458163\tvalid_1's ndcg@5: 0.38816\n",
      "[159]\ttraining's ndcg@5: 0.458066\tvalid_1's ndcg@5: 0.388167\n",
      "[160]\ttraining's ndcg@5: 0.457987\tvalid_1's ndcg@5: 0.388211\n",
      "[161]\ttraining's ndcg@5: 0.458189\tvalid_1's ndcg@5: 0.388043\n",
      "[162]\ttraining's ndcg@5: 0.458293\tvalid_1's ndcg@5: 0.387942\n",
      "[163]\ttraining's ndcg@5: 0.458607\tvalid_1's ndcg@5: 0.387878\n",
      "[164]\ttraining's ndcg@5: 0.458317\tvalid_1's ndcg@5: 0.388167\n",
      "[165]\ttraining's ndcg@5: 0.458376\tvalid_1's ndcg@5: 0.388521\n",
      "[166]\ttraining's ndcg@5: 0.458359\tvalid_1's ndcg@5: 0.388597\n",
      "[167]\ttraining's ndcg@5: 0.458547\tvalid_1's ndcg@5: 0.38871\n",
      "[168]\ttraining's ndcg@5: 0.459206\tvalid_1's ndcg@5: 0.389228\n",
      "[169]\ttraining's ndcg@5: 0.459648\tvalid_1's ndcg@5: 0.389325\n",
      "[170]\ttraining's ndcg@5: 0.459717\tvalid_1's ndcg@5: 0.389241\n",
      "[171]\ttraining's ndcg@5: 0.460472\tvalid_1's ndcg@5: 0.388976\n",
      "[172]\ttraining's ndcg@5: 0.460629\tvalid_1's ndcg@5: 0.389082\n",
      "[173]\ttraining's ndcg@5: 0.4609\tvalid_1's ndcg@5: 0.389039\n",
      "[174]\ttraining's ndcg@5: 0.461455\tvalid_1's ndcg@5: 0.389326\n",
      "[175]\ttraining's ndcg@5: 0.462126\tvalid_1's ndcg@5: 0.389325\n",
      "[176]\ttraining's ndcg@5: 0.462033\tvalid_1's ndcg@5: 0.389353\n",
      "[177]\ttraining's ndcg@5: 0.462049\tvalid_1's ndcg@5: 0.389445\n",
      "[178]\ttraining's ndcg@5: 0.462041\tvalid_1's ndcg@5: 0.389317\n",
      "[179]\ttraining's ndcg@5: 0.462643\tvalid_1's ndcg@5: 0.389756\n",
      "[180]\ttraining's ndcg@5: 0.462535\tvalid_1's ndcg@5: 0.389781\n",
      "[181]\ttraining's ndcg@5: 0.462994\tvalid_1's ndcg@5: 0.389657\n",
      "[182]\ttraining's ndcg@5: 0.463522\tvalid_1's ndcg@5: 0.390106\n",
      "[183]\ttraining's ndcg@5: 0.464438\tvalid_1's ndcg@5: 0.390192\n",
      "[184]\ttraining's ndcg@5: 0.464714\tvalid_1's ndcg@5: 0.38996\n",
      "[185]\ttraining's ndcg@5: 0.464651\tvalid_1's ndcg@5: 0.389796\n",
      "[186]\ttraining's ndcg@5: 0.465075\tvalid_1's ndcg@5: 0.38987\n",
      "[187]\ttraining's ndcg@5: 0.465331\tvalid_1's ndcg@5: 0.39018\n",
      "[188]\ttraining's ndcg@5: 0.466175\tvalid_1's ndcg@5: 0.390148\n",
      "[189]\ttraining's ndcg@5: 0.466142\tvalid_1's ndcg@5: 0.390224\n",
      "[190]\ttraining's ndcg@5: 0.466132\tvalid_1's ndcg@5: 0.390257\n",
      "[191]\ttraining's ndcg@5: 0.466441\tvalid_1's ndcg@5: 0.390175\n",
      "[192]\ttraining's ndcg@5: 0.466909\tvalid_1's ndcg@5: 0.390329\n",
      "[193]\ttraining's ndcg@5: 0.467407\tvalid_1's ndcg@5: 0.390332\n",
      "[194]\ttraining's ndcg@5: 0.467331\tvalid_1's ndcg@5: 0.390278\n",
      "[195]\ttraining's ndcg@5: 0.467317\tvalid_1's ndcg@5: 0.3902\n",
      "[196]\ttraining's ndcg@5: 0.467569\tvalid_1's ndcg@5: 0.38999\n",
      "[197]\ttraining's ndcg@5: 0.467569\tvalid_1's ndcg@5: 0.390007\n",
      "[198]\ttraining's ndcg@5: 0.467551\tvalid_1's ndcg@5: 0.389979\n",
      "[199]\ttraining's ndcg@5: 0.468017\tvalid_1's ndcg@5: 0.39035\n",
      "[200]\ttraining's ndcg@5: 0.46794\tvalid_1's ndcg@5: 0.390271\n",
      "[201]\ttraining's ndcg@5: 0.467879\tvalid_1's ndcg@5: 0.390425\n",
      "[202]\ttraining's ndcg@5: 0.467891\tvalid_1's ndcg@5: 0.390363\n",
      "[203]\ttraining's ndcg@5: 0.468284\tvalid_1's ndcg@5: 0.390459\n",
      "[204]\ttraining's ndcg@5: 0.468156\tvalid_1's ndcg@5: 0.390551\n",
      "[205]\ttraining's ndcg@5: 0.468011\tvalid_1's ndcg@5: 0.390471\n",
      "[206]\ttraining's ndcg@5: 0.468394\tvalid_1's ndcg@5: 0.390157\n",
      "[207]\ttraining's ndcg@5: 0.468385\tvalid_1's ndcg@5: 0.390275\n",
      "[208]\ttraining's ndcg@5: 0.468928\tvalid_1's ndcg@5: 0.390532\n",
      "[209]\ttraining's ndcg@5: 0.468827\tvalid_1's ndcg@5: 0.390531\n",
      "[210]\ttraining's ndcg@5: 0.468876\tvalid_1's ndcg@5: 0.390395\n",
      "[211]\ttraining's ndcg@5: 0.468782\tvalid_1's ndcg@5: 0.390422\n",
      "[212]\ttraining's ndcg@5: 0.469189\tvalid_1's ndcg@5: 0.39029\n",
      "[213]\ttraining's ndcg@5: 0.469512\tvalid_1's ndcg@5: 0.390081\n",
      "[214]\ttraining's ndcg@5: 0.470079\tvalid_1's ndcg@5: 0.389995\n",
      "[215]\ttraining's ndcg@5: 0.470385\tvalid_1's ndcg@5: 0.390435\n",
      "[216]\ttraining's ndcg@5: 0.470288\tvalid_1's ndcg@5: 0.390457\n",
      "[217]\ttraining's ndcg@5: 0.470547\tvalid_1's ndcg@5: 0.390615\n",
      "[218]\ttraining's ndcg@5: 0.471078\tvalid_1's ndcg@5: 0.391131\n",
      "[219]\ttraining's ndcg@5: 0.47105\tvalid_1's ndcg@5: 0.390993\n",
      "[220]\ttraining's ndcg@5: 0.471726\tvalid_1's ndcg@5: 0.391083\n",
      "[221]\ttraining's ndcg@5: 0.471622\tvalid_1's ndcg@5: 0.39107\n",
      "[222]\ttraining's ndcg@5: 0.471974\tvalid_1's ndcg@5: 0.391203\n",
      "[223]\ttraining's ndcg@5: 0.472492\tvalid_1's ndcg@5: 0.391371\n",
      "[224]\ttraining's ndcg@5: 0.47244\tvalid_1's ndcg@5: 0.391247\n",
      "[225]\ttraining's ndcg@5: 0.47231\tvalid_1's ndcg@5: 0.391331\n",
      "[226]\ttraining's ndcg@5: 0.47221\tvalid_1's ndcg@5: 0.391139\n",
      "[227]\ttraining's ndcg@5: 0.472251\tvalid_1's ndcg@5: 0.391155\n",
      "[228]\ttraining's ndcg@5: 0.472796\tvalid_1's ndcg@5: 0.39119\n",
      "[229]\ttraining's ndcg@5: 0.472722\tvalid_1's ndcg@5: 0.391173\n",
      "[230]\ttraining's ndcg@5: 0.472989\tvalid_1's ndcg@5: 0.391154\n",
      "[231]\ttraining's ndcg@5: 0.47364\tvalid_1's ndcg@5: 0.391466\n",
      "[232]\ttraining's ndcg@5: 0.473525\tvalid_1's ndcg@5: 0.39173\n",
      "[233]\ttraining's ndcg@5: 0.473497\tvalid_1's ndcg@5: 0.391753\n",
      "[234]\ttraining's ndcg@5: 0.473777\tvalid_1's ndcg@5: 0.391214\n",
      "[235]\ttraining's ndcg@5: 0.473699\tvalid_1's ndcg@5: 0.391107\n",
      "[236]\ttraining's ndcg@5: 0.47428\tvalid_1's ndcg@5: 0.391309\n",
      "[237]\ttraining's ndcg@5: 0.474654\tvalid_1's ndcg@5: 0.391435\n",
      "[238]\ttraining's ndcg@5: 0.47487\tvalid_1's ndcg@5: 0.391757\n",
      "[239]\ttraining's ndcg@5: 0.475206\tvalid_1's ndcg@5: 0.391703\n",
      "[240]\ttraining's ndcg@5: 0.475541\tvalid_1's ndcg@5: 0.391614\n",
      "[241]\ttraining's ndcg@5: 0.475523\tvalid_1's ndcg@5: 0.391534\n",
      "[242]\ttraining's ndcg@5: 0.47593\tvalid_1's ndcg@5: 0.392022\n",
      "[243]\ttraining's ndcg@5: 0.475905\tvalid_1's ndcg@5: 0.391967\n",
      "[244]\ttraining's ndcg@5: 0.475831\tvalid_1's ndcg@5: 0.392001\n",
      "[245]\ttraining's ndcg@5: 0.476089\tvalid_1's ndcg@5: 0.391919\n",
      "[246]\ttraining's ndcg@5: 0.476179\tvalid_1's ndcg@5: 0.391816\n",
      "[247]\ttraining's ndcg@5: 0.476843\tvalid_1's ndcg@5: 0.391672\n",
      "[248]\ttraining's ndcg@5: 0.476774\tvalid_1's ndcg@5: 0.391591\n",
      "[249]\ttraining's ndcg@5: 0.476722\tvalid_1's ndcg@5: 0.391642\n",
      "[250]\ttraining's ndcg@5: 0.477288\tvalid_1's ndcg@5: 0.391766\n",
      "[251]\ttraining's ndcg@5: 0.47728\tvalid_1's ndcg@5: 0.391787\n",
      "[252]\ttraining's ndcg@5: 0.477206\tvalid_1's ndcg@5: 0.39177\n",
      "[253]\ttraining's ndcg@5: 0.477816\tvalid_1's ndcg@5: 0.39168\n",
      "[254]\ttraining's ndcg@5: 0.478246\tvalid_1's ndcg@5: 0.391606\n",
      "[255]\ttraining's ndcg@5: 0.478709\tvalid_1's ndcg@5: 0.391769\n",
      "[256]\ttraining's ndcg@5: 0.478993\tvalid_1's ndcg@5: 0.391918\n",
      "[257]\ttraining's ndcg@5: 0.47948\tvalid_1's ndcg@5: 0.391791\n",
      "[258]\ttraining's ndcg@5: 0.479916\tvalid_1's ndcg@5: 0.392195\n",
      "[259]\ttraining's ndcg@5: 0.479914\tvalid_1's ndcg@5: 0.392206\n",
      "[260]\ttraining's ndcg@5: 0.479897\tvalid_1's ndcg@5: 0.392278\n",
      "[261]\ttraining's ndcg@5: 0.479805\tvalid_1's ndcg@5: 0.392322\n",
      "[262]\ttraining's ndcg@5: 0.480155\tvalid_1's ndcg@5: 0.392385\n",
      "[263]\ttraining's ndcg@5: 0.480629\tvalid_1's ndcg@5: 0.392185\n",
      "[264]\ttraining's ndcg@5: 0.480976\tvalid_1's ndcg@5: 0.392556\n",
      "[265]\ttraining's ndcg@5: 0.481221\tvalid_1's ndcg@5: 0.392391\n",
      "[266]\ttraining's ndcg@5: 0.481728\tvalid_1's ndcg@5: 0.392882\n",
      "[267]\ttraining's ndcg@5: 0.481699\tvalid_1's ndcg@5: 0.392801\n",
      "[268]\ttraining's ndcg@5: 0.482482\tvalid_1's ndcg@5: 0.392883\n",
      "[269]\ttraining's ndcg@5: 0.482498\tvalid_1's ndcg@5: 0.392945\n",
      "[270]\ttraining's ndcg@5: 0.482953\tvalid_1's ndcg@5: 0.392987\n",
      "[271]\ttraining's ndcg@5: 0.482872\tvalid_1's ndcg@5: 0.392929\n",
      "[272]\ttraining's ndcg@5: 0.483409\tvalid_1's ndcg@5: 0.392743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[273]\ttraining's ndcg@5: 0.483916\tvalid_1's ndcg@5: 0.392874\n",
      "[274]\ttraining's ndcg@5: 0.483813\tvalid_1's ndcg@5: 0.392864\n",
      "[275]\ttraining's ndcg@5: 0.484357\tvalid_1's ndcg@5: 0.39266\n",
      "[276]\ttraining's ndcg@5: 0.484348\tvalid_1's ndcg@5: 0.392586\n",
      "[277]\ttraining's ndcg@5: 0.484469\tvalid_1's ndcg@5: 0.392588\n",
      "[278]\ttraining's ndcg@5: 0.484397\tvalid_1's ndcg@5: 0.39251\n",
      "[279]\ttraining's ndcg@5: 0.484762\tvalid_1's ndcg@5: 0.392853\n",
      "[280]\ttraining's ndcg@5: 0.484777\tvalid_1's ndcg@5: 0.392837\n",
      "[281]\ttraining's ndcg@5: 0.48483\tvalid_1's ndcg@5: 0.392892\n",
      "[282]\ttraining's ndcg@5: 0.48529\tvalid_1's ndcg@5: 0.392944\n",
      "[283]\ttraining's ndcg@5: 0.485273\tvalid_1's ndcg@5: 0.39298\n",
      "[284]\ttraining's ndcg@5: 0.485368\tvalid_1's ndcg@5: 0.392917\n",
      "[285]\ttraining's ndcg@5: 0.485884\tvalid_1's ndcg@5: 0.392942\n",
      "[286]\ttraining's ndcg@5: 0.48644\tvalid_1's ndcg@5: 0.39319\n",
      "[287]\ttraining's ndcg@5: 0.486853\tvalid_1's ndcg@5: 0.393239\n",
      "[288]\ttraining's ndcg@5: 0.486818\tvalid_1's ndcg@5: 0.393208\n",
      "[289]\ttraining's ndcg@5: 0.487569\tvalid_1's ndcg@5: 0.393301\n",
      "[290]\ttraining's ndcg@5: 0.488156\tvalid_1's ndcg@5: 0.393309\n",
      "[291]\ttraining's ndcg@5: 0.488381\tvalid_1's ndcg@5: 0.393183\n",
      "[292]\ttraining's ndcg@5: 0.488408\tvalid_1's ndcg@5: 0.393188\n",
      "[293]\ttraining's ndcg@5: 0.488327\tvalid_1's ndcg@5: 0.393325\n",
      "[294]\ttraining's ndcg@5: 0.488827\tvalid_1's ndcg@5: 0.39301\n",
      "[295]\ttraining's ndcg@5: 0.488777\tvalid_1's ndcg@5: 0.392895\n",
      "[296]\ttraining's ndcg@5: 0.48876\tvalid_1's ndcg@5: 0.392947\n",
      "[297]\ttraining's ndcg@5: 0.48891\tvalid_1's ndcg@5: 0.392703\n",
      "[298]\ttraining's ndcg@5: 0.488901\tvalid_1's ndcg@5: 0.392736\n",
      "[299]\ttraining's ndcg@5: 0.489039\tvalid_1's ndcg@5: 0.392977\n",
      "[300]\ttraining's ndcg@5: 0.489075\tvalid_1's ndcg@5: 0.393006\n",
      "[301]\ttraining's ndcg@5: 0.489033\tvalid_1's ndcg@5: 0.392966\n",
      "[302]\ttraining's ndcg@5: 0.489437\tvalid_1's ndcg@5: 0.392944\n",
      "[303]\ttraining's ndcg@5: 0.48939\tvalid_1's ndcg@5: 0.39311\n",
      "[304]\ttraining's ndcg@5: 0.489315\tvalid_1's ndcg@5: 0.393079\n",
      "[305]\ttraining's ndcg@5: 0.489254\tvalid_1's ndcg@5: 0.393144\n",
      "[306]\ttraining's ndcg@5: 0.489245\tvalid_1's ndcg@5: 0.393044\n",
      "[307]\ttraining's ndcg@5: 0.48928\tvalid_1's ndcg@5: 0.393083\n",
      "[308]\ttraining's ndcg@5: 0.489556\tvalid_1's ndcg@5: 0.393085\n",
      "[309]\ttraining's ndcg@5: 0.489525\tvalid_1's ndcg@5: 0.393164\n",
      "[310]\ttraining's ndcg@5: 0.489663\tvalid_1's ndcg@5: 0.393149\n",
      "[311]\ttraining's ndcg@5: 0.490016\tvalid_1's ndcg@5: 0.393303\n",
      "[312]\ttraining's ndcg@5: 0.49042\tvalid_1's ndcg@5: 0.393345\n",
      "[313]\ttraining's ndcg@5: 0.490673\tvalid_1's ndcg@5: 0.393372\n",
      "[314]\ttraining's ndcg@5: 0.490629\tvalid_1's ndcg@5: 0.393229\n",
      "[315]\ttraining's ndcg@5: 0.490659\tvalid_1's ndcg@5: 0.393255\n",
      "[316]\ttraining's ndcg@5: 0.491021\tvalid_1's ndcg@5: 0.393227\n",
      "[317]\ttraining's ndcg@5: 0.491254\tvalid_1's ndcg@5: 0.393172\n",
      "[318]\ttraining's ndcg@5: 0.491406\tvalid_1's ndcg@5: 0.393005\n",
      "[319]\ttraining's ndcg@5: 0.491356\tvalid_1's ndcg@5: 0.393015\n",
      "[320]\ttraining's ndcg@5: 0.491721\tvalid_1's ndcg@5: 0.392875\n",
      "[321]\ttraining's ndcg@5: 0.491635\tvalid_1's ndcg@5: 0.392891\n",
      "[322]\ttraining's ndcg@5: 0.492084\tvalid_1's ndcg@5: 0.393003\n",
      "[323]\ttraining's ndcg@5: 0.492611\tvalid_1's ndcg@5: 0.393063\n",
      "[324]\ttraining's ndcg@5: 0.49254\tvalid_1's ndcg@5: 0.393072\n",
      "[325]\ttraining's ndcg@5: 0.49322\tvalid_1's ndcg@5: 0.393456\n",
      "[326]\ttraining's ndcg@5: 0.49312\tvalid_1's ndcg@5: 0.393424\n",
      "[327]\ttraining's ndcg@5: 0.493369\tvalid_1's ndcg@5: 0.393325\n",
      "[328]\ttraining's ndcg@5: 0.493372\tvalid_1's ndcg@5: 0.393416\n",
      "[329]\ttraining's ndcg@5: 0.493475\tvalid_1's ndcg@5: 0.393454\n",
      "[330]\ttraining's ndcg@5: 0.493421\tvalid_1's ndcg@5: 0.393412\n",
      "[331]\ttraining's ndcg@5: 0.493431\tvalid_1's ndcg@5: 0.393472\n",
      "[332]\ttraining's ndcg@5: 0.493392\tvalid_1's ndcg@5: 0.393393\n",
      "[333]\ttraining's ndcg@5: 0.493305\tvalid_1's ndcg@5: 0.3934\n",
      "[334]\ttraining's ndcg@5: 0.49333\tvalid_1's ndcg@5: 0.393433\n",
      "[335]\ttraining's ndcg@5: 0.493516\tvalid_1's ndcg@5: 0.393312\n",
      "[336]\ttraining's ndcg@5: 0.493385\tvalid_1's ndcg@5: 0.393287\n",
      "[337]\ttraining's ndcg@5: 0.493736\tvalid_1's ndcg@5: 0.393297\n",
      "[338]\ttraining's ndcg@5: 0.494048\tvalid_1's ndcg@5: 0.3934\n",
      "[339]\ttraining's ndcg@5: 0.493937\tvalid_1's ndcg@5: 0.393467\n",
      "[340]\ttraining's ndcg@5: 0.493917\tvalid_1's ndcg@5: 0.393469\n",
      "[341]\ttraining's ndcg@5: 0.494071\tvalid_1's ndcg@5: 0.393269\n",
      "[342]\ttraining's ndcg@5: 0.49459\tvalid_1's ndcg@5: 0.393415\n",
      "[343]\ttraining's ndcg@5: 0.495168\tvalid_1's ndcg@5: 0.393375\n",
      "[344]\ttraining's ndcg@5: 0.495138\tvalid_1's ndcg@5: 0.393267\n",
      "[345]\ttraining's ndcg@5: 0.495514\tvalid_1's ndcg@5: 0.393521\n",
      "[346]\ttraining's ndcg@5: 0.495626\tvalid_1's ndcg@5: 0.393722\n",
      "[347]\ttraining's ndcg@5: 0.495556\tvalid_1's ndcg@5: 0.393584\n",
      "[348]\ttraining's ndcg@5: 0.496259\tvalid_1's ndcg@5: 0.393733\n",
      "[349]\ttraining's ndcg@5: 0.497002\tvalid_1's ndcg@5: 0.393869\n",
      "[350]\ttraining's ndcg@5: 0.496964\tvalid_1's ndcg@5: 0.393915\n",
      "[351]\ttraining's ndcg@5: 0.497162\tvalid_1's ndcg@5: 0.39399\n",
      "[352]\ttraining's ndcg@5: 0.497567\tvalid_1's ndcg@5: 0.39404\n",
      "[353]\ttraining's ndcg@5: 0.497973\tvalid_1's ndcg@5: 0.394083\n",
      "[354]\ttraining's ndcg@5: 0.497933\tvalid_1's ndcg@5: 0.394071\n",
      "[355]\ttraining's ndcg@5: 0.497875\tvalid_1's ndcg@5: 0.394059\n",
      "[356]\ttraining's ndcg@5: 0.498285\tvalid_1's ndcg@5: 0.394337\n",
      "[357]\ttraining's ndcg@5: 0.498251\tvalid_1's ndcg@5: 0.394335\n",
      "[358]\ttraining's ndcg@5: 0.498373\tvalid_1's ndcg@5: 0.394282\n",
      "[359]\ttraining's ndcg@5: 0.498369\tvalid_1's ndcg@5: 0.394246\n",
      "[360]\ttraining's ndcg@5: 0.498899\tvalid_1's ndcg@5: 0.394386\n",
      "[361]\ttraining's ndcg@5: 0.499091\tvalid_1's ndcg@5: 0.394343\n",
      "[362]\ttraining's ndcg@5: 0.499424\tvalid_1's ndcg@5: 0.394366\n",
      "[363]\ttraining's ndcg@5: 0.499415\tvalid_1's ndcg@5: 0.394287\n",
      "[364]\ttraining's ndcg@5: 0.499369\tvalid_1's ndcg@5: 0.394208\n",
      "[365]\ttraining's ndcg@5: 0.499465\tvalid_1's ndcg@5: 0.394297\n",
      "[366]\ttraining's ndcg@5: 0.499418\tvalid_1's ndcg@5: 0.394276\n",
      "[367]\ttraining's ndcg@5: 0.499342\tvalid_1's ndcg@5: 0.394349\n",
      "[368]\ttraining's ndcg@5: 0.499609\tvalid_1's ndcg@5: 0.394291\n",
      "[369]\ttraining's ndcg@5: 0.499578\tvalid_1's ndcg@5: 0.394213\n",
      "[370]\ttraining's ndcg@5: 0.499832\tvalid_1's ndcg@5: 0.394407\n",
      "[371]\ttraining's ndcg@5: 0.499839\tvalid_1's ndcg@5: 0.394447\n",
      "[372]\ttraining's ndcg@5: 0.5\tvalid_1's ndcg@5: 0.394465\n",
      "[373]\ttraining's ndcg@5: 0.50038\tvalid_1's ndcg@5: 0.394757\n",
      "[374]\ttraining's ndcg@5: 0.500744\tvalid_1's ndcg@5: 0.394504\n",
      "[375]\ttraining's ndcg@5: 0.501027\tvalid_1's ndcg@5: 0.394548\n",
      "[376]\ttraining's ndcg@5: 0.500997\tvalid_1's ndcg@5: 0.394444\n",
      "[377]\ttraining's ndcg@5: 0.50137\tvalid_1's ndcg@5: 0.394277\n",
      "[378]\ttraining's ndcg@5: 0.501296\tvalid_1's ndcg@5: 0.394328\n",
      "[379]\ttraining's ndcg@5: 0.501246\tvalid_1's ndcg@5: 0.394373\n",
      "[380]\ttraining's ndcg@5: 0.501426\tvalid_1's ndcg@5: 0.394634\n",
      "[381]\ttraining's ndcg@5: 0.501391\tvalid_1's ndcg@5: 0.394603\n",
      "[382]\ttraining's ndcg@5: 0.501336\tvalid_1's ndcg@5: 0.394594\n",
      "[383]\ttraining's ndcg@5: 0.501918\tvalid_1's ndcg@5: 0.394725\n",
      "[384]\ttraining's ndcg@5: 0.502102\tvalid_1's ndcg@5: 0.394547\n",
      "[385]\ttraining's ndcg@5: 0.502031\tvalid_1's ndcg@5: 0.394548\n",
      "[386]\ttraining's ndcg@5: 0.502188\tvalid_1's ndcg@5: 0.394387\n",
      "[387]\ttraining's ndcg@5: 0.502462\tvalid_1's ndcg@5: 0.394314\n",
      "[388]\ttraining's ndcg@5: 0.503006\tvalid_1's ndcg@5: 0.394501\n",
      "[389]\ttraining's ndcg@5: 0.503226\tvalid_1's ndcg@5: 0.394403\n",
      "[390]\ttraining's ndcg@5: 0.503684\tvalid_1's ndcg@5: 0.394348\n",
      "[391]\ttraining's ndcg@5: 0.503613\tvalid_1's ndcg@5: 0.3944\n",
      "[392]\ttraining's ndcg@5: 0.503767\tvalid_1's ndcg@5: 0.394269\n",
      "[393]\ttraining's ndcg@5: 0.504145\tvalid_1's ndcg@5: 0.394358\n",
      "[394]\ttraining's ndcg@5: 0.504459\tvalid_1's ndcg@5: 0.39468\n",
      "[395]\ttraining's ndcg@5: 0.50491\tvalid_1's ndcg@5: 0.394736\n",
      "[396]\ttraining's ndcg@5: 0.504845\tvalid_1's ndcg@5: 0.394681\n",
      "[397]\ttraining's ndcg@5: 0.50504\tvalid_1's ndcg@5: 0.394781\n",
      "[398]\ttraining's ndcg@5: 0.505435\tvalid_1's ndcg@5: 0.394739\n",
      "[399]\ttraining's ndcg@5: 0.50586\tvalid_1's ndcg@5: 0.394769\n",
      "[400]\ttraining's ndcg@5: 0.506193\tvalid_1's ndcg@5: 0.394771\n",
      "[401]\ttraining's ndcg@5: 0.50617\tvalid_1's ndcg@5: 0.394707\n",
      "[402]\ttraining's ndcg@5: 0.506636\tvalid_1's ndcg@5: 0.394896\n",
      "[403]\ttraining's ndcg@5: 0.506628\tvalid_1's ndcg@5: 0.394855\n",
      "[404]\ttraining's ndcg@5: 0.506839\tvalid_1's ndcg@5: 0.394871\n",
      "[405]\ttraining's ndcg@5: 0.507077\tvalid_1's ndcg@5: 0.395235\n",
      "[406]\ttraining's ndcg@5: 0.507029\tvalid_1's ndcg@5: 0.395115\n",
      "[407]\ttraining's ndcg@5: 0.507423\tvalid_1's ndcg@5: 0.395127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[408]\ttraining's ndcg@5: 0.507337\tvalid_1's ndcg@5: 0.39506\n",
      "[409]\ttraining's ndcg@5: 0.507568\tvalid_1's ndcg@5: 0.395064\n",
      "[410]\ttraining's ndcg@5: 0.507961\tvalid_1's ndcg@5: 0.39537\n",
      "[411]\ttraining's ndcg@5: 0.508493\tvalid_1's ndcg@5: 0.395474\n",
      "[412]\ttraining's ndcg@5: 0.508415\tvalid_1's ndcg@5: 0.395339\n",
      "[413]\ttraining's ndcg@5: 0.508318\tvalid_1's ndcg@5: 0.395322\n",
      "[414]\ttraining's ndcg@5: 0.508624\tvalid_1's ndcg@5: 0.395193\n",
      "[415]\ttraining's ndcg@5: 0.508942\tvalid_1's ndcg@5: 0.395014\n",
      "[416]\ttraining's ndcg@5: 0.50942\tvalid_1's ndcg@5: 0.394988\n",
      "[417]\ttraining's ndcg@5: 0.509355\tvalid_1's ndcg@5: 0.395003\n",
      "[418]\ttraining's ndcg@5: 0.509322\tvalid_1's ndcg@5: 0.395079\n",
      "[419]\ttraining's ndcg@5: 0.509652\tvalid_1's ndcg@5: 0.394979\n",
      "[420]\ttraining's ndcg@5: 0.509633\tvalid_1's ndcg@5: 0.394894\n",
      "[421]\ttraining's ndcg@5: 0.509833\tvalid_1's ndcg@5: 0.394879\n",
      "[422]\ttraining's ndcg@5: 0.510196\tvalid_1's ndcg@5: 0.395038\n",
      "[423]\ttraining's ndcg@5: 0.510474\tvalid_1's ndcg@5: 0.394968\n",
      "[424]\ttraining's ndcg@5: 0.510778\tvalid_1's ndcg@5: 0.395138\n",
      "[425]\ttraining's ndcg@5: 0.510716\tvalid_1's ndcg@5: 0.395095\n",
      "[426]\ttraining's ndcg@5: 0.511099\tvalid_1's ndcg@5: 0.395074\n",
      "[427]\ttraining's ndcg@5: 0.511342\tvalid_1's ndcg@5: 0.395179\n",
      "[428]\ttraining's ndcg@5: 0.511223\tvalid_1's ndcg@5: 0.395127\n",
      "[429]\ttraining's ndcg@5: 0.511747\tvalid_1's ndcg@5: 0.39512\n",
      "[430]\ttraining's ndcg@5: 0.511756\tvalid_1's ndcg@5: 0.395147\n",
      "[431]\ttraining's ndcg@5: 0.511946\tvalid_1's ndcg@5: 0.395239\n",
      "[432]\ttraining's ndcg@5: 0.512464\tvalid_1's ndcg@5: 0.395279\n",
      "[433]\ttraining's ndcg@5: 0.5127\tvalid_1's ndcg@5: 0.39536\n",
      "[434]\ttraining's ndcg@5: 0.512686\tvalid_1's ndcg@5: 0.395365\n",
      "[435]\ttraining's ndcg@5: 0.512654\tvalid_1's ndcg@5: 0.395373\n",
      "[436]\ttraining's ndcg@5: 0.513116\tvalid_1's ndcg@5: 0.395294\n",
      "[437]\ttraining's ndcg@5: 0.513402\tvalid_1's ndcg@5: 0.395112\n",
      "[438]\ttraining's ndcg@5: 0.513328\tvalid_1's ndcg@5: 0.39518\n",
      "[439]\ttraining's ndcg@5: 0.513321\tvalid_1's ndcg@5: 0.3952\n",
      "[440]\ttraining's ndcg@5: 0.51341\tvalid_1's ndcg@5: 0.395286\n",
      "[441]\ttraining's ndcg@5: 0.513848\tvalid_1's ndcg@5: 0.395227\n",
      "[442]\ttraining's ndcg@5: 0.514205\tvalid_1's ndcg@5: 0.395264\n",
      "[443]\ttraining's ndcg@5: 0.514158\tvalid_1's ndcg@5: 0.395306\n",
      "[444]\ttraining's ndcg@5: 0.514414\tvalid_1's ndcg@5: 0.395226\n",
      "[445]\ttraining's ndcg@5: 0.514726\tvalid_1's ndcg@5: 0.395272\n",
      "[446]\ttraining's ndcg@5: 0.514725\tvalid_1's ndcg@5: 0.395284\n",
      "[447]\ttraining's ndcg@5: 0.51495\tvalid_1's ndcg@5: 0.395274\n",
      "[448]\ttraining's ndcg@5: 0.515364\tvalid_1's ndcg@5: 0.395349\n",
      "[449]\ttraining's ndcg@5: 0.51568\tvalid_1's ndcg@5: 0.395302\n",
      "[450]\ttraining's ndcg@5: 0.515993\tvalid_1's ndcg@5: 0.395667\n",
      "[451]\ttraining's ndcg@5: 0.516263\tvalid_1's ndcg@5: 0.39568\n",
      "[452]\ttraining's ndcg@5: 0.516254\tvalid_1's ndcg@5: 0.395652\n",
      "[453]\ttraining's ndcg@5: 0.516215\tvalid_1's ndcg@5: 0.395609\n",
      "[454]\ttraining's ndcg@5: 0.516239\tvalid_1's ndcg@5: 0.39554\n",
      "[455]\ttraining's ndcg@5: 0.516387\tvalid_1's ndcg@5: 0.395314\n",
      "[456]\ttraining's ndcg@5: 0.516331\tvalid_1's ndcg@5: 0.39538\n",
      "[457]\ttraining's ndcg@5: 0.516265\tvalid_1's ndcg@5: 0.395409\n",
      "[458]\ttraining's ndcg@5: 0.516583\tvalid_1's ndcg@5: 0.395412\n",
      "[459]\ttraining's ndcg@5: 0.516979\tvalid_1's ndcg@5: 0.395512\n",
      "[460]\ttraining's ndcg@5: 0.516975\tvalid_1's ndcg@5: 0.395464\n",
      "[461]\ttraining's ndcg@5: 0.517177\tvalid_1's ndcg@5: 0.39568\n",
      "[462]\ttraining's ndcg@5: 0.517139\tvalid_1's ndcg@5: 0.395712\n",
      "[463]\ttraining's ndcg@5: 0.517045\tvalid_1's ndcg@5: 0.395699\n",
      "[464]\ttraining's ndcg@5: 0.517409\tvalid_1's ndcg@5: 0.395573\n",
      "[465]\ttraining's ndcg@5: 0.517568\tvalid_1's ndcg@5: 0.395611\n",
      "[466]\ttraining's ndcg@5: 0.517598\tvalid_1's ndcg@5: 0.395597\n",
      "[467]\ttraining's ndcg@5: 0.517937\tvalid_1's ndcg@5: 0.395523\n",
      "[468]\ttraining's ndcg@5: 0.517895\tvalid_1's ndcg@5: 0.395539\n",
      "[469]\ttraining's ndcg@5: 0.518469\tvalid_1's ndcg@5: 0.395691\n",
      "[470]\ttraining's ndcg@5: 0.518438\tvalid_1's ndcg@5: 0.395696\n",
      "[471]\ttraining's ndcg@5: 0.518717\tvalid_1's ndcg@5: 0.395418\n",
      "[472]\ttraining's ndcg@5: 0.518674\tvalid_1's ndcg@5: 0.395478\n",
      "[473]\ttraining's ndcg@5: 0.519118\tvalid_1's ndcg@5: 0.395438\n",
      "[474]\ttraining's ndcg@5: 0.519389\tvalid_1's ndcg@5: 0.395414\n",
      "[475]\ttraining's ndcg@5: 0.519702\tvalid_1's ndcg@5: 0.39535\n",
      "[476]\ttraining's ndcg@5: 0.519677\tvalid_1's ndcg@5: 0.395419\n",
      "[477]\ttraining's ndcg@5: 0.519834\tvalid_1's ndcg@5: 0.395368\n",
      "[478]\ttraining's ndcg@5: 0.519759\tvalid_1's ndcg@5: 0.395383\n",
      "[479]\ttraining's ndcg@5: 0.519715\tvalid_1's ndcg@5: 0.395339\n",
      "[480]\ttraining's ndcg@5: 0.519988\tvalid_1's ndcg@5: 0.395292\n",
      "[481]\ttraining's ndcg@5: 0.52015\tvalid_1's ndcg@5: 0.395444\n",
      "[482]\ttraining's ndcg@5: 0.520549\tvalid_1's ndcg@5: 0.395273\n",
      "[483]\ttraining's ndcg@5: 0.520712\tvalid_1's ndcg@5: 0.395428\n",
      "[484]\ttraining's ndcg@5: 0.521069\tvalid_1's ndcg@5: 0.395428\n",
      "[485]\ttraining's ndcg@5: 0.521056\tvalid_1's ndcg@5: 0.395382\n",
      "[486]\ttraining's ndcg@5: 0.521305\tvalid_1's ndcg@5: 0.395398\n",
      "[487]\ttraining's ndcg@5: 0.521712\tvalid_1's ndcg@5: 0.395339\n",
      "[488]\ttraining's ndcg@5: 0.522032\tvalid_1's ndcg@5: 0.395291\n",
      "[489]\ttraining's ndcg@5: 0.52215\tvalid_1's ndcg@5: 0.395354\n",
      "[490]\ttraining's ndcg@5: 0.522079\tvalid_1's ndcg@5: 0.395373\n",
      "[491]\ttraining's ndcg@5: 0.522091\tvalid_1's ndcg@5: 0.395375\n",
      "[492]\ttraining's ndcg@5: 0.522044\tvalid_1's ndcg@5: 0.395311\n",
      "[493]\ttraining's ndcg@5: 0.521957\tvalid_1's ndcg@5: 0.395296\n",
      "[494]\ttraining's ndcg@5: 0.522138\tvalid_1's ndcg@5: 0.395258\n",
      "[495]\ttraining's ndcg@5: 0.522533\tvalid_1's ndcg@5: 0.395353\n",
      "[496]\ttraining's ndcg@5: 0.522695\tvalid_1's ndcg@5: 0.39534\n",
      "[497]\ttraining's ndcg@5: 0.522948\tvalid_1's ndcg@5: 0.395391\n",
      "[498]\ttraining's ndcg@5: 0.523265\tvalid_1's ndcg@5: 0.395565\n",
      "[499]\ttraining's ndcg@5: 0.523242\tvalid_1's ndcg@5: 0.395511\n",
      "[500]\ttraining's ndcg@5: 0.523554\tvalid_1's ndcg@5: 0.395478\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMRanker(boosting_type=&#x27;dart&#x27;, colsample_bytree=0.5433556425106324,\n",
       "           feature_fraction=0.9677058301342538,\n",
       "           learning_rate=0.08925380432712779, max_depth=10, metric=&#x27;ndcg&#x27;,\n",
       "           n_estimators=500, objective=&#x27;lambdarank&#x27;,\n",
       "           reg_alpha=0.00011669441178850782, reg_lambda=0.008250891056480582,\n",
       "           subsample=0.523890758165789)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMRanker</label><div class=\"sk-toggleable__content\"><pre>LGBMRanker(boosting_type=&#x27;dart&#x27;, colsample_bytree=0.5433556425106324,\n",
       "           feature_fraction=0.9677058301342538,\n",
       "           learning_rate=0.08925380432712779, max_depth=10, metric=&#x27;ndcg&#x27;,\n",
       "           n_estimators=500, objective=&#x27;lambdarank&#x27;,\n",
       "           reg_alpha=0.00011669441178850782, reg_lambda=0.008250891056480582,\n",
       "           subsample=0.523890758165789)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMRanker(boosting_type='dart', colsample_bytree=0.5433556425106324,\n",
       "           feature_fraction=0.9677058301342538,\n",
       "           learning_rate=0.08925380432712779, max_depth=10, metric='ndcg',\n",
       "           n_estimators=500, objective='lambdarank',\n",
       "           reg_alpha=0.00011669441178850782, reg_lambda=0.008250891056480582,\n",
       "           subsample=0.523890758165789)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm.fit(\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    group=train_groups,\n",
    "    eval_set=[(X_train, y_train),(X_test, y_test)],\n",
    "    eval_group=[train_groups, test_groups],\n",
    "    eval_at=[5],\n",
    "    feature_name='auto', \n",
    "    categorical_feature = 'auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e7ce3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optimize LGBM with optuna\n",
    "# import optuna\n",
    "# from functools import partial\n",
    "\n",
    "# def objective(trial, X_train, y_train, X_test, test_ideal):\n",
    "#     #y_train_gbm = y_train.astype(int)\n",
    "#     #y_train_gbm[y_train == 5] = 2\n",
    "\n",
    "#     params = {\n",
    "#         \"objective\": \"lambdarank\",\n",
    "#         \"metric\":\"ndcg\",\n",
    "#         \"random_state\": 42,\n",
    "#         \"boosting_type\": \"dart\",\n",
    "#         #\"early_stopping_round\": trial.suggest_int(\"early_stopping_round\", 10, 50),\n",
    "#         \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500),\n",
    "#         \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 200),\n",
    "#         \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "#         \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True),\n",
    "#         \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "#         \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-4, 1e-1, log=True),\n",
    "#         \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-4, 1e-1, log=True),\n",
    "#         \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.2, 1.0),\n",
    "#     }\n",
    "\n",
    "#     gbm = lgb.LGBMRanker(**params)\n",
    "#     gbm.fit(X_train, y_train, group=train_groups, eval_set=[(X_test, y_test)], eval_group=[test_groups])\n",
    "\n",
    "#     pred_lgbm = constructs_predictions(gbm, X_test, ideal_df=test_ideal)\n",
    "#     ndcg = calc_NDCG(test_ideal, pred_lgbm)\n",
    "\n",
    "#     return ndcg\n",
    "\n",
    "# print(\"Training LGBM\")\n",
    "\n",
    "# # Wrap the objective function with the input data\n",
    "# objective_with_data = partial(objective, X_train=X_train, y_train=y_train, X_test=X_test, test_ideal=test_ideal)\n",
    "\n",
    "# # Create an Optuna study and optimize the objective function\n",
    "# study = optuna.create_study(direction=\"maximize\")\n",
    "# study.optimize(objective_with_data, n_trials=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16dc2925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params = study.best_params\n",
    "# print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3db26841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params = study.best_params\n",
    "# gbm = lgb.LGBMRanker(objective=\"lambdarank\", metric=\"ndcg\", boosting_type=\"dart\", **best_params)\n",
    "# gbm.fit(X_train, y_train, group=train_groups, eval_set=[(X_test, y_test)], eval_group=[test_groups])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844703d3",
   "metadata": {},
   "source": [
    "<h1> Validation <h1\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1e8790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_pred_ideal(df_in, df_ideal, y_pred):\n",
    "    df = df_in.copy()\n",
    "    df['pred_grades'] = y_pred\n",
    "    df = df.sort_values(by=['srch_id', 'pred_grades'], ascending=[True, False], inplace=False)\n",
    "\n",
    "    # Merge grades from ideal on srch_id and prop_id\n",
    "    df = df.merge(df_ideal, on=['srch_id', 'prop_id'], how='left')\n",
    "\n",
    "    # Return srch_id, prop_id and pred_grades\n",
    "    return df[['srch_id', 'prop_id', 'pred_grades', 'score']]\n",
    "\n",
    "def construct_pred_submission(df_in, y_pred):\n",
    "    df = df_in.copy()\n",
    "    df['pred_grades'] = y_pred\n",
    "    df = df.sort_values(by=['srch_id', 'pred_grades'], ascending=[True, False], inplace=False)\n",
    "\n",
    "    # Return srch_id, prop_id and pred_grades\n",
    "    return df[['srch_id', 'prop_id']]\n",
    "\n",
    "def constructs_predictions(model, data, ideal_df = None):\n",
    "    y_pred = model.predict(data)\n",
    "\n",
    "    if ideal_df is not None:\n",
    "        pred_df = construct_pred_ideal(data, test_ideal, y_pred)\n",
    "    else:\n",
    "        pred_df = construct_pred_submission(data, y_pred)\n",
    "    return pred_df\n",
    "\n",
    "def calc_NDCG(df_ideal, df_pred, k = 5):\n",
    "    # Group by 5\n",
    "    df_ideal = df_ideal.groupby('srch_id').head(k)\n",
    "    df_pred = df_pred.groupby('srch_id').head(k)\n",
    "\n",
    "    assert df_ideal.shape[0] % k == 0\n",
    "    assert df_pred.shape[0] % k == 0\n",
    "    \n",
    "    # Get grades matrices\n",
    "    ideal_grades = df_ideal['score'].values.reshape(int(df_ideal.shape[0] / k), k)\n",
    "    pred_grades = df_pred['score'].values.reshape(int(df_pred.shape[0] / k), k)\n",
    "\n",
    "    discount_vec = [1/np.log2(i+2) for i in range(k)]\n",
    "\n",
    "    # Calculate NDCG\n",
    "    NDCG = (pred_grades @ discount_vec).sum() / (ideal_grades @ discount_vec).sum()\n",
    "\n",
    "    return NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8816227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lgbm = constructs_predictions(gbm, X_test, ideal_df=test_ideal)\n",
    "pred_random = construct_pred_ideal(X_test, test_ideal, np.random.rand(len(X_test)))\n",
    "pred_lgbm_submission = constructs_predictions(gbm, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6078aa",
   "metadata": {},
   "source": [
    "Highest score: 0.4167321465628548"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437ac91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"LGBM: {calc_NDCG(test_ideal, pred_lgbm)},\\nRandom: {calc_NDCG(test_ideal, pred_random)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f4821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lgbm_submission.to_csv('../data/submission_LGBM.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4374de1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lgb.plot_importance(gbm, figsize = (12,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eae0ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
