{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "788da39c",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95539adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing\n",
    "import timeit\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a95be1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load data\n",
    "df_train = pd.read_parquet('../data/training_set.parquet', engine = 'auto')\n",
    "#df_train = df_train[df_train['srch_id'] < 10000]\n",
    "df_test = pd.read_parquet('../data/test_set.parquet', engine = 'auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581da370",
   "metadata": {},
   "source": [
    "<h1>Data prep<h1\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "671eca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_score(row):\n",
    "    if row['booking_bool'] == 1:\n",
    "        return 5\n",
    "    elif row['click_bool'] == 1:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "# Add features for hour, day and month.\n",
    "def date_time(df):\n",
    "    df_copy = df.copy()\n",
    "    df_copy['date_time'] = pd.to_datetime(df_copy['date_time'])\n",
    "    df_copy['hour'] = df_copy['date_time'].dt.hour\n",
    "    df_copy['day'] = df_copy['date_time'].dt.day\n",
    "    df_copy['month'] = df_copy['date_time'].dt.month\n",
    "    df_copy = df_copy.drop('date_time', axis=1)\n",
    "    return df_copy\n",
    "\n",
    "def remove_cols(df, cols):\n",
    "    return df.drop(cols, axis=1)\n",
    "\n",
    "def remove_cols_nan(df, limit):\n",
    "    df_new = df.copy()\n",
    "    for col in df_new.columns:\n",
    "        if len(df_new[col]) * limit < df_new[col].isna().sum():\n",
    "            df_new = df_new.drop(col, axis=1)\n",
    "    return df_new\n",
    "\n",
    "# Add column with a ranking for each property in a search based on another column.\n",
    "def create_rank_feature(df, col):\n",
    "    df_new = df.copy()\n",
    "    df_new['rank_' + str(col)] = df.groupby('srch_id')[col].rank(ascending=False)\n",
    "    return df_new\n",
    "\n",
    "def add_normalized_column(df, col, group):\n",
    "    df_new = df.copy()\n",
    "    df_new['norm_' + str(col) + \"_\" + str(group)] = (\n",
    "        (df_new[col] - df_new.groupby(group)[col].transform('mean')) \n",
    "        / df_new.groupby(group)[col].transform('std')\n",
    "    )\n",
    "    return df_new\n",
    "\n",
    "def prep_data(df, target_cols, test=False):\n",
    "    df_new = df.copy()\n",
    "    if not test:\n",
    "        df_new['score'] = df_new.apply(lambda row: make_score(row), axis=1)\n",
    "        df_new = df_new.drop(target_cols, axis=1)\n",
    "    df_new = date_time(df_new)\n",
    "    #df_new = remove_cols_nan(df_new, 0.8)\n",
    "    \n",
    "    # difference features\n",
    "    df_new['usd_diff'] = abs(df_new['visitor_hist_adr_usd'] - df_new['price_usd'])\n",
    "    df_new['star_diff'] = abs(df_new['visitor_hist_starrating'] - df_new['prop_starrating'])\n",
    "    \n",
    "    # ranking features\n",
    "    df_new = create_rank_feature(df_new, 'price_usd')\n",
    "    df_new = create_rank_feature(df_new, 'prop_starrating')\n",
    "    df_new = create_rank_feature(df_new, 'prop_review_score')\n",
    "    df_new = create_rank_feature(df_new, 'prop_location_score1')\n",
    "    df_new = create_rank_feature(df_new, 'prop_location_score2')\n",
    "    \n",
    "    df_new.fillna(-1, inplace=True)\n",
    "    \n",
    "    df_new = add_normalized_column(df_new, 'price_usd', 'srch_id')\n",
    "    df_new = add_normalized_column(df_new, 'prop_starrating', 'srch_id')\n",
    "    df_new = add_normalized_column(df_new, 'prop_review_score', 'srch_id')\n",
    "    df_new = add_normalized_column(df_new, 'prop_location_score1', 'srch_id')\n",
    "    df_new = add_normalized_column(df_new, 'prop_location_score2', 'srch_id')\n",
    "\n",
    "    df_new = add_normalized_column(df_new, 'price_usd', 'prop_id')\n",
    "    df_new = add_normalized_column(df_new, 'prop_starrating', 'prop_id')\n",
    "    df_new = add_normalized_column(df_new, 'prop_review_score', 'prop_id')\n",
    "    df_new = add_normalized_column(df_new, 'prop_location_score1', 'prop_id')\n",
    "    df_new = add_normalized_column(df_new, 'prop_location_score2', 'prop_id')\n",
    "    \n",
    "    df_new = add_normalized_column(df_new, 'price_usd', 'prop_country_id')\n",
    "    df_new = add_normalized_column(df_new, 'prop_starrating', 'prop_country_id')\n",
    "    df_new = add_normalized_column(df_new, 'prop_review_score', 'prop_country_id')\n",
    "    df_new = add_normalized_column(df_new, 'prop_location_score1', 'prop_country_id')\n",
    "    df_new = add_normalized_column(df_new, 'prop_location_score2', 'prop_country_id')\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4d6eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = ['click_bool', 'booking_bool', 'gross_bookings_usd', 'position']\n",
    "#df_new[\"norm_\" + str(group) + str(col)] = df.groupby(group).col(df.col-g.transform('min')) / g.transform(np.ptp)\n",
    "\n",
    "df_train = prep_data(df_train, target_cols, False)\n",
    "df_test = prep_data(df_test, target_cols, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaaaca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_new[['price_usd', 'norm_price_usdsrch_id']].loc[df_new['srch_id'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da28ce6",
   "metadata": {},
   "source": [
    "<h1>Data split<h1\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daeec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "splitter = GroupShuffleSplit(test_size=0.2, n_splits=2, random_state = 7)\n",
    "split = splitter.split(df_train, groups=df_train['srch_id'])\n",
    "train_inds, test_inds = next(split)\n",
    "\n",
    "df_ideal = df_train.iloc[test_inds].copy().sort_values(by=['srch_id', 'score'], ascending=[True, False], inplace=False)\n",
    "\n",
    "X = df_train.drop(['score'], axis=1)\n",
    "y = df_train['score']\n",
    "\n",
    "X_train, X_test, y_train, y_test, test_ideal = X.iloc[train_inds], X.iloc[test_inds], y.iloc[train_inds], y.iloc[test_inds], df_ideal\n",
    "\n",
    "train_groups = X_train.groupby('srch_id').size().to_frame('size')['size'].to_numpy()\n",
    "\n",
    "test_groups = X_test.groupby('srch_id').size().to_frame('size')['size'].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe60e35b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2360896",
   "metadata": {},
   "source": [
    "<h1>Training <h1\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d562c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"objective\": \"lambdarank\",\n",
    "    \"metric\":\"ndcg\",\n",
    "    'n_estimators': 498, \n",
    "    'max_depth': 7, \n",
    "    'learning_rate': 0.04938250379207737, \n",
    "    'subsample': 0.5098019827512731, \n",
    "    'colsample_bytree': 0.5433556425106324, \n",
    "    'gamma': 0.33103514405053813, \n",
    "    'reg_alpha': 0.0030927739265164565, \n",
    "    'reg_lambda': 0.0005679745733624298\n",
    "}\n",
    "\n",
    "gbm = lgb.LGBMRanker(**params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f71f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm.fit(X_train, y_train, \n",
    "        group=train_groups, eval_set=[(X_test, y_test)], \n",
    "        eval_group=[test_groups])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7ce3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optimize LGBM with optuna\n",
    "# import optuna\n",
    "# from functools import partial\n",
    "\n",
    "# def objective(trial, X_train, y_train, X_test, test_ideal):\n",
    "#     y_train_xgb = y_train.astype(int)\n",
    "#     y_train_xgb[y_train == 5] = 2\n",
    "\n",
    "#     params = {\n",
    "#         \"objective\": \"lambdarank\",\n",
    "#         \"metric\":\"ndcg\",\n",
    "#         \"random_state\": 42,\n",
    "#         \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500),\n",
    "#         \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "#         \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True),\n",
    "#         \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "#         \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "#         \"gamma\": trial.suggest_float(\"gamma\", 0, 1),\n",
    "#         \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-4, 1e-1, log=True),\n",
    "#         \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-4, 1e-1, log=True),\n",
    "#     }\n",
    "\n",
    "#     gbm = lgb.LGBMRanker(**params)\n",
    "#     gbm.fit(X_train, y_train, group=train_groups, eval_set=[(X_test, y_test)], eval_group=[test_groups])\n",
    "\n",
    "#     pred_lgbm = constructs_predictions(gbm, X_test, ideal_df=test_ideal)\n",
    "#     ndcg = calc_NDCG(test_ideal, pred_lgbm)\n",
    "\n",
    "#     return ndcg\n",
    "\n",
    "# print(\"Training LGBM\")\n",
    "\n",
    "# # Wrap the objective function with the input data\n",
    "# objective_with_data = partial(objective, X_train=X_train, y_train=y_train, X_test=X_test, test_ideal=test_ideal)\n",
    "\n",
    "# # Create an Optuna study and optimize the objective function\n",
    "# study = optuna.create_study(direction=\"maximize\")\n",
    "# study.optimize(objective_with_data, n_trials=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81604ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params = study.best_params\n",
    "# print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e3072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params = study.best_params\n",
    "# gbm = lgb.LGBMRanker(objective=\"lambdarank\",metric=\"ndcg\", **best_params)\n",
    "# gbm.fit(X_train, y_train, group=train_groups, eval_set=[(X_test, y_test)], eval_group=[test_groups])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844703d3",
   "metadata": {},
   "source": [
    "<h1> Validation <h1\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e8790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_pred_ideal(df_in, df_ideal, y_pred):\n",
    "    df = df_in.copy()\n",
    "    df['pred_grades'] = y_pred\n",
    "    df = df.sort_values(by=['srch_id', 'pred_grades'], ascending=[True, False], inplace=False)\n",
    "\n",
    "    # Merge grades from ideal on srch_id and prop_id\n",
    "    df = df.merge(df_ideal, on=['srch_id', 'prop_id'], how='left')\n",
    "\n",
    "    # Return srch_id, prop_id and pred_grades\n",
    "    return df[['srch_id', 'prop_id', 'pred_grades', 'score']]\n",
    "\n",
    "def construct_pred_submission(df_in, y_pred):\n",
    "    df = df_in.copy()\n",
    "    df['pred_grades'] = y_pred\n",
    "    df = df.sort_values(by=['srch_id', 'pred_grades'], ascending=[True, False], inplace=False)\n",
    "\n",
    "    # Return srch_id, prop_id and pred_grades\n",
    "    return df[['srch_id', 'prop_id']]\n",
    "\n",
    "def constructs_predictions(model, data, ideal_df = None):\n",
    "    y_pred = model.predict(data)\n",
    "\n",
    "    if ideal_df is not None:\n",
    "        pred_df = construct_pred_ideal(data, test_ideal, y_pred)\n",
    "    else:\n",
    "        pred_df = construct_pred_submission(data, y_pred)\n",
    "    return pred_df\n",
    "\n",
    "def calc_NDCG(df_ideal, df_pred, k = 5):\n",
    "    # Group by 5\n",
    "    df_ideal = df_ideal.groupby('srch_id').head(k)\n",
    "    df_pred = df_pred.groupby('srch_id').head(k)\n",
    "\n",
    "    assert df_ideal.shape[0] % k == 0\n",
    "    assert df_pred.shape[0] % k == 0\n",
    "    \n",
    "    # Get grades matrices\n",
    "    ideal_grades = df_ideal['score'].values.reshape(int(df_ideal.shape[0] / k), k)\n",
    "    pred_grades = df_pred['score'].values.reshape(int(df_pred.shape[0] / k), k)\n",
    "\n",
    "    discount_vec = [1/np.log2(i+2) for i in range(k)]\n",
    "\n",
    "    # Calculate NDCG\n",
    "    NDCG = (pred_grades @ discount_vec).sum() / (ideal_grades @ discount_vec).sum()\n",
    "\n",
    "    return NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8816227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lgbm = constructs_predictions(gbm, X_test, ideal_df=test_ideal)\n",
    "pred_random = construct_pred_ideal(X_test, test_ideal, np.random.rand(len(X_test)))\n",
    "pred_lgbm_submission = constructs_predictions(gbm, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6078aa",
   "metadata": {},
   "source": [
    "Highest score: 0.40449893782893503"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437ac91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"LGBM: {calc_NDCG(test_ideal, pred_lgbm)},\\nRandom: {calc_NDCG(test_ideal, pred_random)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f4821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lgbm_submission.to_csv('../data/submission_LGBM.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4374de1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
